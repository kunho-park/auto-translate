"""
í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬ ëª¨ë“ˆ
"""

import logging
from typing import Any, Dict, Optional

from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult

logger = logging.getLogger(__name__)


class TokenCountingHandler(BaseCallbackHandler):
    """í† í° ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•˜ëŠ” ì½œë°± í•¸ë“¤ëŸ¬"""

    def __init__(self):
        self.reset_counts()

    def reset_counts(self):
        """í† í° ì¹´ìš´íŠ¸ ì´ˆê¸°í™”"""
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_tokens = 0
        self.provider_counts = {}  # ì œê³µì—…ì²´ë³„ í† í° ì¹´ìš´íŠ¸

    def on_llm_end(
        self,
        response: LLMResult,
        *,
        run_id: str,
        parent_run_id: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        """LLM í˜¸ì¶œ ì™„ë£Œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì """
        if response.llm_output and "token_usage" in response.llm_output:
            token_usage = response.llm_output["token_usage"]

            # í† í° ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
            prompt_tokens = token_usage.get("prompt_tokens", 0)
            completion_tokens = token_usage.get("completion_tokens", 0)
            total_tokens = token_usage.get("total_tokens", 0)

            self.prompt_tokens += prompt_tokens
            self.completion_tokens += completion_tokens
            self.total_tokens += total_tokens

            # ì œê³µì—…ì²´ë³„ ì¹´ìš´íŠ¸ (ì¶”ê°€ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
            model_name = response.llm_output.get("model_name", "unknown")
            if model_name not in self.provider_counts:
                self.provider_counts[model_name] = {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                }

            self.provider_counts[model_name]["prompt_tokens"] += prompt_tokens
            self.provider_counts[model_name]["completion_tokens"] += completion_tokens
            self.provider_counts[model_name]["total_tokens"] += total_tokens

            logger.debug(
                f"í† í° ì‚¬ìš©ëŸ‰ - í”„ë¡¬í”„íŠ¸: {prompt_tokens}, "
                f"ì™„ë£Œ: {completion_tokens}, ì´: {total_tokens}"
            )

    def get_token_summary(self) -> Dict[str, Any]:
        """í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ë°˜í™˜"""
        return {
            "prompt_tokens": self.prompt_tokens,
            "completion_tokens": self.completion_tokens,
            "total_tokens": self.total_tokens,
            "provider_counts": self.provider_counts.copy(),
        }


class UniversalTokenCountingHandler(BaseCallbackHandler):
    """ëª¨ë“  ì œê³µì—…ì²´ì— ëŒ€ì‘í•˜ëŠ” ë²”ìš© í† í° ì¹´ìš´íŒ… í•¸ë“¤ëŸ¬

    Args:
        update_callback: í† í° ì¹´ìš´íŠ¸ê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ í˜¸ì¶œë  ì½œë°± í•¨ìˆ˜.
            ì½œë°±ì—ëŠ” `get_token_summary()` ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ê°€ ì „ë‹¬ë©ë‹ˆë‹¤.
    """

    def __init__(self, update_callback: Optional[callable] = None):
        self.update_callback = update_callback
        self.reset_counts()

    def reset_counts(self):
        """í† í° ì¹´ìš´íŠ¸ ì´ˆê¸°í™”"""
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_tokens = 0
        self.provider_counts = {}
        self.call_count = 0

    def on_llm_end(
        self,
        response: LLMResult,
        *,
        run_id: str,
        parent_run_id: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        """LLM í˜¸ì¶œ ì™„ë£Œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (ëª¨ë“  generation ì§‘ê³„)"""
        self.call_count += 1

        try:
            total_prompt = 0
            total_completion = 0
            found_tokens_in_generations = False

            # 1. ìµœì‹  ë°©ì‹: ëª¨ë“  generationì˜ `usage_metadata` ìˆœíšŒ ë° í•©ì‚°
            if response.generations:
                for gen_list in response.generations:
                    for gen in gen_list:
                        if (
                            hasattr(gen, "message")
                            and hasattr(gen.message, "usage_metadata")
                            and gen.message.usage_metadata
                        ):
                            usage_metadata = gen.message.usage_metadata
                            total_prompt += usage_metadata.get(
                                "input_tokens", 0
                            ) or usage_metadata.get("prompt_tokens", 0)
                            total_completion += usage_metadata.get(
                                "output_tokens", 0
                            ) or usage_metadata.get("completion_tokens", 0)
                            found_tokens_in_generations = True

            if found_tokens_in_generations:
                total = total_prompt + total_completion
                usage_dict = {
                    "prompt_tokens": total_prompt,
                    "completion_tokens": total_completion,
                    "total_tokens": total,
                }
                self._update_counts_from_usage(usage_dict, response.llm_output or {})
                return

            # 2. êµ¬ë²„ì „ ë°©ì‹: llm_output.token_usage (ìœ„ì—ì„œ ëª» ì°¾ì€ ê²½ìš°)
            if response.llm_output and "token_usage" in response.llm_output:
                token_usage_data = response.llm_output["token_usage"]
                prompt = token_usage_data.get(
                    "input_tokens", 0
                ) or token_usage_data.get("prompt_tokens", 0)
                completion = token_usage_data.get(
                    "output_tokens", 0
                ) or token_usage_data.get("completion_tokens", 0)
                total = token_usage_data.get("total_tokens", 0)

                if total == 0:
                    total = prompt + completion

                usage_dict = {
                    "prompt_tokens": prompt,
                    "completion_tokens": completion,
                    "total_tokens": total,
                }

                self._update_counts_from_usage(usage_dict, response.llm_output or {})
                return

            # í† í° ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
            if self.call_count <= 5 or self.call_count % 50 == 0:
                logger.warning(
                    f"í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜¸ì¶œ #{self.call_count}"
                )

        except Exception as e:
            logger.error(f"í† í° ì¹´ìš´íŒ… ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í˜¸ì¶œ #{self.call_count}): {e}")
            if self.call_count <= 2:
                logger.debug(f"ì˜ˆì™¸ ë°œìƒ ì‹œ ì‘ë‹µ êµ¬ì¡°: {vars(response)}")

        # ì„±ê³µ ë¡œê·¸ (í˜¸ì¶œ ë¹ˆë„ ì¤„ì„)
        if self.call_count <= 3:
            logger.info(
                f"í† í° ì¶”ì  ì„±ê³µ #{self.call_count}: "
                f"ì…ë ¥={self.prompt_tokens}, ì¶œë ¥={self.completion_tokens} -> ì´ {self.total_tokens:,}"
            )
        elif self.call_count % 50 == 0:
            logger.info(
                f"í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì  ({self.call_count}íšŒ) - ì´ {self.total_tokens:,} í† í°"
            )

    def _update_counts_from_usage(self, token_usage: Dict, llm_output: Dict):
        """í† í° ì‚¬ìš©ëŸ‰ìœ¼ë¡œ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸"""
        prompt_tokens = token_usage.get("prompt_tokens", 0)
        completion_tokens = token_usage.get("completion_tokens", 0)
        total_tokens = token_usage.get("total_tokens", 0)

        # total_tokensê°€ 0ì´ë©´ ê³„ì‚°
        if total_tokens == 0:
            total_tokens = prompt_tokens + completion_tokens

        self.prompt_tokens += prompt_tokens
        self.completion_tokens += completion_tokens
        self.total_tokens += total_tokens

        # ì œê³µì—…ì²´ë³„ ì¹´ìš´íŠ¸
        model_name = llm_output.get("model_name", "unknown")
        if model_name not in self.provider_counts:
            self.provider_counts[model_name] = {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0,
            }

        provider_count = self.provider_counts[model_name]
        provider_count["prompt_tokens"] += prompt_tokens
        provider_count["completion_tokens"] += completion_tokens
        provider_count["total_tokens"] += total_tokens

        # ì‹¤ì‹œê°„ ì½œë°± ì‹¤í–‰ (ì˜ˆì™¸ëŠ” ë¬´ì‹œí•˜ì—¬ íë¦„ ë°©í•´ ë°©ì§€)
        if self.update_callback:
            try:
                self.update_callback(self.get_token_summary())
            except Exception as cb_err:
                logger.debug(f"í† í° ì—…ë°ì´íŠ¸ ì½œë°± ì˜¤ë¥˜ ë¬´ì‹œ: {cb_err}")

    def get_token_summary(self) -> Dict[str, Any]:
        """í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ë°˜í™˜"""
        return {
            "prompt_tokens": self.prompt_tokens,
            "completion_tokens": self.completion_tokens,
            "total_tokens": self.total_tokens,
            "provider_counts": self.provider_counts.copy(),
            "call_count": self.call_count,
        }

    def get_formatted_summary(self) -> str:
        """í¬ë§·ëœ í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ë°˜í™˜"""
        summary = self.get_token_summary()

        text = "ğŸ“Š í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½\n"
        text += f"ğŸ”¤ ì…ë ¥ í† í°: {summary['prompt_tokens']:,}\n"
        text += f"âœï¸ ì¶œë ¥ í† í°: {summary['completion_tokens']:,}\n"
        text += f"ğŸ“ˆ ì´ í† í°: {summary['total_tokens']:,}\n"
        text += f"ğŸ”„ API í˜¸ì¶œ íšŸìˆ˜: {summary['call_count']}\n"

        if summary["provider_counts"]:
            text += "\nğŸ“‹ ëª¨ë¸ë³„ ìƒì„¸:\n"
            for model, counts in summary["provider_counts"].items():
                text += f"  â€¢ {model}: {counts['total_tokens']:,} í† í°\n"

        return text

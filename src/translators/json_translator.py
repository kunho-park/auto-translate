import asyncio
import json
import logging
import os
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional

import regex as re
from langchain_core.language_models import BaseLLM
from langchain_core.output_parsers import PydanticOutputParser
from langgraph.graph import END, StateGraph

from src.localization import get_message as _m
from src.prompts.llm_prompts import (
    contextual_terms_prompt,
    final_fallback_prompt,
    quality_retranslation_prompt,
    retry_contextual_terms_prompt,
    retry_translation_prompt,
    translation_prompt,
)
from src.translators.multi_llm_manager import MultiLLMManager
from src.translators.token_counter import UniversalTokenCountingHandler

logger = logging.getLogger(__name__)

__all__ = [
    "TranslatorState",
    "TranslatedItem",
    "TermMeaning",
    "GlossaryEntry",
    "Glossary",
    "TranslationPair",
    "TranslationResult",
    "QualityIssue",
    "QualityReview",
    "JSONTranslator",
    "run_example",
]

###############################################################################
# 1. State and data-model definitions                                        #
###############################################################################


# ---------------------------------------------------------------------------
# Externalised data models & utilities (moved to separate modules for clarity)
# ---------------------------------------------------------------------------
from .models import (
    Glossary,
    GlossaryEntry,
    QualityIssue,
    QualityReview,
    SimpleGlossaryTerm,
    TermMeaning,
    TranslatedItem,
    TranslationPair,
    TranslationResult,
    TranslatorState,
)
from .utils import (
    PlaceholderManager,
    RequestDelayManager,
    TokenOptimizer,
    is_korean_text,
)

###############################################################################
# 2. Utility helpers                                                          #
###############################################################################


###############################################################################
# 3. LangGraph node functions (async)                                         #
###############################################################################


async def invoke_with_structured_output_fallback(llm_client: BaseLLM, schema, prompt):
    """êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ LLM í˜¸ì¶œ, ì‹¤íŒ¨ ì‹œ PydanticOutputParserë¡œ í´ë°±"""

    # ì²« ë²ˆì§¸ ì‹œë„: with_structured_output ì‚¬ìš©
    try:
        structured_llm = llm_client.with_structured_output(schema, include_raw=True)

        response = await structured_llm.ainvoke(prompt)
        result = response["parsed"]

        # ê²°ê³¼ê°€ Noneì´ ì•„ë‹Œì§€ í™•ì¸
        if result is not None:
            logger.debug("êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„±ê³µ")
            return result
        else:
            logger.warning(
                "êµ¬ì¡°í™”ëœ ì¶œë ¥ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤. PydanticOutputParserë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤."
            )
            raise ValueError("êµ¬ì¡°í™”ëœ ì¶œë ¥ ê²°ê³¼ê°€ None")

    except Exception as e:
        logger.warning(f"êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‹¤íŒ¨: {e}. PydanticOutputParserë¡œ í´ë°±í•©ë‹ˆë‹¤.")

    # ë‘ ë²ˆì§¸ ì‹œë„: PydanticOutputParser ì‚¬ìš©
    try:
        parser = PydanticOutputParser(pydantic_object=schema)

        # í”„ë¡¬í”„íŠ¸ì— íŒŒì„œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
        enhanced_prompt = f"{prompt}\n\n{parser.get_format_instructions()}"

        response = await llm_client.ainvoke(enhanced_prompt)
        result = parser.parse(response.content)

        if result is not None:
            logger.debug("PydanticOutputParserë¡œ íŒŒì‹± ì„±ê³µ")
            return result
        else:
            logger.error("PydanticOutputParser ê²°ê³¼ë„ Noneì…ë‹ˆë‹¤.")
            return None

    except Exception as e:
        logger.error(f"PydanticOutputParserë„ ì‹¤íŒ¨: {e}")
        return None


def _filter_relevant_glossary_terms(
    chunk: List[Dict[str, str]], all_glossary_terms: List[GlossaryEntry]
) -> List[GlossaryEntry]:
    """í•´ë‹¹ ì²­í¬ì— í¬í•¨ëœ ìš©ì–´ë“¤ë§Œ ê¸€ë¡œì‹œë¦¬ì—ì„œ í•„í„°ë§"""
    if not all_glossary_terms:
        return []

    # ì²­í¬ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    chunk_text = " ".join(item["original"].lower() for item in chunk)

    # ì²­í¬ì— í¬í•¨ëœ ìš©ì–´ë“¤ë§Œ í•„í„°ë§
    relevant_terms = []
    for term in all_glossary_terms:
        # ì›ë³¸ ìš©ì–´ê°€ ì²­í¬ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if term.original.lower() in chunk_text:
            relevant_terms.append(term)

    return relevant_terms


async def parse_and_extract_node(state: TranslatorState) -> TranslatorState:  # noqa: D401
    try:
        # ìƒˆë¡œìš´ ë²ˆì—­ ì‘ì—… ì‹œì‘ ì‹œ ID ë° í”Œë ˆì´ìŠ¤í™€ë” ì¹´ìš´í„° ë¦¬ì…‹
        TokenOptimizer.reset_id_counter()
        PlaceholderManager.reset_counter()

        placeholders: Dict[str, str] = {}
        json_with_placeholders = PlaceholderManager.process_json_object(
            state["parsed_json"], placeholders
        )
        state["placeholders"] = placeholders

        # ì´ë¯¸ ë²ˆì—­ëœ í•­ëª©ë“¤ì„ ì œì™¸í•˜ëŠ” ë¡œì§ ì¶”ê°€
        existing_translations = state.get("existing_translations", {})

        # ì „ì²´ í…ìŠ¤íŠ¸ í•­ëª© ìˆ˜ë¥¼ ë¨¼ì € ê³„ì‚°
        original_text_count = len(
            TokenOptimizer.optimize_json_for_translation(json_with_placeholders)
        )

        id_to_text: Dict[str, str] = {}
        json_with_ids = TokenOptimizer.replace_text_with_ids_selective(
            json_with_placeholders, id_to_text, existing_translations
        )

        state["id_to_text_map"] = id_to_text
        state["processed_json"] = json_with_ids

        logger.info(_m("translator.found_items", count=len(id_to_text)))

        # ì´ë¯¸ ë²ˆì—­ëœ í•­ëª© ê°œìˆ˜ ë¡œê¹…
        if existing_translations:
            already_translated_count = original_text_count - len(id_to_text)
            if already_translated_count > 0:
                logger.info(
                    f"ì´ë¯¸ ë²ˆì—­ëœ í•­ëª© {already_translated_count}ê°œë¥¼ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤."
                )

        return state
    except Exception as exc:
        state["error"] = f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {exc}"
        return state


async def extract_terms_from_json_chunks_node(
    state: TranslatorState,
) -> TranslatorState:
    """Extracts terms by analyzing the full JSON in chunks for contextual accuracy."""
    try:
        # ì—¬ëŸ¬ ì‚¬ì „ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë³‘í•©
        vanilla_glossary = state.get("vanilla_glossary", [])
        primary_glossary = state.get("primary_glossary", [])
        existing_important_terms = state.get("important_terms", [])

        merged_glossary: Dict[str, GlossaryEntry] = {}

        # 1. ê¸°ì¡´ ì¤‘ìš” ìš©ì–´ë“¤ ì¶”ê°€ (ê¸°ë³¸ ì‚¬ì „)
        for term in existing_important_terms:
            merged_glossary[term.original.lower()] = term

        # 2. ë°”ë‹ë¼ ì‚¬ì „ ìš©ì–´ë“¤ ë³‘í•© (ìµœê³  ìš°ì„ ìˆœìœ„)
        vanilla_added_count = 0
        for vanilla_term in vanilla_glossary:
            key = vanilla_term.original.lower()
            if key in merged_glossary:
                # ê¸°ì¡´ ìš©ì–´ì— ë°”ë‹ë¼ ì˜ë¯¸ë“¤ ì¶”ê°€ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                # ë°”ë‹ë¼ ì˜ë¯¸ë¥¼ ì•ìª½ì— ì¶”ê°€í•˜ê³  ì¤‘ë³µ ì œê±°
                vanilla_meanings = TokenOptimizer.deduplicate_glossary_meanings(
                    vanilla_term.meanings
                )
                existing_meanings = merged_glossary[key].meanings

                # ë°”ë‹ë¼ ì˜ë¯¸ë¥¼ ì•ì—, ê¸°ì¡´ ì˜ë¯¸ë¥¼ ë’¤ì— ë°°ì¹˜í•œ í›„ ì¤‘ë³µ ì œê±°
                combined_meanings = vanilla_meanings + existing_meanings
                merged_glossary[
                    key
                ].meanings = TokenOptimizer.deduplicate_glossary_meanings(
                    combined_meanings
                )
            else:
                # ìƒˆë¡œìš´ ë°”ë‹ë¼ ìš©ì–´ ì¶”ê°€ (ì˜ë¯¸ ì¤‘ë³µ ì œê±°)
                deduplicated_term = GlossaryEntry(
                    original=vanilla_term.original,
                    meanings=TokenOptimizer.deduplicate_glossary_meanings(
                        vanilla_term.meanings
                    ),
                )
                merged_glossary[key] = deduplicated_term
                vanilla_added_count += 1

        # 3. 1ì°¨ ì‚¬ì „ ìš©ì–´ë“¤ ë³‘í•© (ê¸°ì¡´ ë²ˆì—­ ë°ì´í„°)
        primary_added_count = 0
        for primary_term in primary_glossary:
            key = primary_term.original.lower()
            if key in merged_glossary:
                # ê¸°ì¡´ ìš©ì–´ì— ìƒˆë¡œìš´ ì˜ë¯¸ë“¤ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                merged_glossary[
                    key
                ].meanings = TokenOptimizer.merge_glossary_entry_meanings(
                    merged_glossary[key].meanings, primary_term.meanings
                )
            else:
                # ìƒˆë¡œìš´ ìš©ì–´ ì¶”ê°€ (ì˜ë¯¸ ì¤‘ë³µ ì œê±°)
                deduplicated_term = GlossaryEntry(
                    original=primary_term.original,
                    meanings=TokenOptimizer.deduplicate_glossary_meanings(
                        primary_term.meanings
                    ),
                )
                merged_glossary[key] = deduplicated_term
                primary_added_count += 1

        # ë³‘í•©ëœ ì‚¬ì „ì„ ìƒíƒœì— ì €ì¥
        state["important_terms"] = list(merged_glossary.values())

        # ë³‘í•© ê²°ê³¼ ë¡œê¹…
        if vanilla_added_count > 0:
            logger.info(
                f"ë°”ë‹ë¼ ì‚¬ì „ì—ì„œ {vanilla_added_count}ê°œ ìƒˆë¡œìš´ ìš©ì–´ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤."
            )
        if primary_added_count > 0:
            logger.info(
                f"1ì°¨ ì‚¬ì „ì—ì„œ {primary_added_count}ê°œ ìƒˆë¡œìš´ ìš©ì–´ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤."
            )

        all_texts = "\n".join(state["id_to_text_map"].values())
        if not all_texts:
            logger.info(_m("translator.contextual_terms_no_new"))
            return state

        # Rough chunking by character count; can be improved
        chunk_size = 2000
        # í…ìŠ¤íŠ¸ë¥¼ \nìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì²­í¬ í¬ê¸°ì— ë§ê²Œ ë¶„í• 
        chunks = []
        lines = all_texts.split("\n")
        current_chunk = []
        current_size = 0

        placeholder_pattern = r"\[(P\d{3,}|NEWLINE)\]"

        for line in lines:
            # placeholder íŒ¨í„´ì„ ë¹ˆ ë¬¸ìì—´ë¡œ êµì²´
            line = re.sub(placeholder_pattern, "", line)

            # ë¹ˆ ë¬¸ìì—´ì´ë©´ ê±´ë„ˆë›°ê¸°
            if not line.strip():
                continue

            # í˜„ì¬ ë¼ì¸ì„ ì¶”ê°€í–ˆì„ ë•Œ ì²­í¬ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
            line_size = len(line)
            if current_size + line_size > chunk_size and current_chunk:
                # í˜„ì¬ ì²­í¬ë¥¼ ì™„ì„±í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                chunks.append("\n".join(current_chunk))
                current_chunk = [line]
                current_size = line_size
            else:
                # í˜„ì¬ ì²­í¬ì— ë¼ì¸ ì¶”ê°€
                current_chunk.append(line)
                current_size += line_size + 1  # +1 for \n

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append("\n".join(current_chunk))

        logger.info(_m("translator.contextual_terms_start", count=len(chunks)))

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ìš©ì–´ ì¶”ì¶œ ì‹œì‘)
        progress_callback = state.get("progress_callback")
        if progress_callback:
            base_msg = (
                f"ğŸ” {len(chunks)}ê°œ JSON ì²­í¬ì˜ ë¬¸ë§¥ì„ ë¶„ì„í•˜ì—¬ ìš©ì–´ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤"
            )
            if primary_added_count > 0:
                base_msg += f" (1ì°¨ ì‚¬ì „ {primary_added_count}ê°œ ìš©ì–´ í¬í•¨)"
            progress_callback(
                base_msg,
                0,
                len(chunks),
                f"ì´ {len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì˜ˆì •",
            )

        sem = asyncio.Semaphore(state["max_concurrent_requests"])

        # ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì™„ì „ ë³‘ë ¬ ëŒ€ì‹ )
        tasks = []
        for chunk_idx, chunk in enumerate(chunks):
            tasks.append(
                _extract_terms_from_chunk_worker_with_progress(
                    text_chunk=chunk,
                    target_language=state["target_language"],
                    semaphore=sem,
                    chunk_idx=chunk_idx,
                    total_chunks=len(chunks),
                    progress_callback=progress_callback,
                    max_retries=state.get(
                        "max_retries", 3
                    ),  # ë©”ì¸ ì„¤ì •ì˜ ì¬ì‹œë„ íšŸìˆ˜ ì‚¬ìš©
                    existing_glossary=list(
                        merged_glossary.values()
                    ),  # 1ì°¨ ì‚¬ì „ì„ LLMì— ì œê³µ
                    llm_client=state.get("llm_client"),  # LLM í´ë¼ì´ì–¸íŠ¸ ì „ë‹¬
                    state=state,  # ë‹¤ì¤‘ API í‚¤ ì§€ì›ì„ ìœ„í•œ ìƒíƒœ ì „ë‹¬
                )
            )

        glossaries = await asyncio.gather(*tasks)

        # Merge glossaries from all chunks
        new_terms_count = 0

        for glossary in glossaries:
            for term in glossary.terms:
                key = term.original.lower()
                if key not in merged_glossary:
                    # ìƒˆë¡œìš´ ìš©ì–´ ì¶”ê°€ (ì˜ë¯¸ ì¤‘ë³µ ì œê±°)
                    deduplicated_term = GlossaryEntry(
                        original=term.original,
                        meanings=TokenOptimizer.deduplicate_glossary_meanings(
                            term.meanings
                        ),
                    )
                    merged_glossary[key] = deduplicated_term
                    new_terms_count += 1
                else:
                    # ê¸°ì¡´ ìš©ì–´ì— ìƒˆë¡œìš´ ì˜ë¯¸ë“¤ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                    merged_glossary[
                        key
                    ].meanings = TokenOptimizer.merge_glossary_entry_meanings(
                        merged_glossary[key].meanings, term.meanings
                    )

        state["important_terms"] = list(merged_glossary.values())

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ìš©ì–´ ì¶”ì¶œ ì™„ë£Œ)
        if progress_callback:
            progress_callback(
                "ğŸ” ìš©ì–´ ì¶”ì¶œ ì™„ë£Œ",
                len(chunks),
                len(chunks),
                f"LLM ë¶„ì„ìœ¼ë¡œ ìƒˆë¡œìš´ ìš©ì–´ {new_terms_count}ê°œ ì¶”ê°€ë¨"
                if new_terms_count > 0
                else "LLM ë¶„ì„ ì™„ë£Œ, ìƒˆë¡œìš´ ìš©ì–´ ì—†ìŒ",
            )

        if new_terms_count > 0:
            logger.info(_m("translator.contextual_terms_finish", count=new_terms_count))
        else:
            logger.info(_m("translator.contextual_terms_no_new"))

        total_terms = len(merged_glossary)
        logger.info(
            f"ìµœì¢… ì‚¬ì „ í¬ê¸°: {total_terms}ê°œ ìš©ì–´ (1ì°¨ ì‚¬ì „: {len(primary_glossary)}ê°œ, LLM ì¶”ê°€: {new_terms_count}ê°œ)"
        )

        return state

    except Exception as exc:
        logger.error(_m("translator.contextual_terms_main_error", error=exc))
        # Non-critical, proceed without new terms
        return state


async def _extract_terms_from_chunk_worker_with_progress(
    *,
    text_chunk: str,
    target_language: str,
    semaphore: asyncio.Semaphore,
    chunk_idx: int,
    total_chunks: int,
    progress_callback: Optional[callable] = None,
    max_retries: int = 3,
    existing_glossary: List[GlossaryEntry] = None,
    llm_client: Any = None,
    state: TranslatorState = None,
) -> Glossary:
    """Worker to extract glossary terms from a single JSON chunk with progress reporting and retry logic."""

    if existing_glossary is None:
        existing_glossary = []

    async with semaphore:
        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì²­í¬ ì²˜ë¦¬ ì‹œì‘)
        if progress_callback:
            progress_callback(
                "ğŸ” JSON ì²­í¬ ë¶„ì„ ì¤‘",
                chunk_idx,
                total_chunks,
                f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ìš©ì–´ ì¶”ì¶œ ì¤‘",
            )

        # 1ì°¨ ì‚¬ì „ ì •ë³´ë¥¼ LLMì— ì œê³µí•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
        existing_glossary_text = ""
        if existing_glossary:
            # ì²­í¬ í…ìŠ¤íŠ¸ì— ì‹¤ì œë¡œ í¬í•¨ëœ ìš©ì–´ë“¤ë§Œ í•„í„°ë§
            chunk_text_lower = text_chunk.lower()
            relevant_existing_terms = [
                term
                for term in existing_glossary
                if term.original.lower() in chunk_text_lower
            ]

            if relevant_existing_terms:
                existing_glossary_text = TokenOptimizer.format_glossary_for_llm(
                    relevant_existing_terms
                )

        # ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
        last_error = None
        for attempt in range(max_retries + 1):  # 0ë²ˆ ì‹œë„ë¶€í„° max_retriesê¹Œì§€
            try:
                if attempt > 0:
                    prompt = retry_contextual_terms_prompt(
                        target_language, text_chunk, existing_glossary_text
                    )
                else:
                    prompt = contextual_terms_prompt(
                        target_language, text_chunk, existing_glossary_text
                    )
                # ì¬ì‹œë„ ì‹œì—ëŠ” temperatureë¥¼ ì¡°ê¸ˆì”© ì˜¬ë¦¼ (ìµœëŒ€ 1.0ê¹Œì§€)
                temperature = 0 if attempt == 0 else min(1.0, attempt * 0.1)

                # LLM í´ë¼ì´ì–¸íŠ¸ ê²€ì¦
                if llm_client is None:
                    logger.error("LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    return Glossary(terms=[])

                # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
                current_llm = llm_client
                if (
                    state
                    and state.get("use_multi_api_keys")
                    and state.get("multi_llm_manager")
                ):
                    multi_manager = state["multi_llm_manager"]
                    fresh_client = await multi_manager.get_client()
                    if fresh_client:
                        # í† í° ì¹´ìš´í„° ì½œë°± ì¶”ê°€
                        try:
                            token_counter = (
                                state.get("token_counter") if state else None
                            )
                            if token_counter and hasattr(fresh_client, "callbacks"):
                                if fresh_client.callbacks is None:
                                    fresh_client.callbacks = []
                                if token_counter not in fresh_client.callbacks:
                                    fresh_client.callbacks.append(token_counter)
                        except Exception:
                            pass

                        current_llm = fresh_client
                        logger.debug(
                            f"ìš©ì–´ ì¶”ì¶œ ì²­í¬ {chunk_idx + 1}: ë‹¤ì¤‘ API í‚¤ì—ì„œ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                        )
                    else:
                        logger.warning(
                            f"ìš©ì–´ ì¶”ì¶œ ì²­í¬ {chunk_idx + 1}: ë‹¤ì¤‘ API í‚¤ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨, ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                        )

                llm = current_llm
                if attempt > 0:
                    logger.info(
                        f"ğŸ”„ ì²­í¬ {chunk_idx + 1} ìš©ì–´ ì¶”ì¶œ ì¬ì‹œë„ {attempt}/{max_retries} (temperature={temperature})"
                    )
                    # ì¬ì‹œë„ ì‹œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    if progress_callback:
                        progress_callback(
                            "ğŸ” JSON ì²­í¬ ë¶„ì„ ì¤‘",
                            chunk_idx,
                            total_chunks,
                            f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ì¬ì‹œë„ ì¤‘ ({attempt}/{max_retries})",
                        )

                # SimpleGlossaryTermì„ ë„êµ¬ë¡œ ë°”ì¸ë”©í•˜ì—¬ LLM í˜¸ì¶œ
                llm_with_tools = llm.bind_tools([SimpleGlossaryTerm])
                response = await llm_with_tools.ainvoke(prompt)

                # LLMì˜ ë„êµ¬ í˜¸ì¶œì—ì„œ SimpleGlossaryTerm ì¶”ì¶œ
                simple_terms: List[SimpleGlossaryTerm] = []
                if response.tool_calls:
                    for tool_call in response.tool_calls:
                        if tool_call["name"] == "SimpleGlossaryTerm":
                            try:
                                term = SimpleGlossaryTerm(**tool_call["args"])
                                simple_terms.append(term)
                            except Exception as e:
                                logger.warning(
                                    f"SimpleGlossaryTerm íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì¬ì‹œë„ë¥¼ ìœ ë°œí•©ë‹ˆë‹¤: {e}, args: {tool_call['args']}"
                                )
                                raise

                # SimpleGlossaryTerm ë¦¬ìŠ¤íŠ¸ë¥¼ GlossaryEntry ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì§‘ê³„)
                aggregated_entries: Dict[str, GlossaryEntry] = {}
                for term in simple_terms:
                    key = term.original.lower()
                    if key not in aggregated_entries:
                        aggregated_entries[key] = GlossaryEntry(
                            original=term.original, meanings=[]
                        )

                    aggregated_entries[key].meanings.append(
                        TermMeaning(translation=term.translation, context=term.context)
                    )

                result = Glossary(terms=list(aggregated_entries.values()))

                # ì„±ê³µ ì‹œ ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
                if progress_callback:
                    success_msg = f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ì™„ë£Œ"
                    if attempt > 0:
                        success_msg += f" (ì¬ì‹œë„ {attempt}íšŒ í›„ ì„±ê³µ)"
                    success_msg += (
                        f" - {len(result.terms) if result else 0}ê°œ ìš©ì–´ ë°œê²¬"
                    )

                    progress_callback(
                        "ğŸ” JSON ì²­í¬ ë¶„ì„ ì¤‘",
                        chunk_idx + 1,
                        total_chunks,
                        success_msg,
                    )

                if attempt > 0:
                    logger.info(f"âœ… ì²­í¬ {chunk_idx + 1} ìš©ì–´ ì¶”ì¶œ ì¬ì‹œë„ ì„±ê³µ")

                return result or Glossary(terms=[])

            except Exception as exc:
                last_error = exc
                logger.warning(
                    f"âš ï¸ ì²­í¬ {chunk_idx + 1} ìš©ì–´ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries + 1}): {exc}"
                )

                # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ í•´ë‹¹ í‚¤ì˜ ì‹¤íŒ¨ë¥¼ ê¸°ë¡
                if (
                    state
                    and state.get("use_multi_api_keys")
                    and state.get("multi_llm_manager")
                ):
                    logger.debug(f"ìš©ì–´ ì¶”ì¶œ ì²­í¬ {chunk_idx + 1}: API í‚¤ ì‹¤íŒ¨ ê¸°ë¡ë¨")

                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì ì‹œ ëŒ€ê¸°
                if attempt < max_retries:
                    await asyncio.sleep(
                        min(2.0, (attempt + 1) * 0.5)
                    )  # 0.5ì´ˆ, 1ì´ˆ, 1.5ì´ˆ, 2ì´ˆ ëŒ€ê¸°

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ
        logger.error(
            f"âŒ ì²­í¬ {chunk_idx + 1} ìš©ì–´ ì¶”ì¶œ {max_retries + 1}íšŒ ëª¨ë‘ ì‹¤íŒ¨: {last_error}"
        )

        # ì‹¤íŒ¨í•´ë„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if progress_callback:
            progress_callback(
                "ğŸ” JSON ì²­í¬ ë¶„ì„ ì¤‘",
                chunk_idx + 1,
                total_chunks,
                f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ì‹¤íŒ¨ (ì¬ì‹œë„ {max_retries}íšŒ í›„)",
            )

        return Glossary(terms=[])


def should_create_glossary(state: TranslatorState) -> str:
    """Determines whether to proceed with glossary creation."""
    if state.get("use_glossary"):
        return "create_glossary"
    logger.info(_m("translator.skipping_glossary"))
    return "skip_glossary"


def should_save_glossary(state: TranslatorState) -> str:
    """Determines whether to save the glossary at the end."""
    if state.get("use_glossary") and state.get("glossary_path"):
        return "save_glossary"
    return "end"


async def smart_translate_node(state: TranslatorState) -> TranslatorState:
    try:
        # LLM í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        llm = state.get("llm_client")
        if llm is None:
            logger.error("LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            state["error"] = "LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            return state
        id_map = state["id_to_text_map"]

        if not id_map:
            state["translation_map"] = {}
            return state

        items = [{"id": k, "original": v} for k, v in id_map.items()]
        chunks = TokenOptimizer.create_text_chunks(items, state["max_tokens_per_chunk"])

        logger.info(
            _m(
                "translator.chunks_split",
                chunks=len(chunks),
                concurrent=state["max_concurrent_requests"],
            )
        )

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ë²ˆì—­ ì‹œì‘)
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ“ ë²ˆì—­ ì§„í–‰ ì¤‘", 0, len(chunks), f"ì´ {len(chunks)}ê°œ ì²­í¬ ë²ˆì—­ ì‹œì‘"
            )

        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])
        sem = asyncio.Semaphore(state["max_concurrent_requests"])

        tasks = [
            _translate_chunk_worker_with_progress(
                chunk=c,
                state=state,
                llm=llm,
                target_language=state["target_language"],
                delay_manager=delay_mgr,
                semaphore=sem,
                chunk_num=i,
                total_chunks=len(chunks),
                progress_callback=progress_callback,
            )
            for i, c in enumerate(chunks, 1)
        ]
        results = await asyncio.gather(*tasks)

        translation_map: Dict[str, str] = {}
        for res in results:
            for item in res:
                translation_map[item.id] = item.translated

        state["translation_map"] = translation_map

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ë²ˆì—­ ì™„ë£Œ)
        if progress_callback:
            progress_callback(
                "ğŸ“ ë²ˆì—­ ì™„ë£Œ",
                len(chunks),
                len(chunks),
                f"ì´ {len(translation_map)}ê°œ í•­ëª© ë²ˆì—­ ì™„ë£Œ",
            )

        return state
    except Exception as exc:
        state["error"] = f"ë²ˆì—­ ì˜¤ë¥˜: {exc}"
        return state


async def _translate_chunk_worker_with_progress(
    *,
    chunk: List[Dict[str, str]],
    state: TranslatorState,
    llm: Any,
    target_language: str,
    delay_manager: RequestDelayManager,
    semaphore: asyncio.Semaphore,
    chunk_num: int,
    total_chunks: int,
    progress_callback: Optional[callable] = None,
    is_retry: bool = False,
    temperature: float = 0.0,
) -> List[TranslatedItem]:
    """ë²ˆì—­ ì²­í¬ ì›Œì»¤ (ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ í¬í•¨)"""
    # ì „ì²´ ê¸€ë¡œì‹œë¦¬ì—ì„œ ì´ ì²­í¬ì— ê´€ë ¨ëœ ìš©ì–´ë“¤ë§Œ í•„í„°ë§
    all_glossary_terms = state.get("important_terms", [])
    relevant_glossary = _filter_relevant_glossary_terms(chunk, all_glossary_terms)

    log_prefix = "retry" if is_retry else "translation"
    async with semaphore:
        await delay_manager.wait()

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì²­í¬ ë²ˆì—­ ì‹œì‘)
        if progress_callback:
            progress_callback(
                "ğŸ“ ë²ˆì—­ ì§„í–‰ ì¤‘",
                chunk_num - 1,
                total_chunks,
                f"ì²­í¬ {chunk_num}/{total_chunks} ë²ˆì—­ ì¤‘ ({len(chunk)}ê°œ í•­ëª©)",
            )

        logger.info(
            _m(
                "translator.chunk_start",
                current=chunk_num,
                total=total_chunks,
                kind=log_prefix,
            )
        )

        # ê¸€ë¡œì‹œë¦¬ í•„í„°ë§ ê²°ê³¼ ë¡œê¹…
        if all_glossary_terms:
            logger.info(
                f"ğŸ“š ì²­í¬ {chunk_num}: ì „ì²´ ê¸€ë¡œì‹œë¦¬ {len(all_glossary_terms)}ê°œ ì¤‘ "
                f"{len(relevant_glossary)}ê°œ ìš©ì–´ í¬í•¨"
            )

        glossary_str = TokenOptimizer.format_glossary_for_llm(relevant_glossary)
        chunk_str = TokenOptimizer.format_chunk_for_llm(chunk)

        # ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì¬ì‹œë„ ì—¬ë¶€ ë°˜ì˜)
        if is_retry:
            prompt = retry_translation_prompt(
                state["target_language"], glossary_str, chunk_str
            )
        else:
            prompt = translation_prompt(
                state["target_language"], glossary_str, chunk_str
            )

        try:
            # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            current_llm = llm
            if state.get("use_multi_api_keys") and state.get("multi_llm_manager"):
                multi_manager = state["multi_llm_manager"]
                fresh_client = await multi_manager.get_client()
                if fresh_client:
                    current_llm = fresh_client
                    logger.debug(
                        f"ì²­í¬ {chunk_num}: ë‹¤ì¤‘ API í‚¤ì—ì„œ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                    )
                else:
                    logger.warning(
                        f"ì²­í¬ {chunk_num}: ë‹¤ì¤‘ API í‚¤ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨, ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                    )

            # TranslatedItemì„ ë„êµ¬ë¡œ ë°”ì¸ë”©í•˜ì—¬ LLM í˜¸ì¶œ
            llm_with_tools = current_llm.bind_tools([TranslatedItem])
            response = await llm_with_tools.ainvoke(prompt)

            # LLMì˜ ë„êµ¬ í˜¸ì¶œì—ì„œ TranslatedItem ì¶”ì¶œ
            translations = []
            if response.tool_calls:
                for tool_call in response.tool_calls:
                    if tool_call["name"] == "TranslatedItem":
                        try:
                            item = TranslatedItem(**tool_call["args"])
                            # ID íŒ¨í„´(T###) ê·¸ëŒ€ë¡œ ë°˜í™˜ë˜ëŠ” ê²½ìš° í•„í„°ë§
                            if re.match(r"^T\d{3,}$", item.translated.strip()):
                                logger.debug(
                                    f"TranslatedItem returned ID unchanged for {item.id}, dropping."
                                )
                            else:
                                translations.append(item)
                        except Exception as e:
                            logger.warning(
                                f"TranslatedItem íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}, args: {tool_call['args']}"
                            )

            # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì²­í¬ ë²ˆì—­ ì™„ë£Œ)
            if progress_callback:
                progress_callback(
                    "ğŸ“ ë²ˆì—­ ì§„í–‰ ì¤‘",
                    chunk_num,
                    total_chunks,
                    f"ì²­í¬ {chunk_num}/{total_chunks} ì™„ë£Œ ({len(translations)}ê°œ í•­ëª©)",
                )

            return translations
        except Exception as exc:
            logger.error(f"ì²­í¬ {chunk_num} ë²ˆì—­ ì‹¤íŒ¨: {exc}")

            # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ í•´ë‹¹ í‚¤ì˜ ì‹¤íŒ¨ë¥¼ ê¸°ë¡
            if state.get("use_multi_api_keys") and state.get("multi_llm_manager"):
                # ì‹¤íŒ¨í•œ í‚¤ ì •ë³´ëŠ” MultiLLMManagerì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
                logger.debug(f"ì²­í¬ {chunk_num}: API í‚¤ ì‹¤íŒ¨ ê¸°ë¡ë¨")

            return []


def restore_placeholders_node(state: TranslatorState) -> TranslatorState:  # noqa: D401
    try:
        logger.info("í”Œë ˆì´ìŠ¤í™€ë” ë³µì› ì‹œì‘...")

        placeholders = state["placeholders"]
        newline_value = placeholders.get("[NEWLINE]")

        # Sort placeholders ONCE, excluding newline
        sorted_placeholders = sorted(
            (item for item in placeholders.items() if item[0] != "[NEWLINE]"),
            key=lambda item: (int(item[0][2:-1]) if item[0].startswith("[P") else -1),
            reverse=True,
        )

        # JSON ê°ì²´ ë ˆë²¨ì—ì„œ ì•ˆì „í•˜ê²Œ placeholder ë³µì›
        restored_json = PlaceholderManager.restore_placeholders_in_json(
            state["translated_json"], sorted_placeholders, newline_value
        )

        # ë³µì›ëœ JSON ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        state["final_json"] = json.dumps(restored_json, ensure_ascii=False, indent=2)
        logger.info("í”Œë ˆì´ìŠ¤í™€ë” ë³µì› ì™„ë£Œ.")
        logger.info(_m("translator.placeholders_restore_finish"))
        return state
    except Exception as exc:
        state["error"] = f"Placeholder ë³µì› ì˜¤ë¥˜: {exc}"
        return state


def apply_translations_to_json(json_obj: Any, translation_map: Dict[str, str]) -> Any:  # noqa: D401
    if isinstance(json_obj, dict):
        return {
            k: apply_translations_to_json(v, translation_map)
            for k, v in json_obj.items()
        }
    if isinstance(json_obj, list):
        return [apply_translations_to_json(i, translation_map) for i in json_obj]
    if isinstance(json_obj, str):
        return translation_map.get(json_obj, json_obj)
    return json_obj


async def validation_and_retry_node(state: TranslatorState) -> TranslatorState:  # noqa: D401
    try:
        id_map = state["id_to_text_map"]
        translation_map = state["translation_map"]
        retry_count = state.get("retry_count", 0) + 1
        state["retry_count"] = retry_count

        # ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª© ì°¾ê¸° (ì§€ëŠ¥ì ì¸ ì²´í¬)
        to_retry = []
        placeholder_issues = 0

        for tid, orig in id_map.items():
            translated = translation_map.get(tid, "").strip()
            original = orig.strip()

            # ë²ˆì—­ì´ í•„ìš”í•œ ìƒí™© ì²´í¬
            should_retry = False
            retry_reason = ""

            # 1. ë²ˆì—­ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
            if not translated:
                should_retry = True
                retry_reason = "ë²ˆì—­ ëˆ„ë½"
            # 2. ë²ˆì—­ ê²°ê³¼ê°€ ID íŒ¨í„´(T###)ì¸ ê²½ìš° (ì‹¤ì œ ë²ˆì—­ì´ ì•„ë‹˜)
            elif re.match(r"^T\d{3,}$", translated):
                should_retry = True
                retry_reason = "ID ê·¸ëŒ€ë¡œ ë°˜í™˜"
            # 3. ì›ë³¸ì´ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ë° ë²ˆì—­ì´ ë¹„ì–´ìˆê±°ë‚˜ í”Œë ˆì´ìŠ¤í™€ë”ì¸ ê²½ìš°
            elif original:
                if len(translated) == 0:
                    should_retry = True
                    retry_reason = "ë¹ˆ ë²ˆì—­"
                elif not PlaceholderManager.validate_placeholder_preservation(
                    original, translated
                ):
                    should_retry = True
                    retry_reason = "í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½"
                    placeholder_issues += 1

                    missing_placeholders = PlaceholderManager.get_missing_placeholders(
                        original, translated
                    )
                    logger.debug(
                        f"í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½ ê°ì§€: '{original}' -> '{translated}' "
                        f"(ëˆ„ë½ëœ í”Œë ˆì´ìŠ¤í™€ë”: {missing_placeholders})"
                    )
                elif (
                    original.startswith("[P") and original.endswith("]")
                ) or original == "[NEWLINE]":
                    should_retry = False
                # 3. ë²ˆì—­ ê²°ê³¼ê°€ ì›ë³¸ê³¼ ë™ì¼í•œ ê²½ìš° (ì˜ì–´ë¡œ ìœ ì§€í•´ì•¼ í•˜ëŠ” ê²½ìš° ì œì™¸)
                elif translated == original and len(original) > 3:
                    should_retry = True
                    retry_reason = "ë™ì¼í•œ ê²°ê³¼"
                    logger.debug(
                        f"ë²ˆì—­ ëˆ„ë½ ê°ì§€: '{original}' -> '{translated}' (ë™ì¼í•œ ê²°ê³¼)"
                    )
            if should_retry:
                to_retry.append({"id": tid, "original": orig, "reason": retry_reason})

        if not to_retry:
            logger.info("ëª¨ë“  í•­ëª©ì´ ì„±ê³µì ìœ¼ë¡œ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤.")
            logger.info(_m("translator.items_translated_ok"))
            return state

        # ì¬ì‹œë„ ì´ìœ ë³„ í†µê³„ ê³„ì‚°
        retry_reasons = {}
        for item in to_retry:
            reason = item.get("reason", "ì•Œ ìˆ˜ ì—†ìŒ")
            retry_reasons[reason] = retry_reasons.get(reason, 0) + 1

        logger.warning(
            _m(
                "translator.missing_retry",
                missing=len(to_retry),
                attempt=retry_count,
                max_attempts=state["max_retries"],
            )
        )

        # ì¬ì‹œë„ ì´ìœ ë³„ í†µê³„ ì¶œë ¥
        if retry_reasons:
            logger.warning("ì¬ì‹œë„ ì´ìœ ë³„ í†µê³„:")
            for reason, count in retry_reasons.items():
                logger.warning(f"  - {reason}: {count}ê°œ")

        # í”Œë ˆì´ìŠ¤í™€ë” ì´ìŠˆê°€ ìˆëŠ” ê²½ìš° íŠ¹ë³„íˆ ë¡œê·¸
        if placeholder_issues > 0:
            logger.warning(f"ğŸ”§ í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½ ê°ì§€: {placeholder_issues}ê°œ í•­ëª©")

        # ì¬ì‹œë„í•  í•­ëª©ë“¤ì˜ ID ëª©ë¡ ì €ì¥ (ë””ë²„ê¹…ìš©)
        retry_ids = [item["id"] for item in to_retry]
        logger.debug(f"ì¬ì‹œë„ ëŒ€ìƒ IDë“¤: {retry_ids[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸

        # ë””ë²„ê¹…: ì²˜ìŒ 5ê°œ ì¬ì‹œë„ í•­ëª©ì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥
        if len(to_retry) > 100:  # 100ê°œ ì´ìƒì¼ ë•Œë§Œ ë””ë²„ê¹… ì¶œë ¥
            logger.warning("ğŸ” ì¬ì‹œë„ ëŒ€ìƒ ìƒ˜í”Œ ë¶„ì„:")
            for i, item in enumerate(to_retry[:5]):
                original = item["original"].strip()
                translated = translation_map.get(item["id"], "").strip()
                reason = item.get("reason", "ì•Œ ìˆ˜ ì—†ìŒ")
                logger.warning(
                    f"  ìƒ˜í”Œ {i + 1}: '{original}' -> '{translated}' (ì´ìœ : {reason}, ID: {item['id']})"
                )
            logger.warning(f"  ... ì´ {len(to_retry)}ê°œ í•­ëª© ì¤‘ ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ")

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì¬ì‹œë„ ì‹œì‘)
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ”„ ë²ˆì—­ ì¬ì‹œë„ ì¤‘",
                0,
                len(to_retry),
                f"ë¯¸ë²ˆì—­ í•­ëª© {len(to_retry)}ê°œ ì¬ì‹œë„ ({retry_count}ì°¨ ì‹œë„)",
            )

        # LLM í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        llm = state.get("llm_client")
        if llm is None:
            logger.error("LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            state["error"] = "LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            return state
        retry_chunks = TokenOptimizer.create_text_chunks(
            to_retry, state["max_tokens_per_chunk"]
        )

        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])
        sem = asyncio.Semaphore(state["max_concurrent_requests"])

        # ì¬ì‹œë„ íšŸìˆ˜ì— ë”°ë¼ temperature ë™ì  ì¡°ì • (ìµœëŒ€ 1.0ê¹Œì§€)
        retry_temperature = min(1.0, retry_count * 0.1)
        logger.info(f"ë²ˆì—­ ì¬ì‹œë„ temperature: {retry_temperature}")

        tasks = [
            _translate_chunk_worker_with_progress(
                chunk=c,
                state=state,
                llm=llm,
                target_language=state["target_language"],
                delay_manager=delay_mgr,
                semaphore=sem,
                chunk_num=i,
                total_chunks=len(retry_chunks),
                progress_callback=progress_callback,
                is_retry=True,
                temperature=retry_temperature,  # ì¬ì‹œë„ íšŸìˆ˜ì— ë”°ë¼ ë™ì  ì¡°ì •
            )
            for i, c in enumerate(retry_chunks, 1)
        ]
        retry_results = await asyncio.gather(*tasks)

        # ì¬ì‹œë„ ê²°ê³¼ ì—…ë°ì´íŠ¸ (í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ í¬í•¨)
        retry_count_success = 0
        retry_count_failed = 0
        retry_count_placeholder_fixed = 0

        for res in retry_results:
            for item in res:
                original_text = id_map.get(item.id, "").strip()
                new_translation = item.translated.strip()
                old_translation = translation_map.get(item.id, "").strip()

                # ë²ˆì—­ì´ ìœ íš¨í•œì§€ ì²´í¬ (í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ í¬í•¨)
                is_valid_translation = False
                validation_passed = False

                if new_translation:
                    # 1. ê¸°ë³¸ ë²ˆì—­ ìœ íš¨ì„± ì²´í¬
                    if new_translation != old_translation:
                        # ì´ì „ ë²ˆì—­ê³¼ ë‹¤ë¥´ë©´ ê°œì„ ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                        is_valid_translation = True
                    elif new_translation != original_text:
                        # ì›ë³¸ê³¼ ë‹¤ë¥´ë©´ ìœ íš¨
                        is_valid_translation = True
                    elif len(new_translation) >= 1:
                        # ìµœì†Œ 1ê¸€ì ì´ìƒì´ë©´ ìœ íš¨ (ë„ˆë¬´ ì—„ê²©í–ˆë˜ 2ê¸€ì ì¡°ê±´ ì™„í™”)
                        is_valid_translation = True

                    # 2. í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ (ë” ì¤‘ìš”í•œ ê²€ì¦)
                    if is_valid_translation:
                        if PlaceholderManager.validate_placeholder_preservation(
                            original_text, new_translation
                        ):
                            validation_passed = True

                            # í”Œë ˆì´ìŠ¤í™€ë” ì´ìŠˆê°€ í•´ê²°ëœ ê²½ìš° ì¶”ê°€ ì¹´ìš´íŠ¸
                            if not PlaceholderManager.validate_placeholder_preservation(
                                original_text, old_translation
                            ):
                                retry_count_placeholder_fixed += 1
                                logger.debug(
                                    f"í”Œë ˆì´ìŠ¤í™€ë” ë³µì› ì„±ê³µ: {item.id} -> {new_translation[:50]}..."
                                )
                        else:
                            # í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì—¬ì „íˆ ëˆ„ë½ëœ ê²½ìš°
                            missing_placeholders = (
                                PlaceholderManager.get_missing_placeholders(
                                    original_text, new_translation
                                )
                            )
                            logger.debug(
                                f"ì¬ì‹œë„ í›„ì—ë„ í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½: {item.id} -> '{new_translation}' "
                                f"(ëˆ„ë½: {missing_placeholders})"
                            )
                            validation_passed = False

                if validation_passed:
                    translation_map[item.id] = new_translation
                    retry_count_success += 1
                    logger.debug(f"ì¬ì‹œë„ ì„±ê³µ: {item.id} -> {new_translation[:50]}...")
                else:
                    retry_count_failed += 1
                    logger.debug(
                        f"ì¬ì‹œë„ ì‹¤íŒ¨: {item.id} -> '{new_translation}' (ì›ë³¸: '{original_text}')"
                    )

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì¬ì‹œë„ ì™„ë£Œ)
        if progress_callback:
            status_msg = f"ì¬ì‹œë„ ê²°ê³¼: ì„±ê³µ {retry_count_success}ê°œ, ì‹¤íŒ¨ {retry_count_failed}ê°œ"
            if retry_count_placeholder_fixed > 0:
                status_msg += f", í”Œë ˆì´ìŠ¤í™€ë” ë³µì› {retry_count_placeholder_fixed}ê°œ"
            progress_callback(
                "ğŸ”„ ë²ˆì—­ ì¬ì‹œë„ ì™„ë£Œ",
                len(retry_chunks),
                len(retry_chunks),
                status_msg,
            )

        log_msg = (
            f"ì¬ì‹œë„ ì™„ë£Œ: ì„±ê³µ {retry_count_success}ê°œ, ì‹¤íŒ¨ {retry_count_failed}ê°œ"
        )
        if retry_count_placeholder_fixed > 0:
            log_msg += f", í”Œë ˆì´ìŠ¤í™€ë” ë³µì› {retry_count_placeholder_fixed}ê°œ"
        logger.info(log_msg)

        # ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨í•œ í•­ëª©ë“¤ì´ ìˆìœ¼ë©´ ë¡œê·¸ ì¶œë ¥
        if retry_count_failed > 0:
            logger.warning(
                f"ì¬ì‹œë„ í›„ì—ë„ {retry_count_failed}ê°œ í•­ëª©ì´ ë²ˆì—­ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )

        return state

    except Exception as exc:
        logger.error(f"ì¬ì‹œë„ ì¤‘ ì˜¤ë¥˜: {exc}")
        state["error"] = f"ì¬ì‹œë„ ì¤‘ ì˜¤ë¥˜: {exc}"
        return state


async def _translate_single_item_worker(
    *,
    tid: str,
    state: TranslatorState,
    llm: Any,
    target_language: str,
    delay_manager: RequestDelayManager,
    semaphore: asyncio.Semaphore,
    max_retries: int = 2,
) -> tuple[str, Optional[str]]:
    """ê°œë³„ í•­ëª© ë²ˆì—­ì„ ìœ„í•œ ë¹„ë™ê¸° ì›Œì»¤ (ì¬ì‹œë„ ê¸°ëŠ¥ í¬í•¨)"""
    async with semaphore:
        id_map = state["id_to_text_map"]
        original_text = id_map[tid]

        # ì´ í•­ëª©ì— í•„ìš”í•œ í”Œë ˆì´ìŠ¤í™€ë” ëª©ë¡ ì¶”ì¶œ
        required_placeholders = PlaceholderManager._extract_internal_placeholders(
            original_text
        )
        placeholders_str = (
            "\n".join(f"- `{p}`" for p in required_placeholders)
            if required_placeholders
            else "ì—†ìŒ"
        )

        # ë‹¨ì¼ í•­ëª©ì— ëŒ€í•œ ê¸€ë¡œì‹œë¦¬ ì¶”ì¶œ ë° í¬ë§·íŒ…
        all_glossary_terms = state.get("important_terms", [])
        relevant_glossary = _filter_relevant_glossary_terms(
            [{"original": original_text}], all_glossary_terms
        )
        glossary_str = TokenOptimizer.format_glossary_for_llm(relevant_glossary)

        # ì¬ì‹œë„ ë£¨í”„
        last_error = None
        for attempt in range(max_retries + 1):
            await delay_manager.wait()

            # ìƒˆ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt = final_fallback_prompt(
                language=target_language,
                text_id=tid,
                original_text=original_text,
                placeholders=placeholders_str,
                glossary=glossary_str,
            )

            try:
                # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
                current_llm = llm
                if state.get("use_multi_api_keys") and state.get("multi_llm_manager"):
                    multi_manager = state["multi_llm_manager"]
                    fresh_client = await multi_manager.get_client()
                    if fresh_client:
                        current_llm = fresh_client
                        logger.debug(
                            f"ìµœì¢… ë²ˆì—­ {tid}: ë‹¤ì¤‘ API í‚¤ì—ì„œ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                        )
                    else:
                        logger.warning(
                            f"ìµœì¢… ë²ˆì—­ {tid}: ë‹¤ì¤‘ API í‚¤ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨, ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                        )

                # ì¬ì‹œë„ ì‹œ temperatureë¥¼ ì•½ê°„ ë†’ì—¬ ë‹¤ë¥¸ ê²°ê³¼ ìœ ë„
                temperature = min(1.0, attempt * 0.2)
                configured_llm = current_llm.with_config(
                    configurable={"temperature": temperature}
                )
                llm_with_tools = configured_llm.bind_tools([TranslatedItem])

                response = await llm_with_tools.ainvoke(prompt)

                if response.tool_calls:
                    for tool_call in response.tool_calls:
                        if (
                            tool_call["name"] == "TranslatedItem"
                            and tool_call["args"].get("id") == tid
                        ):
                            item = TranslatedItem(**tool_call["args"])
                            # ìµœì¢… ê²€ì¦: í”Œë ˆì´ìŠ¤í™€ë” ë³´ì¡´ ì—¬ë¶€
                            if PlaceholderManager.validate_placeholder_preservation(
                                original_text, item.translated
                            ):
                                logger.info(
                                    f"âœ… ìµœì¢… ë²ˆì—­ ì¬ì‹œë„ ì„±ê³µ (ì‹œë„ {attempt + 1}): {tid} -> {item.translated[:50]}..."
                                )
                                return tid, item.translated
                            elif re.match(r"^T\d{3,}$", item.translated):
                                last_error = "ID ê·¸ëŒ€ë¡œ ë°˜í™˜"
                                logger.info(
                                    f"âš ï¸ ìµœì¢… ë²ˆì—­ ì¬ì‹œë„({attempt + 1}) í›„ì—ë„ ID ê·¸ëŒ€ë¡œ ë°˜í™˜: {tid} -> {item.translated}"
                                )
                            else:
                                last_error = "í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½"
                                logger.warning(
                                    f"âš ï¸ ìµœì¢… ë²ˆì—­ ì¬ì‹œë„({attempt + 1}) í›„ì—ë„ í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½: {tid} -> '{item.translated}'"
                                )
                else:
                    last_error = "ì‘ë‹µ ì—†ìŒ"
                    # logger.warning(
                    #     f"âš ï¸ ìµœì¢… ë²ˆì—­ ì¬ì‹œë„({attempt + 1}) ì‹¤íŒ¨ (ì‘ë‹µ ì—†ìŒ): {tid}"
                    # )

            except Exception as e:
                last_error = str(e)
                logger.error(
                    f"ğŸš¨ ìµœì¢… ë²ˆì—­ ì¬ì‹œë„({attempt + 1}) API í˜¸ì¶œ ì˜¤ë¥˜ (í•­ëª©: {tid}): {e}"
                )

                # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ í•´ë‹¹ í‚¤ì˜ ì‹¤íŒ¨ë¥¼ ê¸°ë¡
                if state.get("use_multi_api_keys") and state.get("multi_llm_manager"):
                    logger.debug(f"ìµœì¢… ë²ˆì—­ {tid}: API í‚¤ ì‹¤íŒ¨ ê¸°ë¡ë¨")

            # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
            if attempt < max_retries:
                await asyncio.sleep(min(2.0, (attempt + 1) * 0.5))

        logger.error(
            f"âŒ ìµœì¢… ë²ˆì—­ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ({max_retries + 1}íšŒ): {tid}, ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}"
        )
        return tid, None


async def final_fallback_translation_node(state: TranslatorState) -> TranslatorState:
    """ìµœì¢…ì ìœ¼ë¡œ ëˆ„ë½ëœ í•­ëª©ë“¤ì„ í•˜ë‚˜ì”© ë‹¤ì‹œ ë²ˆì—­ ìš”ì²­í•©ë‹ˆë‹¤."""
    try:
        logger.info("ìµœì¢… ë²ˆì—­ ë‹¨ê³„ ì‹œì‘: ëˆ„ë½ëœ í•­ëª©ì„ ê°œë³„ì ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.")
        logger.info(_m("translator.final_translation_start"))

        id_map = state["id_to_text_map"]
        translation_map = state["translation_map"]
        llm = state.get("llm_client")
        if llm is None:
            state["error"] = "LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return state

        # ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª© ì°¾ê¸° (should_retryì™€ ë™ì¼í•œ ë¡œì§)
        untranslated_items = []
        for tid, original_text in id_map.items():
            translated = translation_map.get(tid, "").strip()
            original = original_text.strip()
            needs_translation = False

            if not translated:
                needs_translation = True
            elif original:
                if len(translated) == 0:
                    needs_translation = True
                elif not PlaceholderManager.validate_placeholder_preservation(
                    original, translated
                ):
                    needs_translation = True
                elif (
                    original.startswith("[P") and original.endswith("]")
                ) or original == "[NEWLINE]":
                    needs_translation = False
                elif translated.strip() == original.strip() and len(original) > 3:
                    needs_translation = True

            if needs_translation:
                untranslated_items.append(tid)

        if not untranslated_items:
            logger.info("ëˆ„ë½ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ìµœì¢… ë²ˆì—­ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            logger.info(_m("translator.final_translation_skip_no_missing"))
            return state

        logger.info(
            f"{len(untranslated_items)}ê°œì˜ ëˆ„ë½ëœ í•­ëª©ì— ëŒ€í•œ ê°œë³„ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤."
        )

        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ“ ìµœì¢… ë²ˆì—­ ì¤‘",
                0,
                len(untranslated_items),
                f"ëˆ„ë½ëœ {len(untranslated_items)}ê°œ í•­ëª© ê°œë³„ ë²ˆì—­ ì‹œì‘",
            )

        target_language = state["target_language"]
        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])
        sem = asyncio.Semaphore(state["max_concurrent_requests"])

        final_fallback_max_retries = state.get("final_fallback_max_retries", 4)
        logger.info(f"ê°œë³„ í•­ëª©ë‹¹ ìµœëŒ€ {final_fallback_max_retries}íšŒ ì¬ì‹œë„í•©ë‹ˆë‹¤.")

        # ê°œë³„ ìš”ì²­ì´ì§€ë§Œ, ë™ì‹œì— ì—¬ëŸ¬ ê°œë¥¼ ë³´ë‚´ì„œ ì†ë„ í–¥ìƒ
        tasks = []
        for tid in untranslated_items:
            tasks.append(
                _translate_single_item_worker(
                    tid=tid,
                    state=state,
                    llm=llm,
                    target_language=target_language,
                    delay_manager=delay_mgr,
                    semaphore=sem,
                    max_retries=final_fallback_max_retries,
                )
            )

        results = await asyncio.gather(*tasks)

        count_success = 0
        for i, (tid, new_translation) in enumerate(results):
            if new_translation:
                translation_map[tid] = new_translation
                count_success += 1

            if progress_callback:
                progress_callback(
                    "ğŸ“ ìµœì¢… ë²ˆì—­ ì¤‘",
                    i + 1,
                    len(untranslated_items),
                    f"{i + 1}/{len(untranslated_items)} í•­ëª© ì²˜ë¦¬ë¨",
                )

        logger.info(
            f"ìµœì¢… ë²ˆì—­ ì™„ë£Œ. {count_success}/{len(untranslated_items)}ê°œ í•­ëª© ë²ˆì—­ ì„±ê³µ."
        )

        return state

    except Exception as exc:
        state["error"] = f"ìµœì¢… ê°œë³„ ë²ˆì—­ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {exc}"
        logger.error(f"ìµœì¢… ê°œë³„ ë²ˆì—­ ë‹¨ê³„ ì˜¤ë¥˜: {traceback.format_exc()}")
        return state


def rebuild_json_node(state: TranslatorState) -> TranslatorState:  # noqa: D401
    try:
        logger.info("ê²°ê³¼ JSON ì¬êµ¬ì„± ì‹œì‘...")
        logger.info(_m("translator.rebuild_json_start"))
        id_map = state["translation_map"]

        def replace(obj: Any) -> Any:
            if isinstance(obj, dict):
                return {k: replace(v) for k, v in obj.items()}
            if isinstance(obj, list):
                return [replace(i) for i in obj]
            if isinstance(obj, str):
                # T001, T002 ê°™ì€ IDê°€ translation_mapì— ìˆëŠ”ì§€ í™•ì¸
                if obj in id_map:
                    return id_map[obj]
                # ID íŒ¨í„´ì´ì§€ë§Œ ë²ˆì—­ì´ ì—†ëŠ” ê²½ìš° ê²½ê³ 
                elif re.match(r"^T\d{3,}$", obj):
                    logger.warning(f"ë²ˆì—­ë˜ì§€ ì•Šì€ ID ë°œê²¬: {obj}")
                    # ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ë³µì› ì‹œë„
                    original_text = state["id_to_text_map"].get(obj, obj)
                    logger.warning(f"ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ë³µì›: {obj} -> {original_text}")
                    return original_text
            return obj

        state["translated_json"] = replace(state["processed_json"])
        logger.info("ê²°ê³¼ JSON ì¬êµ¬ì„± ì™„ë£Œ.")
        logger.info(_m("translator.rebuild_json_finish"))
        return state
    except Exception as exc:
        state["error"] = f"JSON ì¬êµ¬ì„± ì˜¤ë¥˜: {exc}"
        return state


def should_retry(state: TranslatorState) -> str:  # noqa: D401
    if state.get("error"):
        logger.error(_m("translator.error_abort", error=state["error"]))
        return "end"

    current_retry = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 3)

    id_map = state.get("id_to_text_map", {})
    translation_map = state.get("translation_map", {})

    if not id_map:
        return "complete"

    # ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª© ì²´í¬ (ì§€ëŠ¥ì ìœ¼ë¡œ, í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ í¬í•¨)
    untranslated_items = []
    placeholder_failed_items = []

    for tid, original_text in id_map.items():
        translated = translation_map.get(tid, "").strip()
        original = original_text.strip()

        # ë²ˆì—­ì´ í•„ìš”í•œ ìƒí™© ì²´í¬
        needs_translation = False

        # 1. ë²ˆì—­ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
        if not translated:
            needs_translation = True
        # 2. ë²ˆì—­ ê²°ê³¼ê°€ ID íŒ¨í„´(T###)ì¸ ê²½ìš° (ì‹¤ì œ ë²ˆì—­ì´ ì•„ë‹˜)
        elif re.match(r"^T\d{3,}$", translated):
            needs_translation = True
        # 3. ì›ë³¸ì´ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ë° ì œëŒ€ë¡œ ë²ˆì—­ë˜ì§€ ì•Šì€ ê²½ìš°
        elif original:
            if len(translated) == 0:
                needs_translation = True
            elif re.match(r"^T\d{3,}$", translated):
                needs_translation = True
            elif not PlaceholderManager.validate_placeholder_preservation(
                original, translated
            ):
                needs_translation = True
                placeholder_failed_items.append(tid)
            elif (
                original.startswith("[P") and original.endswith("]")
            ) or original == "[NEWLINE]":
                needs_translation = False
            elif translated.strip() == original.strip() and len(original) > 3:
                needs_translation = True
        if needs_translation:
            untranslated_items.append(tid)

    needs_retry = len(untranslated_items) > 0

    if needs_retry and current_retry < max_retries:
        logger.info(
            _m(
                "translator.untranslated_retry",
                attempt=current_retry + 1,
                max_attempts=max_retries,
            )
        )
        logger.debug(f"ì¬ì‹œë„ í•„ìš”í•œ í•­ëª© ìˆ˜: {len(untranslated_items)}")
        if placeholder_failed_items:
            logger.debug(f"í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½ í•­ëª© ìˆ˜: {len(placeholder_failed_items)}")
        return "retry"
    elif needs_retry:
        logger.warning(_m("translator.max_retry_reached", max_attempts=max_retries))
        logger.warning(f"ìµœì¢…ì ìœ¼ë¡œ ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª©: {len(untranslated_items)}ê°œ")
        if placeholder_failed_items:
            logger.warning(f"í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½ í•­ëª©: {len(placeholder_failed_items)}ê°œ")
        # ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª©ì´ ìˆì–´ë„ ì™„ë£Œë¡œ ì²˜ë¦¬ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        logger.info("ê°œë³„ í•­ëª© ì¬ë²ˆì—­ì„ ìœ„í•œ ìµœì¢… ë‹¨ê³„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
        logger.info(_m("translator.individual_retranslation_stage"))
        return "final_fallback"
    else:
        logger.info(_m("translator.translation_ok"))
        return "complete"


async def load_vanilla_glossary_node(state: TranslatorState) -> TranslatorState:
    """ë°”ë‹ë¼ ë§ˆì¸í¬ë˜í”„íŠ¸ glossaryë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not state.get("use_vanilla_glossary", False):
        state["vanilla_glossary"] = []
        return state

    vanilla_path = state.get("vanilla_glossary_path", "vanilla_glossary.json")

    # í•œêµ­ì–´ì¸ ê²½ìš° ë¯¸ë¦¬ ì¤€ë¹„ëœ ì‚¬ì „ ê²½ë¡œ ì‚¬ìš©
    target_language = state.get("target_language", "")
    if target_language == "í•œêµ­ì–´" and vanilla_path == "vanilla_glossary.json":
        # ê¸°ë³¸ ê²½ë¡œì¸ ê²½ìš°ë§Œ ë¯¸ë¦¬ ì¤€ë¹„ëœ ì‚¬ì „ìœ¼ë¡œ ë³€ê²½
        preset_path = "src/assets/vanilla_glossary/ko_kr.json"
        if Path(preset_path).exists():
            vanilla_path = preset_path
            logger.info("í•œêµ­ì–´ íƒ€ê²Ÿ ì–¸ì–´ ê°ì§€, ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°”ë‹ë¼ ì‚¬ì „ ì‚¬ìš©")
            logger.info(_m("translator.use_vanilla_glossary"))

    try:
        # ë°”ë‹ë¼ glossary builderë¥¼ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸ (ìˆœí™˜ ì„í¬íŠ¸ ë°©ì§€)
        from .vanilla_glossary_builder import VanillaGlossaryBuilder

        builder = VanillaGlossaryBuilder()

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ® ë°”ë‹ë¼ ì‚¬ì „ ë¡œë“œ ì¤‘",
                0,
                1,
                "ë°”ë‹ë¼ ë§ˆì¸í¬ë˜í”„íŠ¸ ì‚¬ì „ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            )

        vanilla_glossary = await builder.create_or_load_vanilla_glossary(
            glossary_path=vanilla_path,
            force_rebuild=False,  # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±
            max_entries_per_batch=200,
            max_concurrent_requests=3,
            progress_callback=progress_callback,
        )

        state["vanilla_glossary"] = vanilla_glossary

        if progress_callback:
            progress_callback(
                "ğŸ® ë°”ë‹ë¼ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ",
                1,
                1,
                f"ë°”ë‹ë¼ ì‚¬ì „ {len(vanilla_glossary)}ê°œ ìš©ì–´ ë¡œë“œ ì™„ë£Œ",
            )

        logger.info(f"ë°”ë‹ë¼ glossary ë¡œë“œ ì™„ë£Œ: {len(vanilla_glossary)}ê°œ ìš©ì–´")

    except Exception as exc:
        logger.warning(f"ë°”ë‹ë¼ glossary ë¡œë“œ ì‹¤íŒ¨: {exc}")
        state["vanilla_glossary"] = []

    return state


def load_glossary_node(state: TranslatorState) -> TranslatorState:
    """Load existing glossary from the specified file path."""
    path = state.get("glossary_path")
    if path and os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                state["important_terms"] = [GlossaryEntry(**item) for item in data]
                logger.info(
                    _m(
                        "translator.glossary_loaded",
                        count=len(state["important_terms"]),
                        path=path,
                    )
                )
        except (IOError, json.JSONDecodeError, TypeError) as exc:
            logger.warning(_m("translator.glossary_load_error", path=path, error=exc))
            state["important_terms"] = []
    else:
        state["important_terms"] = []
    return state


def save_glossary_node(state: TranslatorState) -> TranslatorState:
    """Save the final glossary to the specified file path."""
    path = state.get("glossary_path")
    glossary = state.get("important_terms")
    if path and glossary:
        try:
            logger.info("ìš©ì–´ì§‘ ì €ì¥ ì‹œì‘...")
            logger.info(_m("translator.glossary_save_start"))
            with open(path, "w", encoding="utf-8") as f:
                json.dump(
                    [term.dict() for term in glossary], f, ensure_ascii=False, indent=2
                )
            logger.info(_m("translator.glossary_saved", count=len(glossary), path=path))
        except IOError as exc:
            logger.error(_m("translator.glossary_save_error", path=path, error=exc))
    return state


def create_primary_glossary_node(state: TranslatorState) -> TranslatorState:
    """ê¸°ì¡´ ë²ˆì—­ ë°ì´í„°ë¡œ 1ì°¨ ì‚¬ì „ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    existing_translations = state.get("existing_translations")
    if not existing_translations:
        logger.info("ê¸°ì¡´ ë²ˆì—­ ë°ì´í„°ê°€ ì—†ì–´ 1ì°¨ ì‚¬ì „ êµ¬ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        state["primary_glossary"] = []
        logger.info(_m("translator.primary_glossary_skip"))
        return state

    logger.info(f"ê¸°ì¡´ ë²ˆì—­ ë°ì´í„° {len(existing_translations)}ê°œë¡œ 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì‹œì‘")

    # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
    progress_callback = state.get("progress_callback")
    if progress_callback:
        progress_callback(
            "ğŸ“– 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì¤‘",
            0,
            len(existing_translations),
            f"ê¸°ì¡´ ë²ˆì—­ ë°ì´í„° {len(existing_translations)}ê°œ ë¶„ì„ ì¤‘",
        )

    primary_terms = []
    processed_count = 0
    korean_translated_count = 0
    valid_term_count = 0

    # ê¸°ì¡´ ë²ˆì—­ ë°ì´í„°ë¥¼ GlossaryEntryë¡œ ë³€í™˜
    for source_text, target_text in existing_translations.items():
        try:
            # íƒ€ê²Ÿ í…ìŠ¤íŠ¸ê°€ í•œê¸€ì¸ì§€ í™•ì¸
            if is_korean_text(target_text):
                korean_translated_count += 1

                # ê°„ë‹¨í•œ ìš©ì–´ ì¶”ì¶œ (ë‹¨ì–´ ë‹¨ìœ„)
                words = source_text.split()
                if len(words) <= 3:  # 3ë‹¨ì–´ ì´í•˜ì˜ ì§§ì€ í‘œí˜„ë§Œ ìš©ì–´ë¡œ ê°„ì£¼
                    valid_term_count += 1

                    # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìš©ì–´ì¸ì§€ í™•ì¸
                    existing_term = None
                    for term in primary_terms:
                        if term.original.lower() == source_text.lower():
                            existing_term = term
                            break

                    if existing_term:
                        # ê¸°ì¡´ ìš©ì–´ì— ìƒˆë¡œìš´ ì˜ë¯¸ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                        new_meaning = TermMeaning(
                            translation=target_text, context="ê¸°ì¡´ ë²ˆì—­"
                        )

                        # ì¤‘ë³µ ì²´í¬ (ë²ˆì—­ë§Œ ë¹„êµ)
                        translation_exists = any(
                            m.translation.lower().strip() == target_text.lower().strip()
                            for m in existing_term.meanings
                        )

                        if not translation_exists:
                            existing_term.meanings.append(new_meaning)
                    else:
                        # ìƒˆë¡œìš´ ìš©ì–´ ì¶”ê°€
                        new_term = GlossaryEntry(
                            original=source_text,
                            meanings=[
                                TermMeaning(
                                    translation=target_text, context="ê¸°ì¡´ ë²ˆì—­"
                                )
                            ],
                        )
                        primary_terms.append(new_term)

            processed_count += 1

            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (500ê°œë§ˆë‹¤)
            if processed_count % 500 == 0 and progress_callback:
                progress_callback(
                    "ğŸ“– 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì¤‘",
                    processed_count,
                    len(existing_translations),
                    f"{processed_count}/{len(existing_translations)} í•­ëª© ì²˜ë¦¬ë¨",
                )

        except Exception as e:
            logger.debug(f"ìš©ì–´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({source_text}): {e}")

    # ìµœì¢…ì ìœ¼ë¡œ ëª¨ë“  ìš©ì–´ì—ì„œ ì¤‘ë³µ ì˜ë¯¸ ì œê±°
    for term in primary_terms:
        term.meanings = TokenOptimizer.deduplicate_glossary_meanings(term.meanings)

    state["primary_glossary"] = primary_terms

    # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì™„ë£Œ)
    if progress_callback:
        progress_callback(
            "ğŸ“– 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ",
            len(existing_translations),
            len(existing_translations),
            f"ì´ {len(primary_terms)}ê°œ ìš©ì–´ê°€ í¬í•¨ëœ 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ",
        )

    logger.info(f"1ì°¨ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ: {len(primary_terms)}ê°œ ìš©ì–´ ìƒì„±")
    logger.info(
        f"í•œê¸€ ë²ˆì—­ í•­ëª©: {korean_translated_count}ê°œ, ìœ íš¨ ìš©ì–´: {valid_term_count}ê°œ"
    )
    return state


async def quality_review_node(state: TranslatorState) -> TranslatorState:
    """ë²ˆì—­ í’ˆì§ˆì„ ê²€í† í•˜ëŠ” ë…¸ë“œ"""
    try:
        # í’ˆì§ˆ ê²€í† ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if not state.get("enable_quality_review", True):
            logger.info("í’ˆì§ˆ ê²€í† ê°€ ë¹„í™œì„±í™”ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            logger.info(_m("translator.quality_review_disabled"))
            return state

        logger.info("ë²ˆì—­ í’ˆì§ˆ ê²€í†  ì‹œì‘...")
        logger.info(_m("translator.quality_review_start"))

        id_map = state["id_to_text_map"]
        translation_map = state["translation_map"]
        llm = state.get("llm_client")

        if not llm or not id_map or not translation_map:
            logger.info("í’ˆì§ˆ ê²€í† ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤ (í•„ìˆ˜ ë°ì´í„° ì—†ìŒ)")
            logger.info(_m("translator.quality_review_skip_missing_data"))
            return state

        # ê²€í† í•  í•­ëª©ë“¤ ì¤€ë¹„ (ë²ˆì—­ëœ ê²ƒë§Œ)
        placeholder_only_pattern = r"^\[(P\d{3,}|NEWLINE)\]$"
        review_items = []
        for tid, original_text in id_map.items():
            translated_text = translation_map.get(tid, "")
            if not translated_text.strip():
                continue

            # ì›ë¬¸ê³¼ ë²ˆì—­ì´ ëª¨ë‘ ë‚´ë¶€ í”Œë ˆì´ìŠ¤í™€ë”ë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê²½ìš° í’ˆì§ˆ ê²€í†  ê±´ë„ˆëœ€
            if re.match(placeholder_only_pattern, original_text.strip()) and re.match(
                placeholder_only_pattern, translated_text.strip()
            ):
                continue

            review_items.append(
                {
                    "id": tid,
                    "original": original_text,
                    "translated": translated_text,
                }
            )

        if not review_items:
            logger.info("ê²€í† í•  ë²ˆì—­ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info(_m("translator.quality_review_no_items"))
            return state

        logger.info(f"í’ˆì§ˆ ê²€í†  ëŒ€ìƒ: {len(review_items)}ê°œ í•­ëª©")

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ” ë²ˆì—­ í’ˆì§ˆ ê²€í†  ì¤‘",
                0,
                len(review_items),
                f"ì´ {len(review_items)}ê°œ í•­ëª© í’ˆì§ˆ ê²€í†  ì‹œì‘",
            )

        # 2000ê¸€ì ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        chunks = _create_quality_review_chunks(review_items, max_chars=4000)
        logger.info(f"í’ˆì§ˆ ê²€í† ë¥¼ ìœ„í•´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")

        # ë™ì‹œ ìš”ì²­ ì œí•œ
        sem = asyncio.Semaphore(state["max_concurrent_requests"])
        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])

        # ê° ì²­í¬ë³„ë¡œ í’ˆì§ˆ ê²€í†  ì‹¤í–‰
        tasks = []
        for chunk_idx, chunk in enumerate(chunks):
            tasks.append(
                _review_chunk_worker(
                    chunk=chunk,
                    target_language=state["target_language"],
                    llm=llm,
                    state=state,
                    semaphore=sem,
                    delay_manager=delay_mgr,
                    chunk_idx=chunk_idx,
                    total_chunks=len(chunks),
                    progress_callback=progress_callback,
                )
            )

        review_results = await asyncio.gather(*tasks)

        # ê²°ê³¼ ì§‘ê³„ - ê° ì²­í¬ì—ì„œ ê°œë³„ QualityIssueë“¤ì„ ìˆ˜ì§‘
        all_issues = []

        for review_result in review_results:
            if review_result:  # review_resultëŠ” List[QualityIssue]
                all_issues.extend(review_result)

        # í’ˆì§ˆ ê²€í†  ê²°ê³¼ ë¡œê¹…
        if all_issues:
            logger.warning(f"ğŸ” í’ˆì§ˆ ê²€í†  ê²°ê³¼: {len(all_issues)}ê°œ ë¬¸ì œ ë°œê²¬")

            # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
            severity_counts = {}
            issue_type_counts = {}

            for issue in all_issues:
                severity_counts[issue.severity] = (
                    severity_counts.get(issue.severity, 0) + 1
                )
                issue_type_counts[issue.issue_type] = (
                    issue_type_counts.get(issue.issue_type, 0) + 1
                )

            logger.warning("ì‹¬ê°ë„ë³„ ë¶„ë¥˜:")
            for severity, count in severity_counts.items():
                logger.warning(f"  - {severity}: {count}ê°œ")

            logger.warning("ë¬¸ì œ ìœ í˜•ë³„ ë¶„ë¥˜:")
            for issue_type, count in issue_type_counts.items():
                logger.warning(f"  - {issue_type}: {count}ê°œ")

            # ì‹¬ê°í•œ ë¬¸ì œë“¤ ìƒì„¸ ë¡œê¹…
            high_severity_issues = [
                issue for issue in all_issues if issue.severity == "high"
            ]
            if high_severity_issues:
                logger.warning(f"ğŸš¨ ì‹¬ê°í•œ ë¬¸ì œ {len(high_severity_issues)}ê°œ:")
                for issue in high_severity_issues[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    logger.warning(
                        f"  - [{issue.text_id}] {issue.issue_type}: {issue.description}"
                    )
                    if issue.suggested_fix:
                        logger.warning(f"    ì œì•ˆ: {issue.suggested_fix}")
        else:
            logger.info("âœ… í’ˆì§ˆ ê²€í†  ê²°ê³¼: ì‹¬ê°í•œ ë¬¸ì œ ì—†ìŒ")
            logger.info(_m("translator.quality_review_no_critical"))

        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°ì€ ê°œë³„ QualityIssue ë°©ì‹ì—ì„œëŠ” ìƒëµ
        # ëŒ€ì‹  ì‹¬ê°ë„ë³„ í†µê³„ë¥¼ í†µí•´ ì „ì²´ í’ˆì§ˆ ìƒíƒœë¥¼ íŒŒì•…

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì™„ë£Œ)
        if progress_callback:
            summary = f"í’ˆì§ˆ ê²€í†  ì™„ë£Œ - ë¬¸ì œ {len(all_issues)}ê°œ ë°œê²¬"
            progress_callback(
                "ğŸ” ë²ˆì—­ í’ˆì§ˆ ê²€í†  ì™„ë£Œ", len(chunks), len(chunks), summary
            )

        # ìƒíƒœì— ê²€í†  ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)
        state["quality_issues"] = all_issues

        return state

    except Exception as exc:
        logger.error(f"í’ˆì§ˆ ê²€í†  ì¤‘ ì˜¤ë¥˜: {exc}")
        logger.error(traceback.format_exc())
        # í’ˆì§ˆ ê²€í†  ì‹¤íŒ¨ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
        return state


def _create_quality_review_chunks(
    review_items: List[Dict], max_chars: int = 2000
) -> List[List[Dict]]:
    """í’ˆì§ˆ ê²€í† ë¥¼ ìœ„í•´ í•­ëª©ë“¤ì„ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
    chunks = []
    current_chunk = []
    current_chars = 0

    for item in review_items:
        # í•­ëª©ì˜ ì˜ˆìƒ ë¬¸ì ìˆ˜ ê³„ì‚° (ID + ì›ë³¸ + ë²ˆì—­ + í¬ë§·íŒ…)
        item_chars = (
            len(item["id"]) + len(item["original"]) + len(item["translated"]) + 50
        )

        # ë‹¨ì¼ í•­ëª©ì´ max_charsë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë³„ë„ ì²­í¬ë¡œ ì²˜ë¦¬
        if item_chars > max_chars:
            if current_chunk:
                chunks.append(current_chunk)
                current_chunk = []
                current_chars = 0
            chunks.append([item])
        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í–ˆì„ ë•Œ ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
        elif current_chars + item_chars > max_chars:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = [item]
            current_chars = item_chars
        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
        else:
            current_chunk.append(item)
            current_chars += item_chars

    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append(current_chunk)

    return chunks


async def _review_chunk_worker(
    *,
    chunk: List[Dict],
    target_language: str,
    llm: Any,
    state: TranslatorState,
    semaphore: asyncio.Semaphore,
    delay_manager: RequestDelayManager,
    chunk_idx: int,
    total_chunks: int,
    progress_callback: Optional[callable] = None,
) -> List[QualityIssue]:
    """ì²­í¬ë³„ í’ˆì§ˆ ê²€í† ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì›Œì»¤ - ê°œë³„ QualityIssueë“¤ì„ ë°˜í™˜"""
    async with semaphore:
        await delay_manager.wait()

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        if progress_callback:
            progress_callback(
                "ğŸ” ë²ˆì—­ í’ˆì§ˆ ê²€í†  ì¤‘",
                chunk_idx,
                total_chunks,
                f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ê²€í†  ì¤‘ ({len(chunk)}ê°œ í•­ëª©)",
            )

        try:
            # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            current_llm = llm
            if state.get("use_multi_api_keys") and state.get("multi_llm_manager"):
                multi_manager = state["multi_llm_manager"]
                fresh_client = await multi_manager.get_client()
                if fresh_client:
                    current_llm = fresh_client
                    logger.debug(
                        f"í’ˆì§ˆ ê²€í†  ì²­í¬ {chunk_idx + 1}: ë‹¤ì¤‘ API í‚¤ì—ì„œ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                    )
                else:
                    logger.warning(
                        f"í’ˆì§ˆ ê²€í†  ì²­í¬ {chunk_idx + 1}: ë‹¤ì¤‘ API í‚¤ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨, ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                    )

            # ê²€í† ìš© í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            review_text = _format_chunk_for_quality_review(chunk)

            # í’ˆì§ˆ ê²€í†  í”„ë¡¬í”„íŠ¸ ìƒì„±
            from src.prompts.llm_prompts import quality_review_prompt

            prompt = quality_review_prompt(target_language, review_text)

            # LLM í˜¸ì¶œ - QualityIssue ë„êµ¬ ë°”ì¸ë”©
            llm_with_tools = current_llm.bind_tools([QualityIssue])
            response = await llm_with_tools.ainvoke(prompt)

            # ì‘ë‹µ íŒŒì‹± - ê°œë³„ QualityIssueë“¤ ìˆ˜ì§‘
            quality_issues = []
            if response.tool_calls:
                for tool_call in response.tool_calls:
                    if tool_call["name"] == "QualityIssue":
                        try:
                            issue = QualityIssue(**tool_call["args"])
                            quality_issues.append(issue)
                            logger.debug(
                                f"í’ˆì§ˆ ë¬¸ì œ ë°œê²¬: [{issue.text_id}] {issue.issue_type} ({issue.severity})"
                            )
                        except Exception as e:
                            logger.warning(
                                f"QualityIssue íŒŒì‹± ì˜¤ë¥˜: {e}, args: {tool_call['args']}"
                            )

            logger.debug(
                f"ì²­í¬ {chunk_idx + 1} í’ˆì§ˆ ê²€í†  ì™„ë£Œ: {len(quality_issues)}ê°œ ë¬¸ì œ ë°œê²¬"
            )
            return quality_issues

        except Exception as exc:
            logger.error(f"ì²­í¬ {chunk_idx + 1} í’ˆì§ˆ ê²€í†  ì‹¤íŒ¨: {exc}")

            # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ í•´ë‹¹ í‚¤ì˜ ì‹¤íŒ¨ë¥¼ ê¸°ë¡
            if state.get("use_multi_api_keys") and state.get("multi_llm_manager"):
                logger.debug(f"í’ˆì§ˆ ê²€í†  ì²­í¬ {chunk_idx + 1}: API í‚¤ ì‹¤íŒ¨ ê¸°ë¡ë¨")

            return []


def _format_chunk_for_quality_review(chunk: List[Dict]) -> str:
    """í’ˆì§ˆ ê²€í† ìš© ì²­í¬ í¬ë§·íŒ…"""
    lines = []

    for item in chunk:
        text_id = item["id"]
        original = item["original"]
        translated = item["translated"]

        lines.append(f"[{text_id}]")
        lines.append(f"ì›ë³¸: {original}")
        lines.append(f"ë²ˆì—­: {translated}")
        lines.append("")  # ë¹ˆ ì¤„ ì¶”ê°€

    return "\n".join(lines)


def should_retranslate_based_on_quality(state: TranslatorState) -> str:
    """í’ˆì§ˆ ê²€í†  ê²°ê³¼ì— ë”°ë¼ ì¬ë²ˆì—­ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if state.get("error"):
        return "end"

    quality_issues = state.get("quality_issues", [])

    # ëª¨ë“  í’ˆì§ˆ ë¬¸ì œë¥¼ ì¬ë²ˆì—­ ëŒ€ìƒìœ¼ë¡œ ê³ ë ¤
    if quality_issues:
        quality_retry_count = state.get("quality_retry_count", 0)
        max_quality_retries = state.get("max_quality_retries", 2)

        if quality_retry_count < max_quality_retries:
            logger.info(
                f"í’ˆì§ˆ ê²€í†  ê²°ê³¼ {len(quality_issues)}ê°œ ë¬¸ì œ ë°œê²¬, ì¬ë²ˆì—­ ì§„í–‰ ({quality_retry_count + 1}/{max_quality_retries})"
            )
            return "quality_retranslate"
        else:
            logger.warning(
                f"í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ìµœëŒ€ íšŸìˆ˜ ({max_quality_retries}) ë„ë‹¬, ì™„ë£Œë¡œ ì§„í–‰"
            )
            return "complete"
    else:
        logger.info("í’ˆì§ˆ ê²€í†  ê²°ê³¼ ë¬¸ì œ ì—†ìŒ, ì™„ë£Œë¡œ ì§„í–‰")
        logger.info(_m("translator.quality_review_ok_complete"))
        return "complete"


async def quality_based_retranslation_node(state: TranslatorState) -> TranslatorState:
    """í’ˆì§ˆ ê²€í†  ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œê°€ ìˆëŠ” í•­ëª©ë“¤ì„ ë‹¤ì‹œ ë²ˆì—­í•©ë‹ˆë‹¤."""
    try:
        quality_issues = state.get("quality_issues", [])
        id_map = state["id_to_text_map"]
        translation_map = state["translation_map"]
        llm = state.get("llm_client")

        # ì¬ë²ˆì—­ ì¹´ìš´í„° ì¦ê°€
        quality_retry_count = state.get("quality_retry_count", 0) + 1
        state["quality_retry_count"] = quality_retry_count

        logger.info(f"í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì‹œì‘ ({quality_retry_count}ì°¨ ì‹œë„)")

        # ëª¨ë“  í’ˆì§ˆ ë¬¸ì œê°€ ìˆëŠ” í•­ëª©ë“¤ ì¶”ì¶œ
        all_issues = quality_issues

        # ì¬ë²ˆì—­í•  í•­ëª©ë“¤ ì¤€ë¹„
        items_to_retranslate = []
        for issue in all_issues:
            text_id = issue.text_id
            if text_id in id_map:
                original_text = id_map[text_id]
                current_translation = translation_map.get(text_id, "")

                items_to_retranslate.append(
                    {
                        "id": text_id,
                        "original": original_text,
                        "current_translation": current_translation,
                        "issue": issue,
                    }
                )

        # ì¤‘ë³µ ì œê±° (ê°™ì€ IDê°€ ì—¬ëŸ¬ ë¬¸ì œë¡œ ì¤‘ë³µë  ìˆ˜ ìˆìŒ)
        unique_items = {}
        for item in items_to_retranslate:
            text_id = item["id"]
            if text_id not in unique_items:
                unique_items[text_id] = item
            else:
                # ê¸°ì¡´ í•­ëª©ì— ì¶”ê°€ ì´ìŠˆ ì •ë³´ ë³‘í•©
                if "issues" not in unique_items[text_id]:
                    unique_items[text_id]["issues"] = [unique_items[text_id]["issue"]]
                    del unique_items[text_id]["issue"]
                unique_items[text_id]["issues"].append(item["issue"])

        items_to_retranslate = list(unique_items.values())

        if not items_to_retranslate:
            logger.info("ì¬ë²ˆì—­í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info(_m("translator.no_items_for_retranslation"))
            return state

        logger.info(
            f"í’ˆì§ˆ ë¬¸ì œë¡œ {len(items_to_retranslate)}ê°œ í•­ëª© ì¬ë²ˆì—­ ì§„í–‰ (ì „ì²´ ë¬¸ì œ {len(all_issues)}ê°œ)"
        )

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ”„ í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì¤‘",
                0,
                len(items_to_retranslate),
                f"í’ˆì§ˆ ë¬¸ì œ {len(items_to_retranslate)}ê°œ í•­ëª© ì¬ë²ˆì—­ ì‹œì‘",
            )

        # ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        chunks = TokenOptimizer.create_text_chunks(
            items_to_retranslate, state["max_tokens_per_chunk"]
        )

        # ë™ì‹œ ìš”ì²­ ì œí•œ
        sem = asyncio.Semaphore(state["max_concurrent_requests"])
        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])

        # ì¬ë²ˆì—­ ì‹¤í–‰
        tasks = []
        for chunk_idx, chunk in enumerate(chunks):
            tasks.append(
                _quality_retranslate_chunk_worker(
                    chunk=chunk,
                    state=state,
                    llm=llm,
                    target_language=state["target_language"],
                    delay_manager=delay_mgr,
                    semaphore=sem,
                    chunk_idx=chunk_idx,
                    total_chunks=len(chunks),
                    progress_callback=progress_callback,
                    max_retries=3,  # í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ì—ì„œëŠ” ë” ë§ì€ ì¬ì‹œë„
                )
            )

        results = await asyncio.gather(*tasks)

        # ê²°ê³¼ ì—…ë°ì´íŠ¸
        success_count = 0
        failed_count = 0

        for chunk_results in results:
            for item in chunk_results:
                text_id = item.id
                new_translation = item.translated.strip()
                original_text = id_map.get(text_id, "")

                # í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦
                if PlaceholderManager.validate_placeholder_preservation(
                    original_text, new_translation
                ):
                    translation_map[text_id] = new_translation
                    success_count += 1
                    logger.debug(
                        f"í’ˆì§ˆ ì¬ë²ˆì—­ ì„±ê³µ: {text_id} -> {new_translation[:50]}..."
                    )
                else:
                    failed_count += 1
                    missing_placeholders = PlaceholderManager.get_missing_placeholders(
                        original_text, new_translation
                    )
                    logger.warning(
                        f"í’ˆì§ˆ ì¬ë²ˆì—­ í›„ì—ë„ í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½: {text_id} (ëˆ„ë½: {missing_placeholders})"
                    )

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì™„ë£Œ)
        if progress_callback:
            progress_callback(
                "ğŸ”„ í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì™„ë£Œ",
                len(chunks),
                len(chunks),
                f"ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {failed_count}ê°œ",
            )

        logger.info(
            f"í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ"
        )

        # ì¬ë²ˆì—­ì„ í•œ ë²ˆ ìˆ˜í–‰í–ˆìœ¼ë¯€ë¡œ ì´í›„ í’ˆì§ˆ ê²€í† ëŠ” ìƒëµí•˜ë„ë¡ í”Œë˜ê·¸ ë¹„í™œì„±í™”
        # (ë¬´í•œ ë£¨í”„ ë° ë¶ˆí•„ìš”í•œ ì¶”ê°€ í’ˆì§ˆ ê²€í†  ë°©ì§€)
        state["enable_quality_review"] = False

        # JSON ì¬êµ¬ì„± ë° í”Œë ˆì´ìŠ¤í™€ë” ë³µì›
        try:
            # JSON ì¬êµ¬ì„±
            logger.info("í’ˆì§ˆ ì¬ë²ˆì—­ í›„ JSON ì¬êµ¬ì„± ì‹œì‘...")
            id_map = state["translation_map"]

            def replace(obj: Any) -> Any:
                if isinstance(obj, dict):
                    return {k: replace(v) for k, v in obj.items()}
                if isinstance(obj, list):
                    return [replace(i) for i in obj]
                if isinstance(obj, str) and obj in id_map:
                    return id_map[obj]
                return obj

            state["translated_json"] = replace(state["processed_json"])
            logger.info("í’ˆì§ˆ ì¬ë²ˆì—­ í›„ JSON ì¬êµ¬ì„± ì™„ë£Œ.")

            # í”Œë ˆì´ìŠ¤í™€ë” ë³µì›
            logger.info("í’ˆì§ˆ ì¬ë²ˆì—­ í›„ í”Œë ˆì´ìŠ¤í™€ë” ë³µì› ì‹œì‘...")
            placeholders = state["placeholders"]
            newline_value = placeholders.get("[NEWLINE]")

            # Sort placeholders ONCE, excluding newline
            sorted_placeholders = sorted(
                (item for item in placeholders.items() if item[0] != "[NEWLINE]"),
                key=lambda item: (
                    int(item[0][2:-1]) if item[0].startswith("[P") else -1
                ),
                reverse=True,
            )

            # JSON ê°ì²´ ë ˆë²¨ì—ì„œ ì•ˆì „í•˜ê²Œ placeholder ë³µì›
            restored_json = PlaceholderManager.restore_placeholders_in_json(
                state["translated_json"], sorted_placeholders, newline_value
            )

            # ë³µì›ëœ JSON ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            state["final_json"] = json.dumps(
                restored_json, ensure_ascii=False, indent=2
            )
            logger.info("í’ˆì§ˆ ì¬ë²ˆì—­ í›„ í”Œë ˆì´ìŠ¤í™€ë” ë³µì› ì™„ë£Œ.")

        except Exception as exc:
            logger.error(f"í’ˆì§ˆ ì¬ë²ˆì—­ í›„ JSON ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {exc}")
            state["error"] = f"í’ˆì§ˆ ì¬ë²ˆì—­ í›„ JSON ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {exc}"

        return state

    except Exception as exc:
        logger.error(f"í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {exc}")
        logger.error(traceback.format_exc())
        return state


async def _quality_retranslate_chunk_worker(
    *,
    chunk: List[Dict],
    state: TranslatorState,
    llm: Any,
    target_language: str,
    delay_manager: RequestDelayManager,
    semaphore: asyncio.Semaphore,
    chunk_idx: int,
    total_chunks: int,
    progress_callback: Optional[callable] = None,
    max_retries: int = 3,
) -> List[TranslatedItem]:
    """í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ì„ ìœ„í•œ ì²­í¬ ì›Œì»¤"""
    async with semaphore:
        await delay_manager.wait()

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        if progress_callback:
            progress_callback(
                "ğŸ”„ í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì¤‘",
                chunk_idx,
                total_chunks,
                f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ì¬ë²ˆì—­ ì¤‘ ({len(chunk)}ê°œ í•­ëª©)",
            )

        # ê¸€ë¡œì‹œë¦¬ì—ì„œ ê´€ë ¨ ìš©ì–´ í•„í„°ë§
        all_glossary_terms = state.get("important_terms", [])
        relevant_glossary = _filter_relevant_glossary_terms(chunk, all_glossary_terms)

        # ì¬ë²ˆì—­ ì‹œë„
        for attempt in range(max_retries + 1):
            try:
                # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
                current_llm = llm
                if state.get("use_multi_api_keys") and state.get("multi_llm_manager"):
                    multi_manager = state["multi_llm_manager"]
                    fresh_client = await multi_manager.get_client()
                    if fresh_client:
                        current_llm = fresh_client
                        logger.debug(
                            f"í’ˆì§ˆ ì¬ë²ˆì—­ ì²­í¬ {chunk_idx + 1}: ë‹¤ì¤‘ API í‚¤ì—ì„œ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                        )
                    else:
                        logger.warning(
                            f"í’ˆì§ˆ ì¬ë²ˆì—­ ì²­í¬ {chunk_idx + 1}: ë‹¤ì¤‘ API í‚¤ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨, ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"
                        )

                # í”„ë¡¬í”„íŠ¸ ìƒì„± (í’ˆì§ˆ ë¬¸ì œë¥¼ ê³ ë ¤í•œ ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸)
                glossary_text = TokenOptimizer.format_glossary_for_llm(
                    relevant_glossary
                )
                retry_info = (
                    f"âš ï¸ ì¬ì‹œë„ {attempt}íšŒ - ì´ì „ ë²ˆì—­ì—ì„œ ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì£¼ì˜ê¹Šê²Œ í™•ì¸í•´ì£¼ì„¸ìš”: 1. í”Œë ˆì´ìŠ¤í™€ë”([P###], [NEWLINE] ë“±)ë¥¼ ì •í™•íˆ ë³´ì¡´ 2. ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ì „ë‹¬ 3. ìì—°ìŠ¤ëŸ½ê³  ì¼ê´€ëœ ë²ˆì—­"
                    if attempt > 0
                    else "í’ˆì§ˆ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¬ë²ˆì—­"
                )
                formatted_items = _format_items_for_quality_retranslation(chunk)

                prompt = quality_retranslation_prompt(
                    target_language, glossary_text, retry_info, formatted_items
                )

                # LLM í˜¸ì¶œ
                llm_with_tools = current_llm.bind_tools([TranslatedItem])
                response = await llm_with_tools.ainvoke(prompt)

                # ì‘ë‹µ íŒŒì‹±
                translations = []
                if response.tool_calls:
                    for tool_call in response.tool_calls:
                        if tool_call["name"] == "TranslatedItem":
                            try:
                                item = TranslatedItem(**tool_call["args"])
                                translations.append(item)
                            except Exception as e:
                                logger.warning(f"TranslatedItem íŒŒì‹± ì˜¤ë¥˜: {e}")

                # í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦
                valid_translations = []
                for translation in translations:
                    original_text = next(
                        (
                            item["original"]
                            for item in chunk
                            if item["id"] == translation.id
                        ),
                        "",
                    )

                    # ID íŒ¨í„´(T###)ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜ëœ ê²½ìš° ë¬´ì‹œ
                    if re.match(r"^T\d{3,}$", translation.translated.strip()):
                        logger.debug(f"ID ê·¸ëŒ€ë¡œ ë°˜í™˜ëœ í•­ëª© ë¬´ì‹œ: {translation.id}")
                        continue

                    if PlaceholderManager.validate_placeholder_preservation(
                        original_text, translation.translated
                    ):
                        valid_translations.append(translation)
                    else:
                        logger.debug(f"í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ ì‹¤íŒ¨: {translation.id}")

                # ëª¨ë“  ë²ˆì—­ì´ ìœ íš¨í•˜ë©´ ì„±ê³µ
                if len(valid_translations) == len(chunk):
                    if attempt > 0:
                        logger.info(
                            f"í’ˆì§ˆ ì¬ë²ˆì—­ ì²­í¬ {chunk_idx + 1} ì„±ê³µ (ì¬ì‹œë„ {attempt}íšŒ)"
                        )
                    return valid_translations
                else:
                    logger.warning(
                        f"í’ˆì§ˆ ì¬ë²ˆì—­ ì²­í¬ {chunk_idx + 1} ì‹œë„ {attempt + 1}: ìœ íš¨í•œ ë²ˆì—­ {len(valid_translations)}/{len(chunk)}"
                    )

                    # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ê³„ì† ì§„í–‰
                    if attempt < max_retries:
                        await asyncio.sleep(min(2.0, (attempt + 1) * 0.5))
                        continue
                    else:
                        # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œëŠ” ìœ íš¨í•œ ë²ˆì—­ì´ë¼ë„ ë°˜í™˜
                        return valid_translations

            except Exception as exc:
                logger.error(
                    f"í’ˆì§ˆ ì¬ë²ˆì—­ ì²­í¬ {chunk_idx + 1} ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {exc}"
                )

                # ë‹¤ì¤‘ API í‚¤ ì‚¬ìš© ì‹œ í•´ë‹¹ í‚¤ì˜ ì‹¤íŒ¨ë¥¼ ê¸°ë¡
                if state.get("use_multi_api_keys") and state.get("multi_llm_manager"):
                    logger.debug(
                        f"í’ˆì§ˆ ì¬ë²ˆì—­ ì²­í¬ {chunk_idx + 1}: API í‚¤ ì‹¤íŒ¨ ê¸°ë¡ë¨"
                    )

                if attempt < max_retries:
                    await asyncio.sleep(min(2.0, (attempt + 1) * 0.5))
                else:
                    return []

        return []


def _format_items_for_quality_retranslation(chunk: List[Dict]) -> str:
    """í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±ì— í•„ìš”í•œ í¬ë§·íŒ…"""
    lines = []
    for i, item in enumerate(chunk, 1):
        text_id = item["id"]
        original = item["original"]
        current_translation = item.get("current_translation", "")

        # í’ˆì§ˆ ë¬¸ì œ ì •ë³´ ì¶”ê°€
        issues = item.get("issues", [item.get("issue")] if item.get("issue") else [])

        lines.append(f"{i}. [{text_id}]")
        lines.append(f"   ì›ë³¸: {original}")
        if current_translation:
            lines.append(f"   ì´ì „ ë²ˆì—­: {current_translation}")

        if issues:
            lines.append("   ë°œê²¬ëœ ë¬¸ì œ:")
            for issue in issues:
                if issue:
                    lines.append(f"   - {issue.issue_type}: {issue.description}")
                    if issue.suggested_fix:
                        lines.append(f"     ì œì•ˆ: {issue.suggested_fix}")

        lines.append("")

    return "\n".join(lines)


###############################################################################
# 4. Main translator class                                                    #
###############################################################################


class JSONTranslator:
    """JSON ë²ˆì—­ê¸°"""

    def __init__(self, *, glossary_path: Optional[str] = None) -> None:
        self.glossary_path = glossary_path
        self.token_counter = UniversalTokenCountingHandler()
        self._workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:  # noqa: D401
        """Create the translation workflow."""
        wf = StateGraph(TranslatorState)

        # Add nodes
        wf.add_node("parse_and_extract", parse_and_extract_node)
        wf.add_node("load_vanilla_glossary", load_vanilla_glossary_node)
        wf.add_node("load_glossary", load_glossary_node)
        wf.add_node(
            "extract_terms_from_json_chunks", extract_terms_from_json_chunks_node
        )
        wf.add_node("smart_translate", smart_translate_node)
        wf.add_node("validation_and_retry", validation_and_retry_node)
        wf.add_node("final_fallback_translation", final_fallback_translation_node)
        wf.add_node("rebuild_json", rebuild_json_node)
        wf.add_node("restore_placeholders", restore_placeholders_node)
        wf.add_node("quality_review", quality_review_node)
        wf.add_node("quality_based_retranslation", quality_based_retranslation_node)
        wf.add_node("save_glossary", save_glossary_node)
        wf.add_node("final_check", lambda state: state)

        # Set entry point
        wf.set_entry_point("parse_and_extract")

        # Define edges
        wf.add_edge("parse_and_extract", "load_vanilla_glossary")
        wf.add_edge("load_vanilla_glossary", "load_glossary")
        wf.add_edge("load_glossary", "extract_terms_from_json_chunks")
        wf.add_edge("extract_terms_from_json_chunks", "smart_translate")

        # Main translation path with retries
        wf.add_conditional_edges(
            "smart_translate",
            should_retry,
            {
                "retry": "validation_and_retry",
                "complete": "rebuild_json",
                "end": "final_check",  # Go to final check on error
                "final_fallback": "final_fallback_translation",
            },
        )
        wf.add_conditional_edges(
            "validation_and_retry",
            should_retry,
            {
                "retry": "validation_and_retry",
                "complete": "rebuild_json",
                "end": "final_check",  # Go to final check on error
                "final_fallback": "final_fallback_translation",
            },
        )

        # Success path
        wf.add_edge("final_fallback_translation", "rebuild_json")
        wf.add_edge("rebuild_json", "restore_placeholders")

        # í’ˆì§ˆ ê²€í† ë¥¼ ì¬ë²ˆì—­ ì „ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ë„ë¡ ì¡°ê±´ë¶€ ë¶„ê¸°
        wf.add_conditional_edges(
            "restore_placeholders",
            should_run_quality_review,
            {"review": "quality_review", "skip": "final_check"},
        )

        # Quality review based retranslation
        wf.add_conditional_edges(
            "quality_review",
            should_retranslate_based_on_quality,
            {
                "quality_retranslate": "quality_based_retranslation",
                "complete": "final_check",
                "end": "final_check",
            },
        )

        # After quality-based retranslation, go directly to final check
        wf.add_edge("quality_based_retranslation", "final_check")

        # Final check to decide on saving
        wf.add_conditional_edges(
            "final_check",
            should_save_glossary,
            {"save_glossary": "save_glossary", "end": END},
        )
        wf.add_edge("save_glossary", END)

        return wf.compile()

    def get_token_summary(self) -> Dict[str, Any]:
        """í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ë°˜í™˜"""
        return self.token_counter.get_token_summary()

    def get_formatted_token_summary(self) -> str:
        """í¬ë§·ëœ í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ë°˜í™˜"""
        return self.token_counter.get_formatted_summary()

    def reset_token_counter(self):
        """í† í° ì¹´ìš´í„° ì´ˆê¸°í™”"""
        self.token_counter.reset_counts()

    async def translate(
        self,
        json_input: Any,
        target_language: str = "í•œêµ­ì–´",
        *,
        use_glossary: bool = True,
        use_vanilla_glossary: bool = True,
        vanilla_glossary_path: Optional[str] = None,
        max_retries: int = 3,
        max_tokens_per_chunk: int = 3000,
        max_concurrent_requests: int = 5,
        delay_between_requests_ms: int = 200,
        progress_callback: Optional[callable] = None,
        existing_translations: Optional[Dict[str, str]] = None,
        llm_provider: str = "gemini",
        llm_model: str = "gemini-1.5-flash",
        temperature: float = 0.1,
        final_fallback_max_retries: int = 2,
        enable_quality_review: bool = True,
        max_quality_retries: int = 1,
        use_multi_api_keys: bool = False,
        multi_llm_manager: Optional[MultiLLMManager] = None,
        track_tokens: bool = True,
    ) -> Dict[str, Any]:
        """ë²ˆì—­ ì‹¤í–‰ ë° í† í° ì‚¬ìš©ëŸ‰ ì¶”ì """
        if isinstance(json_input, dict):
            json_dict = json_input
        else:
            json_dict = json.loads(str(json_input).strip())

        # í† í° ì¹´ìš´í„° ì´ˆê¸°í™”
        if track_tokens:
            self.reset_token_counter()

        logger.info(
            _m(
                "translator.settings",
                max_retries=max_retries,
                chunk_tokens=max_tokens_per_chunk,
            )
        )
        logger.info(
            _m(
                "translator.parallel_settings",
                concurrent=max_concurrent_requests,
                delay=delay_between_requests_ms,
            )
        )

        # í•­ìƒ ë‹¤ì¤‘ API í‚¤ ëª¨ë“œ ì‚¬ìš©
        # multi_llm_managerê°€ ì „ë‹¬ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìƒˆë¡œ ìƒì„±
        multi_llm_manager = multi_llm_manager or MultiLLMManager()
        active_keys = multi_llm_manager.get_active_keys()
        if not active_keys:
            raise RuntimeError(
                "ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì¤‘ API í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."
            )
        logger.info(f"ë‹¤ì¤‘ API í‚¤ ëª¨ë“œ í™œì„±í™”: {len(active_keys)}ê°œ í‚¤ ì‚¬ìš© ê°€ëŠ¥")

        # LLM í´ë¼ì´ì–¸íŠ¸ íšë“ (ë¡œí…Œì´ì…˜ ì ìš©)
        llm_client = await multi_llm_manager.get_client()
        if not llm_client:
            raise RuntimeError("ë‹¤ì¤‘ API í‚¤ í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # í† í° ì¹´ìš´í„°ë¥¼ LLM í´ë¼ì´ì–¸íŠ¸ì— ì¶”ê°€
        if track_tokens:
            if hasattr(llm_client, "callbacks"):
                if llm_client.callbacks is None:
                    llm_client.callbacks = []
                llm_client.callbacks.append(self.token_counter)
            else:
                # ìƒˆë¡œìš´ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œ ì½œë°± ì¶”ê°€
                logger.warning("LLM í´ë¼ì´ì–¸íŠ¸ì— ì½œë°±ì„ ì¶”ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        initial_state: TranslatorState = TranslatorState(
            parsed_json=json_dict,
            placeholders={},
            id_to_text_map={},
            important_terms=[],
            processed_json={},
            translation_map={},
            translated_json={},
            final_json="",
            target_language=target_language,
            retry_count=0,
            max_retries=max_retries,
            max_tokens_per_chunk=max_tokens_per_chunk,
            max_concurrent_requests=max_concurrent_requests,
            delay_between_requests_ms=delay_between_requests_ms,
            error=None,
            glossary_path=self.glossary_path,
            use_glossary=use_glossary,
            progress_callback=progress_callback,
            existing_translations=existing_translations,
            primary_glossary=[],
            use_vanilla_glossary=use_vanilla_glossary,
            vanilla_glossary_path=vanilla_glossary_path or "vanilla_glossary.json",
            vanilla_glossary=[],
            llm_client=llm_client,
            final_fallback_max_retries=final_fallback_max_retries,
            enable_quality_review=enable_quality_review,
            quality_issues=[],
            quality_retry_count=0,
            max_quality_retries=max_quality_retries,
            # ë‹¤ì¤‘ API í‚¤ ê´€ë ¨ ì¶”ê°€
            use_multi_api_keys=use_multi_api_keys,
            multi_llm_manager=multi_llm_manager,
            token_counter=self.token_counter,
        )

        result = await self._workflow.ainvoke(initial_state, {"recursion_limit": 50})
        if result.get("error"):
            raise RuntimeError(result["error"])

        # í† í° ì‚¬ìš©ëŸ‰ ë¡œê·¸ ì¶œë ¥
        if track_tokens:
            token_summary = self.get_formatted_token_summary()
            logger.info(f"ë²ˆì—­ ì™„ë£Œ - í† í° ì‚¬ìš©ëŸ‰:\n{token_summary}")

        return json.loads(result["final_json"])


###############################################################################
# 5. Optional runnable example                                                #
###############################################################################


async def run_example() -> None:  # pragma: no cover â€“ utility function
    """Run a quick interactive demo similar to the original script."""

    logging.basicConfig(level=logging.INFO)
    logger.info(_m("translator.init_translator"))
    translator = JSONTranslator()

    # Put your test JSON here or load from file.
    test_json = {"hello": "world"}

    try:
        logger.info(_m("translator.starting_translation"))
        translated = await translator.translate(test_json, "í•œêµ­ì–´")
        logger.info(_m("translator.result"))
        logger.info(translated)
    except Exception:  # pragma: no cover â€“ human experimentation
        logger.error(_m("translator.error_occurred"))
        logger.error(traceback.format_exc())


if __name__ == "__main__":  # pragma: no cover
    asyncio.run(run_example())


def should_run_quality_review(state: TranslatorState) -> str:
    """í’ˆì§ˆ ê²€í† ë¥¼ ìˆ˜í–‰í• ì§€ ê²°ì •í•œë‹¤.

    - ì¬ë²ˆì—­ ì „(quality_retry_count == 0) ì—ë§Œ í’ˆì§ˆ ê²€í†  ì‹¤í–‰
    - ì¬ë²ˆì—­ í›„ì—ëŠ” ë°”ë¡œ ì™„ë£Œ ë‹¨ê³„ë¡œ ì´ë™í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë‘ ë²ˆì§¸ ê²€í† ë¥¼ ê±´ë„ˆë›´ë‹¤
    """
    if (
        state.get("enable_quality_review", True)
        and state.get("quality_retry_count", 0) == 0
    ):
        return "review"
    return "skip"

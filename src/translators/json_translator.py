"""ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ translator.pyì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜ëœ JSON ë²ˆì—­ê¸° ëª¨ë“ˆì…ë‹ˆë‹¤.

ì´ ëª¨ë“ˆì€ LangGraphë¥¼ í†µí•´ Google Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì„ì˜ì˜ JSON êµ¬ì¡°ë¥¼
ë²ˆì—­í•˜ê¸° ìœ„í•œ `JSONTranslator` í´ë˜ìŠ¤ì™€ ì§€ì› ìœ í‹¸ë¦¬í‹°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ê¸°ì¡´ì˜ ë…ë¦½ ì‹¤í–‰í˜• ìŠ¤í¬ë¦½íŠ¸ëŠ” ì´ ëª¨ë“ˆë¡œ ë¦¬íŒ©í„°ë§ë˜ì–´ ë‹¤ìŒê³¼ ê°™ì´ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    from translators import JSONTranslator

ì›ë³¸ íŒŒì¼ í•˜ë‹¨ì— ìˆë˜ ëŒ€í™”í˜• ì˜ˆì œ ë° CLI ë¡œì§ì€ ëª¨ë“ˆì„ ë…ë¦½ì ìœ¼ë¡œ
ìœ ì§€í•˜ê¸° ìœ„í•´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. ëŒ€ì‹  í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ì‹¤í—˜í•˜ê³  ì‹¶ë‹¤ë©´
í¸ì˜ë¥¼ ìœ„í•´ `run_example()` ì½”ë£¨í‹´ì´ ì œê³µë©ë‹ˆë‹¤.
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional, TypedDict

import regex as re
from langchain_core.language_models import BaseLLM
from langchain_core.output_parsers import PydanticOutputParser
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, Field

from src.localization import get_message as _m
from src.prompts.llm_prompts import (
    contextual_terms_prompt,
    final_fallback_prompt,
    quality_retranslation_prompt,
    retry_contextual_terms_prompt,
    retry_translation_prompt,
    translation_prompt,
)
from src.translators.llm_manager import LLMManager

logger = logging.getLogger(__name__)

__all__ = [
    "TranslatorState",
    "TranslatedItem",
    "TermMeaning",
    "GlossaryEntry",
    "Glossary",
    "TranslationPair",
    "TranslationResult",
    "QualityIssue",
    "QualityReview",
    "JSONTranslator",
    "run_example",
]

###############################################################################
# 1. State and data-model definitions                                        #
###############################################################################


class TranslatorState(TypedDict):
    """Runtime state carried between LangGraph nodes."""

    # Input & settings
    parsed_json: Dict[str, Any]
    target_language: str
    max_retries: int
    max_tokens_per_chunk: int
    max_concurrent_requests: int
    delay_between_requests_ms: int

    # Intermediate processing data
    placeholders: Dict[str, str]
    id_to_text_map: Dict[str, str]  # text_id -> original_text
    important_terms: List[GlossaryEntry]  # Glossary for consistent translation
    processed_json: Dict[str, Any]  # text with IDs substituted JSON
    translation_map: Dict[str, str]  # text_id -> translated_text

    # Final result
    translated_json: Dict[str, Any]  # id replaced back with translated text
    final_json: str

    # Workflow tracking
    retry_count: int
    error: Optional[str]
    glossary_path: Optional[str]
    use_glossary: bool
    progress_callback: Optional[callable]  # ì§„í–‰ë¥  ì½œë°± ì¶”ê°€

    # ê¸°ì¡´ ë²ˆì—­ ë°ì´í„° (ì¶”ê°€)
    existing_translations: Optional[Dict[str, str]]  # source_text -> target_text
    primary_glossary: List[GlossaryEntry]  # ê¸°ì¡´ ë²ˆì—­ ë°ì´í„°ë¡œ ë§Œë“  1ì°¨ ì‚¬ì „

    # ë°”ë‹ë¼ glossary ê´€ë ¨ (ì¶”ê°€)
    use_vanilla_glossary: bool  # ë°”ë‹ë¼ ì‚¬ì „ ì‚¬ìš© ì—¬ë¶€
    vanilla_glossary_path: Optional[str]  # ë°”ë‹ë¼ ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
    vanilla_glossary: List[GlossaryEntry]  # ë°”ë‹ë¼ ì‚¬ì „

    # LLM í´ë¼ì´ì–¸íŠ¸ (ì¶”ê°€)
    llm_client: Optional[Any]  # LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤

    # ìµœì¢… ì¬ì‹œë„ íšŸìˆ˜
    final_fallback_max_retries: int

    # í’ˆì§ˆ ê²€í†  ê´€ë ¨ (ì¶”ê°€)
    enable_quality_review: bool  # í’ˆì§ˆ ê²€í†  ì‚¬ìš© ì—¬ë¶€
    quality_issues: List[Any]  # í’ˆì§ˆ ê²€í†  ê²°ê³¼
    quality_retry_count: int  # í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ íšŸìˆ˜
    max_quality_retries: int  # í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ìµœëŒ€ íšŸìˆ˜


class TranslatedItem(BaseModel):
    """Translation result item (ID based)."""

    id: str = Field(description="Unique ID of the text to translate")
    translated: str = Field(description="Translated text")


class TermMeaning(BaseModel):
    """A single meaning for a term, with context."""

    translation: str = Field(description="Translated term")
    context: str = Field(
        description="A very concise snippet of the surrounding text (under 10 words) to differentiate its meaning"
    )


class GlossaryEntry(BaseModel):
    """A glossary entry for a single original term, which may have multiple meanings."""

    original: str = Field(description="Original term")
    meanings: List[TermMeaning] = Field(
        description="A list of possible translations for the term, each with its own context"
    )


class SimpleGlossaryTerm(BaseModel):
    """A single glossary term with one meaning and its context."""

    original: str = Field(description="Original term")
    translation: str = Field(description="Translated term")
    context: str = Field(
        description="A very concise snippet of the surrounding text (under 10 words) to differentiate its meaning. This field is REQUIRED - if no specific context is available, provide a default value like 'ì¼ë°˜ì ì¸ ì‚¬ìš©' (general usage)"
    )


class Glossary(BaseModel):
    """The entire glossary, composed of multiple entries."""

    terms: List[GlossaryEntry]


class TranslationPair(BaseModel):
    """(original, translated) pair."""

    original: str = Field(description="Original text")
    translated: str = Field(description="Translated text")


class TranslationResult(BaseModel):
    """Structured output container returned from the LLM."""

    translations: List[TranslatedItem] = Field(description="List of translations")


class QualityIssue(BaseModel):
    """ë²ˆì—­ í’ˆì§ˆ ë¬¸ì œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸"""

    text_id: str = Field(description="ë¬¸ì œê°€ ìˆëŠ” í…ìŠ¤íŠ¸ì˜ ID")
    issue_type: str = Field(
        description="ë¬¸ì œ ìœ í˜• (ì˜ˆ: ì˜¤ì—­, ëˆ„ë½, ë¶€ìì—°ìŠ¤ëŸ¬ì›€, í”Œë ˆì´ìŠ¤í™€ë” ë¬¸ì œ)"
    )
    severity: str = Field(description="ì‹¬ê°ë„ (low, medium, high)")
    description: str = Field(description="ë¬¸ì œì— ëŒ€í•œ ì„¤ëª…")
    suggested_fix: Optional[str] = Field(description="ìˆ˜ì • ì œì•ˆ (ì„ íƒì‚¬í•­)")


class QualityReview(BaseModel):
    """ë²ˆì—­ í’ˆì§ˆ ê²€í†  ê²°ê³¼"""

    issues: List[QualityIssue] = Field(description="ë°œê²¬ëœ í’ˆì§ˆ ë¬¸ì œë“¤")
    overall_quality: str = Field(
        description="ì „ì²´ì ì¸ í’ˆì§ˆ í‰ê°€ (excellent, good, fair, poor)"
    )
    summary: str = Field(description="ê²€í†  ìš”ì•½")


###############################################################################
# 2. Utility helpers                                                          #
###############################################################################


class RequestDelayManager:
    """Small helper for yielding between API calls."""

    def __init__(self, delay_ms: int):
        self.delay_seconds = delay_ms / 1000.0

    async def wait(self) -> None:  # noqa: D401 â€“ imperative mood is fine
        """Sleep for the configured delay."""
        if self.delay_seconds > 0:
            await asyncio.sleep(self.delay_seconds)


class PlaceholderManager:
    """Extraction and restoration of special placeholder patterns."""

    # ------------------------------------------------------------------
    # Regex patterns for placeholders (expanded per user request)
    # ------------------------------------------------------------------
    FORMAT_CODE_PATTERN = r"[Â§&][0-9a-fk-or]"
    C_PLACEHOLDER_PATTERN = r"%(?:[0-9]+\$[sd]|[sd])"  # %1$s, %2$d, %s, %d ë“±
    ITEM_PLACEHOLDER_PATTERN = r"\$\([^)]*\)"
    JSON_PLACEHOLDER_PATTERN = r"\{(?:[^{}]|(?R))*\}"
    HTML_TAG_PATTERN = r"<[^>]*>"
    MINECRAFT_ITEM_CODE_PATTERN = (
        r"((\[[a-zA-Z_0-9]+[:.]([0-9a-zA-Z_]+([./][0-9a-zA-Z_]+)*)\])|"
        r"([a-zA-Z_0-9]+[:.]([0-9a-zA-Z_]+([./][0-9a-zA-Z_]+)*)))"
    )
    JS_TEMPLATE_LITERAL_PATTERN = r"\$\{[^}]+\}"
    SQUARE_BRACKET_TAG_PATTERN = r"\[[A-Za-z0-9_]+\]"
    LEGACY_MINECRAFT_PATTERN = (
        r"%[a-zA-Z_][a-zA-Z0-9_]*%"  # %username% ë“±, ë” ì •í™•í•œ ë§¤ì¹­
    )
    NEWLINE_PATTERN = r"\\n|\\r\\n|\\r"  # \n, \r\n, \r ê°œí–‰ ë¬¸ìë“¤

    # Aggregate list for easy iteration (ìˆœì„œ ì¤‘ìš”: ë” êµ¬ì²´ì ì¸ íŒ¨í„´ì„ ë¨¼ì €)
    _PLACEHOLDER_PATTERNS: List[str] = [
        NEWLINE_PATTERN,  # ê°œí–‰ ë¬¸ìë¥¼ ë¨¼ì € ì²˜ë¦¬ (ê°€ì¥ êµ¬ì²´ì )
        C_PLACEHOLDER_PATTERN,  # %1$s, %2$d ë“±ì„ ë¨¼ì € ë§¤ì¹­
        FORMAT_CODE_PATTERN,
        ITEM_PLACEHOLDER_PATTERN,
        JSON_PLACEHOLDER_PATTERN,
        HTML_TAG_PATTERN,
        MINECRAFT_ITEM_CODE_PATTERN,
        JS_TEMPLATE_LITERAL_PATTERN,
        SQUARE_BRACKET_TAG_PATTERN,
        LEGACY_MINECRAFT_PATTERN,  # %username% ë“±ì€ ë‚˜ì¤‘ì—
        r"\{\{[^}]+\}\}",  # double-brace mustache style
    ]
    # ë‚´ë¶€ ì¹˜í™˜ ë§ˆì»¤ ê²€ì¦ì„ ìœ„í•œ ì •ê·œì‹
    _INTERNAL_PLACEHOLDER_PATTERN = r"\[P\d{3,}\]"
    _INTERNAL_NEWLINE_PATTERN = r"\[NEWLINE\]"
    _placeholder_counter = 0

    @staticmethod
    def reset_counter() -> None:
        """í”Œë ˆì´ìŠ¤í™€ë” ì¹´ìš´í„°ë¥¼ ë¦¬ì…‹í•©ë‹ˆë‹¤."""
        PlaceholderManager._placeholder_counter = 0

    @staticmethod
    def extract_special_patterns_from_value(
        text: str, placeholders: Dict[str, str]
    ) -> str:
        """Find special patterns in *text* and replace them with placeholders."""

        if not isinstance(text, str):
            return text

        # ë¨¼ì € ê°œí–‰ ë¬¸ìë¥¼ íŠ¹ë³„íˆ ì²˜ë¦¬
        text = PlaceholderManager._extract_newlines(text, placeholders)

        matches: List[str] = []
        for pattern in PlaceholderManager._PLACEHOLDER_PATTERNS:
            # ê°œí–‰ íŒ¨í„´ì€ ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
            if pattern == PlaceholderManager.NEWLINE_PATTERN:
                continue

            found_matches = re.findall(pattern, text)
            # Handle case where findall returns tuples due to groups in regex
            for match in found_matches:
                if isinstance(match, tuple):
                    # Use the first non-empty group or the full match
                    match_str = next((group for group in match if group), match[0])
                else:
                    match_str = match
                matches.append(match_str)

        # Deduplicate while preserving order
        seen: set[str] = set()
        unique_matches = [m for m in matches if not (m in seen or seen.add(m))]

        for match in unique_matches:
            PlaceholderManager._placeholder_counter += 1
            placeholder_id = f"[P{PlaceholderManager._placeholder_counter:03d}]"
            placeholders[placeholder_id] = match
            text = text.replace(match, placeholder_id, 1)

        return text

    @staticmethod
    def _extract_newlines(text: str, placeholders: Dict[str, str]) -> str:
        """ê°œí–‰ ë¬¸ìë¥¼ íŠ¹ë³„íˆ ì²˜ë¦¬í•˜ì—¬ [NEWLINE] placeholderë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
        if not isinstance(text, str):
            return text

        newline_placeholder = "[NEWLINE]"
        # ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ê°œí–‰ ë¬¸ìë¥¼ í•˜ë‚˜ì˜ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ í†µì¼
        text = text.replace("\\r\\n", newline_placeholder)
        text = text.replace("\\n", newline_placeholder)
        text = text.replace("\\r", newline_placeholder)
        if newline_placeholder in text:
            placeholders[newline_placeholder] = "\\n"  # ë³µì› ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ê°œí–‰ ë¬¸ì

        return text

    @staticmethod
    def process_json_object(obj: Any, placeholders: Dict[str, str]) -> Any:
        """Recursively traverse *obj* replacing string values with placeholders."""
        if isinstance(obj, dict):
            return {
                k: PlaceholderManager.process_json_object(v, placeholders)
                for k, v in obj.items()
            }
        if isinstance(obj, list):
            return [
                PlaceholderManager.process_json_object(i, placeholders) for i in obj
            ]
        if isinstance(obj, str):
            return PlaceholderManager.extract_special_patterns_from_value(
                obj, placeholders
            )
        return obj

    @staticmethod
    def restore_placeholders(text: str, placeholders: Dict[str, str]) -> str:  # noqa: D401
        """Replace placeholder IDs back with their original patterns."""
        # ë¨¼ì € ê°œí–‰ ë¬¸ì placeholderë“¤ì„ ë³µì›
        text = PlaceholderManager._restore_newlines(text, placeholders)

        # ê·¸ ë‹¤ìŒ ì¼ë°˜ placeholderë“¤ì„ ë³µì› (IDê°€ í° ê²ƒë¶€í„° ì²˜ë¦¬í•˜ì—¬ ì¤‘ì²© ë°©ì§€)
        sorted_placeholders = sorted(
            placeholders.items(),
            key=lambda item: (int(item[0][2:-1]) if item[0].startswith("[P") else -1),
            reverse=True,
        )

        for pid, original in sorted_placeholders:
            if pid == "[NEWLINE]":
                continue
            text = text.replace(pid, original)
        return text

    @staticmethod
    def _restore_newlines(text: str, placeholders: Dict[str, str]) -> str:
        """ê°œí–‰ ë¬¸ì placeholderë“¤ì„ ì›ë˜ ë¬¸ìë¡œ ë³µì›í•©ë‹ˆë‹¤."""
        if not isinstance(text, str):
            return text

        # [NEWLINE] í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì €ì¥ëœ ê°œí–‰ ë¬¸ìë¡œ ë³µì›
        if "[NEWLINE]" in placeholders:
            text = text.replace("[NEWLINE]", placeholders["[NEWLINE]"])

        return text

    @staticmethod
    def _restore_placeholders_in_string(
        text: str, sorted_placeholders: List[tuple[str, str]], newline_value: str | None
    ) -> str:
        """Helper to restore placeholders in a single string using a pre-sorted list."""
        if not isinstance(text, str):
            return text

        if newline_value and "[NEWLINE]" in text:
            text = text.replace("[NEWLINE]", newline_value)

        # Then iterate through the pre-sorted list
        for pid, original in sorted_placeholders:
            if pid in text:
                text = text.replace(pid, original)
        return text

    @staticmethod
    def restore_placeholders_in_json(
        json_obj: Any,
        sorted_placeholders: List[tuple[str, str]],
        newline_value: str | None,
    ) -> Any:
        """JSON ê°ì²´ ë ˆë²¨ì—ì„œ ì•ˆì „í•˜ê²Œ placeholderë¥¼ ë³µì›í•©ë‹ˆë‹¤. (ìµœì í™”ëœ ë²„ì „)"""
        if isinstance(json_obj, dict):
            return {
                k: PlaceholderManager.restore_placeholders_in_json(
                    v, sorted_placeholders, newline_value
                )
                for k, v in json_obj.items()
            }
        elif isinstance(json_obj, (list, tuple)):
            return [
                PlaceholderManager.restore_placeholders_in_json(
                    i, sorted_placeholders, newline_value
                )
                for i in json_obj
            ]
        elif isinstance(json_obj, str):
            return PlaceholderManager._restore_placeholders_in_string(
                json_obj, sorted_placeholders, newline_value
            )
        else:
            return json_obj

    @staticmethod
    def extract_placeholders_from_text(text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í”Œë ˆì´ìŠ¤í™€ë” íŒ¨í„´ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not isinstance(text, str):
            return []

        placeholders = []

        for pattern in PlaceholderManager._PLACEHOLDER_PATTERNS:
            matches = re.findall(pattern, text)
            for match in matches:
                if isinstance(match, tuple):
                    # ì •ê·œì‹ ê·¸ë£¹ì´ ìˆëŠ” ê²½ìš°, ë¹„ì–´ìˆì§€ ì•Šì€ ì²« ë²ˆì§¸ ê·¸ë£¹ ì„ íƒ
                    match_str = next((group for group in match if group), match[0])
                else:
                    match_str = match

                if match_str and match_str not in placeholders:
                    placeholders.append(match_str)

        return placeholders

    @staticmethod
    def validate_placeholder_preservation(original: str, translated: str) -> bool:
        """ì›ë¬¸ì˜ í”Œë ˆì´ìŠ¤í™€ë”ë“¤ì´ ë²ˆì—­ë¬¸ì—ì„œë„ ë³´ì¡´ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤."""
        if not isinstance(original, str) or not isinstance(translated, str):
            return True

        # ë‚´ë¶€ í”Œë ˆì´ìŠ¤í™€ë” ë§ˆì»¤ë¥¼ ì§ì ‘ ë¹„êµ
        original_placeholders = sorted(
            PlaceholderManager._extract_internal_placeholders(original)
        )
        translated_placeholders = sorted(
            PlaceholderManager._extract_internal_placeholders(translated)
        )

        return original_placeholders == translated_placeholders

    @staticmethod
    def get_missing_placeholders(original: str, translated: str) -> List[str]:
        """ì›ë¬¸ì—ëŠ” ìˆì§€ë§Œ ë²ˆì—­ë¬¸ì—ì„œ ë¹ ì§„ í”Œë ˆì´ìŠ¤í™€ë”ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not isinstance(original, str) or not isinstance(translated, str):
            return []

        original_placeholders = set(
            PlaceholderManager._extract_internal_placeholders(original)
        )
        translated_placeholders = set(
            PlaceholderManager._extract_internal_placeholders(translated)
        )

        missing = list(original_placeholders - translated_placeholders)
        return missing

    @staticmethod
    def _extract_internal_placeholders(text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë‚´ë¶€ í”Œë ˆì´ìŠ¤í™€ë” ë§ˆì»¤([PXXX] ë˜ëŠ” [NEWLINE])ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not isinstance(text, str):
            return []

        placeholders = re.findall(
            PlaceholderManager._INTERNAL_PLACEHOLDER_PATTERN, text
        )
        newlines = re.findall(PlaceholderManager._INTERNAL_NEWLINE_PATTERN, text)
        return placeholders + newlines


class TokenOptimizer:
    """Helpers for ID substitution and token-count heuristics."""

    _id_counter = 0  # í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ID ì¹´ìš´í„° ê´€ë¦¬

    @staticmethod
    def reset_id_counter() -> None:
        """ID ì¹´ìš´í„°ë¥¼ ë¦¬ì…‹í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ë²ˆì—­ ì‘ì—… ì‹œì‘ ì‹œ í˜¸ì¶œí•˜ì„¸ìš”."""
        TokenOptimizer._id_counter = 0

    @staticmethod
    def format_chunk_for_llm(chunk: List[Dict[str, str]]) -> str:
        """JSON ì²­í¬ë¥¼ LLMì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·í•©ë‹ˆë‹¤."""
        if not chunk:
            return "No items."

        lines = [f"TEXTS ({len(chunk)}):"]

        for i, item in enumerate(chunk, 1):
            text_id = item.get("id", f"item_{i}")
            original_text = item.get("original", "")
            lines.append(f"{i}. [{text_id}]\n```\n{original_text}\n```")

        return "\n\n".join(lines)

    @staticmethod
    def format_glossary_for_llm(glossary_entries: List[GlossaryEntry]) -> str:
        """ê¸€ë¡œì‹œë¦¬ë¥¼ LLMì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·í•©ë‹ˆë‹¤."""
        if not glossary_entries:
            return "No glossary."

        lines = [f"GLOSSARY ({len(glossary_entries)}):"]

        for i, entry in enumerate(glossary_entries, 1):
            original = entry.original
            meanings = []

            for meaning in entry.meanings:
                translation = meaning.translation
                context = meaning.context
                if context and context != "ê¸°ì¡´ ë²ˆì—­":
                    meanings.append(f"{translation} (Context: {context})")
                else:
                    meanings.append(translation)

            meanings_str = " / ".join(meanings)
            lines.append(f"{i}. {original} -> {meanings_str}")

        return "\n".join(lines)

    @staticmethod
    def deduplicate_glossary_meanings(meanings: List[TermMeaning]) -> List[TermMeaning]:
        """ê¸€ë¡œì‹œë¦¬ ì˜ë¯¸ë“¤ì—ì„œ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤. ê°™ì€ ë²ˆì—­ì´ë©´ contextê°€ ë‹¬ë¼ë„ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤."""
        if not meanings:
            return meanings

        original_count = len(meanings)
        seen_translations = set()
        deduplicated = []

        for meaning in meanings:
            translation_key = meaning.translation.lower().strip()

            if translation_key not in seen_translations:
                seen_translations.add(translation_key)
                deduplicated.append(meaning)

        # ì¤‘ë³µ ì œê±° ê²°ê³¼ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        removed_count = original_count - len(deduplicated)
        if removed_count > 0:
            logger.debug(
                f"ê¸€ë¡œì‹œë¦¬ ì˜ë¯¸ ì¤‘ë³µ ì œê±°: {original_count}ê°œ â†’ {len(deduplicated)}ê°œ ({removed_count}ê°œ ì œê±°)"
            )

        return deduplicated

    @staticmethod
    def merge_glossary_entry_meanings(
        existing_meanings: List[TermMeaning], new_meanings: List[TermMeaning]
    ) -> List[TermMeaning]:
        """ê¸°ì¡´ ì˜ë¯¸ë“¤ê³¼ ìƒˆë¡œìš´ ì˜ë¯¸ë“¤ì„ ë³‘í•©í•˜ë©´ì„œ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤."""
        # ê¸°ì¡´ ë²ˆì—­ë“¤ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•´ì„œ ì§‘í•©ì— ì €ì¥
        existing_translations = {
            meaning.translation.lower().strip() for meaning in existing_meanings
        }

        # ìƒˆë¡œìš´ ì˜ë¯¸ë“¤ ì¤‘ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ ì¶”ê°€
        merged_meanings = existing_meanings.copy()

        for new_meaning in new_meanings:
            translation_key = new_meaning.translation.lower().strip()
            if translation_key not in existing_translations:
                merged_meanings.append(new_meaning)
                existing_translations.add(translation_key)

        # ìµœì¢…ì ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µì„ ìœ„í•´)
        return TokenOptimizer.deduplicate_glossary_meanings(merged_meanings)

    @staticmethod
    def replace_text_with_ids(json_obj: Any, id_map: Dict[str, str]) -> Any:
        if isinstance(json_obj, dict):
            return {
                k: TokenOptimizer.replace_text_with_ids(v, id_map)
                for k, v in json_obj.items()
            }
        if isinstance(json_obj, list):
            return [TokenOptimizer.replace_text_with_ids(i, id_map) for i in json_obj]
        if isinstance(json_obj, str) and json_obj.strip():
            # ê°„ë‹¨í•œ ID ìƒì„± (T001, T002, ...) - ì´ 4ì
            TokenOptimizer._id_counter += 1
            text_id = f"T{TokenOptimizer._id_counter:03d}"
            id_map[text_id] = json_obj
            return text_id
        return json_obj

    @staticmethod
    def optimize_json_for_translation(data: Dict[str, Any]) -> List[str]:
        texts: List[str] = []

        def collect(obj: Any) -> None:
            if isinstance(obj, dict):
                for v in obj.values():
                    collect(v)
            elif isinstance(obj, list):
                for i in obj:
                    collect(i)
            elif isinstance(obj, str) and obj.strip():
                # ìƒˆë¡œìš´ íŒ¨í„´: T001, T002 ë“±ì˜ ê°„ë‹¨í•œ IDì™€ [P...], [NEWLINE] ì œì™¸
                if not re.match(r"^T\d{3,}$", obj) and not re.match(
                    r"^\[(P\d{3,}|NEWLINE)\]$", obj
                ):
                    texts.append(obj)

        collect(data)
        return list(set(texts))

    @staticmethod
    def estimate_tokens(text: str) -> int:
        return len(text) // 4 + 1

    @staticmethod
    def create_text_chunks(
        items: List[Dict[str, str]], max_tokens_per_chunk: int = 3000
    ) -> List[List[Dict[str, str]]]:
        chunks: List[List[Dict[str, str]]] = []
        current_chunk: List[Dict[str, str]] = []
        current_tokens = 0
        prompt_overhead = 500
        effective_max = max_tokens_per_chunk - prompt_overhead

        for item in items:
            text_tokens = TokenOptimizer.estimate_tokens(item["original"])
            if text_tokens > effective_max:
                if current_chunk:
                    chunks.append(current_chunk)
                chunks.append([item])
                current_chunk = []
                current_tokens = 0
            elif current_tokens + text_tokens > effective_max:
                chunks.append(current_chunk)
                current_chunk = [item]
                current_tokens = text_tokens
            else:
                current_chunk.append(item)
                current_tokens += text_tokens

        if current_chunk:
            chunks.append(current_chunk)
        return chunks

    @staticmethod
    def handle_oversized_text(text: str, max_tokens: int) -> str:  # noqa: D401
        tokens = TokenOptimizer.estimate_tokens(text)
        if tokens > max_tokens:
            logger.warning(
                "âš ï¸  Single text contains %s tokens, exceeding limit (%s).",
                tokens,
                max_tokens,
            )
            logger.warning("   Preview: %s...", text[:100])
            logger.warning(
                "   The text will be sent as-is. Ensure the LLM can handle it."
            )
        return text


###############################################################################
# 3. LangGraph node functions (async)                                         #
###############################################################################


async def invoke_with_structured_output_fallback(llm_client: BaseLLM, schema, prompt):
    """êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ LLM í˜¸ì¶œ, ì‹¤íŒ¨ ì‹œ PydanticOutputParserë¡œ í´ë°±"""

    # ì²« ë²ˆì§¸ ì‹œë„: with_structured_output ì‚¬ìš©
    try:
        structured_llm = llm_client.with_structured_output(schema, include_raw=True)

        response = await structured_llm.ainvoke(prompt)
        result = response["parsed"]

        # ê²°ê³¼ê°€ Noneì´ ì•„ë‹Œì§€ í™•ì¸
        if result is not None:
            logger.debug("êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„±ê³µ")
            return result
        else:
            logger.warning(
                "êµ¬ì¡°í™”ëœ ì¶œë ¥ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤. PydanticOutputParserë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤."
            )
            raise ValueError("êµ¬ì¡°í™”ëœ ì¶œë ¥ ê²°ê³¼ê°€ None")

    except Exception as e:
        logger.warning(f"êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‹¤íŒ¨: {e}. PydanticOutputParserë¡œ í´ë°±í•©ë‹ˆë‹¤.")

    # ë‘ ë²ˆì§¸ ì‹œë„: PydanticOutputParser ì‚¬ìš©
    try:
        parser = PydanticOutputParser(pydantic_object=schema)

        # í”„ë¡¬í”„íŠ¸ì— íŒŒì„œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
        enhanced_prompt = f"{prompt}\n\n{parser.get_format_instructions()}"

        response = await llm_client.ainvoke(enhanced_prompt)
        result = parser.parse(response.content)

        if result is not None:
            logger.debug("PydanticOutputParserë¡œ íŒŒì‹± ì„±ê³µ")
            return result
        else:
            logger.error("PydanticOutputParser ê²°ê³¼ë„ Noneì…ë‹ˆë‹¤.")
            return None

    except Exception as e:
        logger.error(f"PydanticOutputParserë„ ì‹¤íŒ¨: {e}")
        return None


def _filter_relevant_glossary_terms(
    chunk: List[Dict[str, str]], all_glossary_terms: List[GlossaryEntry]
) -> List[GlossaryEntry]:
    """í•´ë‹¹ ì²­í¬ì— í¬í•¨ëœ ìš©ì–´ë“¤ë§Œ ê¸€ë¡œì‹œë¦¬ì—ì„œ í•„í„°ë§"""
    if not all_glossary_terms:
        return []

    # ì²­í¬ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    chunk_text = " ".join(item["original"].lower() for item in chunk)

    # ì²­í¬ì— í¬í•¨ëœ ìš©ì–´ë“¤ë§Œ í•„í„°ë§
    relevant_terms = []
    for term in all_glossary_terms:
        # ì›ë³¸ ìš©ì–´ê°€ ì²­í¬ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if term.original.lower() in chunk_text:
            relevant_terms.append(term)

    return relevant_terms


async def parse_and_extract_node(state: TranslatorState) -> TranslatorState:  # noqa: D401
    try:
        # ìƒˆë¡œìš´ ë²ˆì—­ ì‘ì—… ì‹œì‘ ì‹œ ID ë° í”Œë ˆì´ìŠ¤í™€ë” ì¹´ìš´í„° ë¦¬ì…‹
        TokenOptimizer.reset_id_counter()
        PlaceholderManager.reset_counter()

        placeholders: Dict[str, str] = {}
        json_with_placeholders = PlaceholderManager.process_json_object(
            state["parsed_json"], placeholders
        )
        state["placeholders"] = placeholders

        id_to_text: Dict[str, str] = {}
        json_with_ids = TokenOptimizer.replace_text_with_ids(
            json_with_placeholders, id_to_text
        )

        state["id_to_text_map"] = id_to_text
        state["processed_json"] = json_with_ids

        logger.info(_m("translator.found_items", count=len(id_to_text)))
        return state
    except Exception as exc:
        state["error"] = f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {exc}"
        return state


async def extract_terms_from_json_chunks_node(
    state: TranslatorState,
) -> TranslatorState:
    """Extracts terms by analyzing the full JSON in chunks for contextual accuracy."""
    try:
        # ì—¬ëŸ¬ ì‚¬ì „ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë³‘í•©
        vanilla_glossary = state.get("vanilla_glossary", [])
        primary_glossary = state.get("primary_glossary", [])
        existing_important_terms = state.get("important_terms", [])

        merged_glossary: Dict[str, GlossaryEntry] = {}

        # 1. ê¸°ì¡´ ì¤‘ìš” ìš©ì–´ë“¤ ì¶”ê°€ (ê¸°ë³¸ ì‚¬ì „)
        for term in existing_important_terms:
            merged_glossary[term.original.lower()] = term

        # 2. ë°”ë‹ë¼ ì‚¬ì „ ìš©ì–´ë“¤ ë³‘í•© (ìµœê³  ìš°ì„ ìˆœìœ„)
        vanilla_added_count = 0
        for vanilla_term in vanilla_glossary:
            key = vanilla_term.original.lower()
            if key in merged_glossary:
                # ê¸°ì¡´ ìš©ì–´ì— ë°”ë‹ë¼ ì˜ë¯¸ë“¤ ì¶”ê°€ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                # ë°”ë‹ë¼ ì˜ë¯¸ë¥¼ ì•ìª½ì— ì¶”ê°€í•˜ê³  ì¤‘ë³µ ì œê±°
                vanilla_meanings = TokenOptimizer.deduplicate_glossary_meanings(
                    vanilla_term.meanings
                )
                existing_meanings = merged_glossary[key].meanings

                # ë°”ë‹ë¼ ì˜ë¯¸ë¥¼ ì•ì—, ê¸°ì¡´ ì˜ë¯¸ë¥¼ ë’¤ì— ë°°ì¹˜í•œ í›„ ì¤‘ë³µ ì œê±°
                combined_meanings = vanilla_meanings + existing_meanings
                merged_glossary[
                    key
                ].meanings = TokenOptimizer.deduplicate_glossary_meanings(
                    combined_meanings
                )
            else:
                # ìƒˆë¡œìš´ ë°”ë‹ë¼ ìš©ì–´ ì¶”ê°€ (ì˜ë¯¸ ì¤‘ë³µ ì œê±°)
                deduplicated_term = GlossaryEntry(
                    original=vanilla_term.original,
                    meanings=TokenOptimizer.deduplicate_glossary_meanings(
                        vanilla_term.meanings
                    ),
                )
                merged_glossary[key] = deduplicated_term
                vanilla_added_count += 1

        # 3. 1ì°¨ ì‚¬ì „ ìš©ì–´ë“¤ ë³‘í•© (ê¸°ì¡´ ë²ˆì—­ ë°ì´í„°)
        primary_added_count = 0
        for primary_term in primary_glossary:
            key = primary_term.original.lower()
            if key in merged_glossary:
                # ê¸°ì¡´ ìš©ì–´ì— ìƒˆë¡œìš´ ì˜ë¯¸ë“¤ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                merged_glossary[
                    key
                ].meanings = TokenOptimizer.merge_glossary_entry_meanings(
                    merged_glossary[key].meanings, primary_term.meanings
                )
            else:
                # ìƒˆë¡œìš´ ìš©ì–´ ì¶”ê°€ (ì˜ë¯¸ ì¤‘ë³µ ì œê±°)
                deduplicated_term = GlossaryEntry(
                    original=primary_term.original,
                    meanings=TokenOptimizer.deduplicate_glossary_meanings(
                        primary_term.meanings
                    ),
                )
                merged_glossary[key] = deduplicated_term
                primary_added_count += 1

        # ë³‘í•©ëœ ì‚¬ì „ì„ ìƒíƒœì— ì €ì¥
        state["important_terms"] = list(merged_glossary.values())

        # ë³‘í•© ê²°ê³¼ ë¡œê¹…
        if vanilla_added_count > 0:
            logger.info(
                f"ë°”ë‹ë¼ ì‚¬ì „ì—ì„œ {vanilla_added_count}ê°œ ìƒˆë¡œìš´ ìš©ì–´ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤."
            )
        if primary_added_count > 0:
            logger.info(
                f"1ì°¨ ì‚¬ì „ì—ì„œ {primary_added_count}ê°œ ìƒˆë¡œìš´ ìš©ì–´ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤."
            )

        all_texts = "\n".join(state["id_to_text_map"].values())
        if not all_texts:
            logger.info(_m("translator.contextual_terms_no_new"))
            return state

        # Rough chunking by character count; can be improved
        chunk_size = 2000
        # í…ìŠ¤íŠ¸ë¥¼ \nìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì²­í¬ í¬ê¸°ì— ë§ê²Œ ë¶„í• 
        chunks = []
        lines = all_texts.split("\n")
        current_chunk = []
        current_size = 0

        placeholder_pattern = r"\[(P\d{3,}|NEWLINE)\]"

        for line in lines:
            # placeholder íŒ¨í„´ì„ ë¹ˆ ë¬¸ìì—´ë¡œ êµì²´
            line = re.sub(placeholder_pattern, "", line)

            # ë¹ˆ ë¬¸ìì—´ì´ë©´ ê±´ë„ˆë›°ê¸°
            if not line.strip():
                continue

            # í˜„ì¬ ë¼ì¸ì„ ì¶”ê°€í–ˆì„ ë•Œ ì²­í¬ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
            line_size = len(line)
            if current_size + line_size > chunk_size and current_chunk:
                # í˜„ì¬ ì²­í¬ë¥¼ ì™„ì„±í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                chunks.append("\n".join(current_chunk))
                current_chunk = [line]
                current_size = line_size
            else:
                # í˜„ì¬ ì²­í¬ì— ë¼ì¸ ì¶”ê°€
                current_chunk.append(line)
                current_size += line_size + 1  # +1 for \n

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append("\n".join(current_chunk))

        logger.info(_m("translator.contextual_terms_start", count=len(chunks)))

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ìš©ì–´ ì¶”ì¶œ ì‹œì‘)
        progress_callback = state.get("progress_callback")
        if progress_callback:
            base_msg = (
                f"ğŸ” {len(chunks)}ê°œ JSON ì²­í¬ì˜ ë¬¸ë§¥ì„ ë¶„ì„í•˜ì—¬ ìš©ì–´ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤"
            )
            if primary_added_count > 0:
                base_msg += f" (1ì°¨ ì‚¬ì „ {primary_added_count}ê°œ ìš©ì–´ í¬í•¨)"
            progress_callback(
                base_msg,
                0,
                len(chunks),
                f"ì´ {len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì˜ˆì •",
            )

        sem = asyncio.Semaphore(state["max_concurrent_requests"])

        # ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì™„ì „ ë³‘ë ¬ ëŒ€ì‹ )
        tasks = []
        for chunk_idx, chunk in enumerate(chunks):
            tasks.append(
                _extract_terms_from_chunk_worker_with_progress(
                    text_chunk=chunk,
                    target_language=state["target_language"],
                    semaphore=sem,
                    chunk_idx=chunk_idx,
                    total_chunks=len(chunks),
                    progress_callback=progress_callback,
                    max_retries=state.get(
                        "max_retries", 3
                    ),  # ë©”ì¸ ì„¤ì •ì˜ ì¬ì‹œë„ íšŸìˆ˜ ì‚¬ìš©
                    existing_glossary=list(
                        merged_glossary.values()
                    ),  # 1ì°¨ ì‚¬ì „ì„ LLMì— ì œê³µ
                    llm_client=state.get("llm_client"),  # LLM í´ë¼ì´ì–¸íŠ¸ ì „ë‹¬
                )
            )

        glossaries = await asyncio.gather(*tasks)

        # Merge glossaries from all chunks
        new_terms_count = 0

        for glossary in glossaries:
            for term in glossary.terms:
                key = term.original.lower()
                if key not in merged_glossary:
                    # ìƒˆë¡œìš´ ìš©ì–´ ì¶”ê°€ (ì˜ë¯¸ ì¤‘ë³µ ì œê±°)
                    deduplicated_term = GlossaryEntry(
                        original=term.original,
                        meanings=TokenOptimizer.deduplicate_glossary_meanings(
                            term.meanings
                        ),
                    )
                    merged_glossary[key] = deduplicated_term
                    new_terms_count += 1
                else:
                    # ê¸°ì¡´ ìš©ì–´ì— ìƒˆë¡œìš´ ì˜ë¯¸ë“¤ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                    merged_glossary[
                        key
                    ].meanings = TokenOptimizer.merge_glossary_entry_meanings(
                        merged_glossary[key].meanings, term.meanings
                    )

        state["important_terms"] = list(merged_glossary.values())

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ìš©ì–´ ì¶”ì¶œ ì™„ë£Œ)
        if progress_callback:
            progress_callback(
                "ğŸ” ìš©ì–´ ì¶”ì¶œ ì™„ë£Œ",
                len(chunks),
                len(chunks),
                f"LLM ë¶„ì„ìœ¼ë¡œ ìƒˆë¡œìš´ ìš©ì–´ {new_terms_count}ê°œ ì¶”ê°€ë¨"
                if new_terms_count > 0
                else "LLM ë¶„ì„ ì™„ë£Œ, ìƒˆë¡œìš´ ìš©ì–´ ì—†ìŒ",
            )

        if new_terms_count > 0:
            logger.info(_m("translator.contextual_terms_finish", count=new_terms_count))
        else:
            logger.info(_m("translator.contextual_terms_no_new"))

        total_terms = len(merged_glossary)
        logger.info(
            f"ìµœì¢… ì‚¬ì „ í¬ê¸°: {total_terms}ê°œ ìš©ì–´ (1ì°¨ ì‚¬ì „: {len(primary_glossary)}ê°œ, LLM ì¶”ê°€: {new_terms_count}ê°œ)"
        )

        return state

    except Exception as exc:
        logger.error(_m("translator.contextual_terms_main_error", error=exc))
        # Non-critical, proceed without new terms
        return state


async def _extract_terms_from_chunk_worker_with_progress(
    *,
    text_chunk: str,
    target_language: str,
    semaphore: asyncio.Semaphore,
    chunk_idx: int,
    total_chunks: int,
    progress_callback: Optional[callable] = None,
    max_retries: int = 3,
    existing_glossary: List[GlossaryEntry] = None,
    llm_client: Any = None,
) -> Glossary:
    """Worker to extract glossary terms from a single JSON chunk with progress reporting and retry logic."""

    if existing_glossary is None:
        existing_glossary = []

    async with semaphore:
        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì²­í¬ ì²˜ë¦¬ ì‹œì‘)
        if progress_callback:
            progress_callback(
                "ğŸ” JSON ì²­í¬ ë¶„ì„ ì¤‘",
                chunk_idx,
                total_chunks,
                f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ìš©ì–´ ì¶”ì¶œ ì¤‘",
            )

        # 1ì°¨ ì‚¬ì „ ì •ë³´ë¥¼ LLMì— ì œê³µí•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
        existing_glossary_text = ""
        if existing_glossary:
            # ì²­í¬ í…ìŠ¤íŠ¸ì— ì‹¤ì œë¡œ í¬í•¨ëœ ìš©ì–´ë“¤ë§Œ í•„í„°ë§
            chunk_text_lower = text_chunk.lower()
            relevant_existing_terms = [
                term
                for term in existing_glossary
                if term.original.lower() in chunk_text_lower
            ]

            if relevant_existing_terms:
                existing_glossary_text = TokenOptimizer.format_glossary_for_llm(
                    relevant_existing_terms
                )

        # ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
        last_error = None
        for attempt in range(max_retries + 1):  # 0ë²ˆ ì‹œë„ë¶€í„° max_retriesê¹Œì§€
            try:
                if attempt > 0:
                    prompt = retry_contextual_terms_prompt(
                        target_language, text_chunk, existing_glossary_text
                    )
                else:
                    prompt = contextual_terms_prompt(
                        target_language, text_chunk, existing_glossary_text
                    )
                # ì¬ì‹œë„ ì‹œì—ëŠ” temperatureë¥¼ ì¡°ê¸ˆì”© ì˜¬ë¦¼ (ìµœëŒ€ 1.0ê¹Œì§€)
                temperature = 0 if attempt == 0 else min(1.0, attempt * 0.1)

                # LLM í´ë¼ì´ì–¸íŠ¸ ê²€ì¦
                if llm_client is None:
                    logger.error("LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    return Glossary(terms=[])

                llm = llm_client
                if attempt > 0:
                    logger.info(
                        f"ğŸ”„ ì²­í¬ {chunk_idx + 1} ìš©ì–´ ì¶”ì¶œ ì¬ì‹œë„ {attempt}/{max_retries} (temperature={temperature})"
                    )
                    # ì¬ì‹œë„ ì‹œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    if progress_callback:
                        progress_callback(
                            "ğŸ” JSON ì²­í¬ ë¶„ì„ ì¤‘",
                            chunk_idx,
                            total_chunks,
                            f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ì¬ì‹œë„ ì¤‘ ({attempt}/{max_retries})",
                        )

                # SimpleGlossaryTermì„ ë„êµ¬ë¡œ ë°”ì¸ë”©í•˜ì—¬ LLM í˜¸ì¶œ
                llm_with_tools = llm.bind_tools([SimpleGlossaryTerm])
                response = await llm_with_tools.ainvoke(prompt)

                # LLMì˜ ë„êµ¬ í˜¸ì¶œì—ì„œ SimpleGlossaryTerm ì¶”ì¶œ
                simple_terms: List[SimpleGlossaryTerm] = []
                if response.tool_calls:
                    for tool_call in response.tool_calls:
                        if tool_call["name"] == "SimpleGlossaryTerm":
                            try:
                                term = SimpleGlossaryTerm(**tool_call["args"])
                                simple_terms.append(term)
                            except Exception as e:
                                logger.warning(
                                    f"SimpleGlossaryTerm íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì¬ì‹œë„ë¥¼ ìœ ë°œí•©ë‹ˆë‹¤: {e}, args: {tool_call['args']}"
                                )
                                raise

                # SimpleGlossaryTerm ë¦¬ìŠ¤íŠ¸ë¥¼ GlossaryEntry ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì§‘ê³„)
                aggregated_entries: Dict[str, GlossaryEntry] = {}
                for term in simple_terms:
                    key = term.original.lower()
                    if key not in aggregated_entries:
                        aggregated_entries[key] = GlossaryEntry(
                            original=term.original, meanings=[]
                        )

                    aggregated_entries[key].meanings.append(
                        TermMeaning(translation=term.translation, context=term.context)
                    )

                result = Glossary(terms=list(aggregated_entries.values()))

                # ì„±ê³µ ì‹œ ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
                if progress_callback:
                    success_msg = f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ì™„ë£Œ"
                    if attempt > 0:
                        success_msg += f" (ì¬ì‹œë„ {attempt}íšŒ í›„ ì„±ê³µ)"
                    success_msg += (
                        f" - {len(result.terms) if result else 0}ê°œ ìš©ì–´ ë°œê²¬"
                    )

                    progress_callback(
                        "ğŸ” JSON ì²­í¬ ë¶„ì„ ì¤‘",
                        chunk_idx + 1,
                        total_chunks,
                        success_msg,
                    )

                if attempt > 0:
                    logger.info(f"âœ… ì²­í¬ {chunk_idx + 1} ìš©ì–´ ì¶”ì¶œ ì¬ì‹œë„ ì„±ê³µ")

                return result or Glossary(terms=[])

            except Exception as exc:
                last_error = exc
                logger.warning(
                    f"âš ï¸ ì²­í¬ {chunk_idx + 1} ìš©ì–´ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries + 1}): {exc}"
                )

                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì ì‹œ ëŒ€ê¸°
                if attempt < max_retries:
                    await asyncio.sleep(
                        min(2.0, (attempt + 1) * 0.5)
                    )  # 0.5ì´ˆ, 1ì´ˆ, 1.5ì´ˆ, 2ì´ˆ ëŒ€ê¸°

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ
        logger.error(
            f"âŒ ì²­í¬ {chunk_idx + 1} ìš©ì–´ ì¶”ì¶œ {max_retries + 1}íšŒ ëª¨ë‘ ì‹¤íŒ¨: {last_error}"
        )

        # ì‹¤íŒ¨í•´ë„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if progress_callback:
            progress_callback(
                "ğŸ” JSON ì²­í¬ ë¶„ì„ ì¤‘",
                chunk_idx + 1,
                total_chunks,
                f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ì‹¤íŒ¨ (ì¬ì‹œë„ {max_retries}íšŒ í›„)",
            )

        return Glossary(terms=[])


def should_create_glossary(state: TranslatorState) -> str:
    """Determines whether to proceed with glossary creation."""
    if state.get("use_glossary"):
        return "create_glossary"
    logger.info(_m("translator.skipping_glossary"))
    return "skip_glossary"


def should_save_glossary(state: TranslatorState) -> str:
    """Determines whether to save the glossary at the end."""
    if state.get("use_glossary") and state.get("glossary_path"):
        return "save_glossary"
    return "end"


async def smart_translate_node(state: TranslatorState) -> TranslatorState:
    try:
        # LLM í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        llm = state.get("llm_client")
        if llm is None:
            logger.error("LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            state["error"] = "LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            return state
        id_map = state["id_to_text_map"]

        if not id_map:
            state["translation_map"] = {}
            return state

        items = [{"id": k, "original": v} for k, v in id_map.items()]
        chunks = TokenOptimizer.create_text_chunks(items, state["max_tokens_per_chunk"])

        logger.info(
            _m(
                "translator.chunks_split",
                chunks=len(chunks),
                concurrent=state["max_concurrent_requests"],
            )
        )

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ë²ˆì—­ ì‹œì‘)
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ“ ë²ˆì—­ ì§„í–‰ ì¤‘", 0, len(chunks), f"ì´ {len(chunks)}ê°œ ì²­í¬ ë²ˆì—­ ì‹œì‘"
            )

        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])
        sem = asyncio.Semaphore(state["max_concurrent_requests"])

        tasks = [
            _translate_chunk_worker_with_progress(
                chunk=c,
                state=state,
                llm=llm,
                target_language=state["target_language"],
                delay_manager=delay_mgr,
                semaphore=sem,
                chunk_num=i,
                total_chunks=len(chunks),
                progress_callback=progress_callback,
            )
            for i, c in enumerate(chunks, 1)
        ]
        results = await asyncio.gather(*tasks)

        translation_map: Dict[str, str] = {}
        for res in results:
            for item in res:
                translation_map[item.id] = item.translated

        state["translation_map"] = translation_map

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ë²ˆì—­ ì™„ë£Œ)
        if progress_callback:
            progress_callback(
                "ğŸ“ ë²ˆì—­ ì™„ë£Œ",
                len(chunks),
                len(chunks),
                f"ì´ {len(translation_map)}ê°œ í•­ëª© ë²ˆì—­ ì™„ë£Œ",
            )

        return state
    except Exception as exc:
        state["error"] = f"ë²ˆì—­ ì˜¤ë¥˜: {exc}"
        return state


async def _translate_chunk_worker_with_progress(
    *,
    chunk: List[Dict[str, str]],
    state: TranslatorState,
    llm: Any,
    target_language: str,
    delay_manager: RequestDelayManager,
    semaphore: asyncio.Semaphore,
    chunk_num: int,
    total_chunks: int,
    progress_callback: Optional[callable] = None,
    is_retry: bool = False,
    temperature: float = 0.0,
) -> List[TranslatedItem]:
    """ë²ˆì—­ ì²­í¬ ì›Œì»¤ (ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ í¬í•¨)"""
    # ì „ì²´ ê¸€ë¡œì‹œë¦¬ì—ì„œ ì´ ì²­í¬ì— ê´€ë ¨ëœ ìš©ì–´ë“¤ë§Œ í•„í„°ë§
    all_glossary_terms = state.get("important_terms", [])
    relevant_glossary = _filter_relevant_glossary_terms(chunk, all_glossary_terms)

    log_prefix = "retry" if is_retry else "translation"
    async with semaphore:
        await delay_manager.wait()

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì²­í¬ ë²ˆì—­ ì‹œì‘)
        if progress_callback:
            progress_callback(
                "ğŸ“ ë²ˆì—­ ì§„í–‰ ì¤‘",
                chunk_num - 1,
                total_chunks,
                f"ì²­í¬ {chunk_num}/{total_chunks} ë²ˆì—­ ì¤‘ ({len(chunk)}ê°œ í•­ëª©)",
            )

        logger.info(
            _m(
                "translator.chunk_start",
                current=chunk_num,
                total=total_chunks,
                kind=log_prefix,
            )
        )

        # ê¸€ë¡œì‹œë¦¬ í•„í„°ë§ ê²°ê³¼ ë¡œê¹…
        if all_glossary_terms:
            logger.info(
                f"ğŸ“š ì²­í¬ {chunk_num}: ì „ì²´ ê¸€ë¡œì‹œë¦¬ {len(all_glossary_terms)}ê°œ ì¤‘ "
                f"{len(relevant_glossary)}ê°œ ìš©ì–´ í¬í•¨"
            )

        glossary_str = TokenOptimizer.format_glossary_for_llm(relevant_glossary)
        chunk_str = TokenOptimizer.format_chunk_for_llm(chunk)

        # ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì¬ì‹œë„ ì—¬ë¶€ ë°˜ì˜)
        if is_retry:
            prompt = retry_translation_prompt(
                state["target_language"], glossary_str, chunk_str
            )
        else:
            prompt = translation_prompt(
                state["target_language"], glossary_str, chunk_str
            )

        try:
            # TranslatedItemì„ ë„êµ¬ë¡œ ë°”ì¸ë”©í•˜ì—¬ LLM í˜¸ì¶œ
            llm_with_tools = llm.bind_tools([TranslatedItem])
            response = await llm_with_tools.ainvoke(prompt)

            # LLMì˜ ë„êµ¬ í˜¸ì¶œì—ì„œ TranslatedItem ì¶”ì¶œ
            translations = []
            if response.tool_calls:
                for tool_call in response.tool_calls:
                    if tool_call["name"] == "TranslatedItem":
                        try:
                            item = TranslatedItem(**tool_call["args"])
                            translations.append(item)
                        except Exception as e:
                            logger.warning(
                                f"TranslatedItem íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}, args: {tool_call['args']}"
                            )

            # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì²­í¬ ë²ˆì—­ ì™„ë£Œ)
            if progress_callback:
                progress_callback(
                    "ğŸ“ ë²ˆì—­ ì§„í–‰ ì¤‘",
                    chunk_num,
                    total_chunks,
                    f"ì²­í¬ {chunk_num}/{total_chunks} ì™„ë£Œ ({len(translations)}ê°œ í•­ëª©)",
                )

            return translations
        except Exception as exc:
            logger.error(f"ì²­í¬ {chunk_num} ë²ˆì—­ ì‹¤íŒ¨: {exc}")
            return []


def restore_placeholders_node(state: TranslatorState) -> TranslatorState:  # noqa: D401
    try:
        logger.info("í”Œë ˆì´ìŠ¤í™€ë” ë³µì› ì‹œì‘...")

        placeholders = state["placeholders"]
        newline_value = placeholders.get("[NEWLINE]")

        # Sort placeholders ONCE, excluding newline
        sorted_placeholders = sorted(
            (item for item in placeholders.items() if item[0] != "[NEWLINE]"),
            key=lambda item: (int(item[0][2:-1]) if item[0].startswith("[P") else -1),
            reverse=True,
        )

        # JSON ê°ì²´ ë ˆë²¨ì—ì„œ ì•ˆì „í•˜ê²Œ placeholder ë³µì›
        restored_json = PlaceholderManager.restore_placeholders_in_json(
            state["translated_json"], sorted_placeholders, newline_value
        )

        # ë³µì›ëœ JSON ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        state["final_json"] = json.dumps(restored_json, ensure_ascii=False, indent=2)
        logger.info("í”Œë ˆì´ìŠ¤í™€ë” ë³µì› ì™„ë£Œ.")
        return state
    except Exception as exc:
        state["error"] = f"Placeholder ë³µì› ì˜¤ë¥˜: {exc}"
        return state


def apply_translations_to_json(json_obj: Any, translation_map: Dict[str, str]) -> Any:  # noqa: D401
    if isinstance(json_obj, dict):
        return {
            k: apply_translations_to_json(v, translation_map)
            for k, v in json_obj.items()
        }
    if isinstance(json_obj, list):
        return [apply_translations_to_json(i, translation_map) for i in json_obj]
    if isinstance(json_obj, str):
        return translation_map.get(json_obj, json_obj)
    return json_obj


async def validation_and_retry_node(state: TranslatorState) -> TranslatorState:  # noqa: D401
    try:
        id_map = state["id_to_text_map"]
        translation_map = state["translation_map"]
        retry_count = state.get("retry_count", 0) + 1
        state["retry_count"] = retry_count

        # ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª© ì°¾ê¸° (ì§€ëŠ¥ì ì¸ ì²´í¬)
        to_retry = []
        placeholder_issues = 0

        for tid, orig in id_map.items():
            translated = translation_map.get(tid, "").strip()
            original = orig.strip()

            # ë²ˆì—­ì´ í•„ìš”í•œ ìƒí™© ì²´í¬
            should_retry = False
            retry_reason = ""

            # 1. ë²ˆì—­ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
            if not translated:
                should_retry = True
                retry_reason = "ë²ˆì—­ ëˆ„ë½"
            # 2. ì›ë³¸ì´ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ë° ë²ˆì—­ì´ ë¹„ì–´ìˆê±°ë‚˜ í”Œë ˆì´ìŠ¤í™€ë”ì¸ ê²½ìš°
            elif original:
                if len(translated) == 0:
                    should_retry = True
                    retry_reason = "ë¹ˆ ë²ˆì—­"
                elif not PlaceholderManager.validate_placeholder_preservation(
                    original, translated
                ):
                    should_retry = True
                    retry_reason = "í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½"
                    placeholder_issues += 1

                    missing_placeholders = PlaceholderManager.get_missing_placeholders(
                        original, translated
                    )
                    logger.debug(
                        f"í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½ ê°ì§€: '{original}' -> '{translated}' "
                        f"(ëˆ„ë½ëœ í”Œë ˆì´ìŠ¤í™€ë”: {missing_placeholders})"
                    )
                elif (
                    original.startswith("[P") and original.endswith("]")
                ) or original == "[NEWLINE]":
                    should_retry = False
                # 3. ë²ˆì—­ ê²°ê³¼ê°€ ì›ë³¸ê³¼ ë™ì¼í•œ ê²½ìš° (ì˜ì–´ë¡œ ìœ ì§€í•´ì•¼ í•˜ëŠ” ê²½ìš° ì œì™¸)
                elif translated == original and len(original) > 3:
                    should_retry = True
                    retry_reason = "ë™ì¼í•œ ê²°ê³¼"
                    logger.debug(
                        f"ë²ˆì—­ ëˆ„ë½ ê°ì§€: '{original}' -> '{translated}' (ë™ì¼í•œ ê²°ê³¼)"
                    )
            if should_retry:
                to_retry.append({"id": tid, "original": orig, "reason": retry_reason})

        if not to_retry:
            logger.info("ëª¨ë“  í•­ëª©ì´ ì„±ê³µì ìœ¼ë¡œ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return state

        # ì¬ì‹œë„ ì´ìœ ë³„ í†µê³„ ê³„ì‚°
        retry_reasons = {}
        for item in to_retry:
            reason = item.get("reason", "ì•Œ ìˆ˜ ì—†ìŒ")
            retry_reasons[reason] = retry_reasons.get(reason, 0) + 1

        logger.warning(
            _m(
                "translator.missing_retry",
                missing=len(to_retry),
                attempt=retry_count,
                max_attempts=state["max_retries"],
            )
        )

        # ì¬ì‹œë„ ì´ìœ ë³„ í†µê³„ ì¶œë ¥
        if retry_reasons:
            logger.warning("ì¬ì‹œë„ ì´ìœ ë³„ í†µê³„:")
            for reason, count in retry_reasons.items():
                logger.warning(f"  - {reason}: {count}ê°œ")

        # í”Œë ˆì´ìŠ¤í™€ë” ì´ìŠˆê°€ ìˆëŠ” ê²½ìš° íŠ¹ë³„íˆ ë¡œê·¸
        if placeholder_issues > 0:
            logger.warning(f"ğŸ”§ í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½ ê°ì§€: {placeholder_issues}ê°œ í•­ëª©")

        # ì¬ì‹œë„í•  í•­ëª©ë“¤ì˜ ID ëª©ë¡ ì €ì¥ (ë””ë²„ê¹…ìš©)
        retry_ids = [item["id"] for item in to_retry]
        logger.debug(f"ì¬ì‹œë„ ëŒ€ìƒ IDë“¤: {retry_ids[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸

        # ë””ë²„ê¹…: ì²˜ìŒ 5ê°œ ì¬ì‹œë„ í•­ëª©ì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥
        if len(to_retry) > 100:  # 100ê°œ ì´ìƒì¼ ë•Œë§Œ ë””ë²„ê¹… ì¶œë ¥
            logger.warning("ğŸ” ì¬ì‹œë„ ëŒ€ìƒ ìƒ˜í”Œ ë¶„ì„:")
            for i, item in enumerate(to_retry[:5]):
                original = item["original"].strip()
                translated = translation_map.get(item["id"], "").strip()
                reason = item.get("reason", "ì•Œ ìˆ˜ ì—†ìŒ")
                logger.warning(
                    f"  ìƒ˜í”Œ {i + 1}: '{original}' -> '{translated}' (ì´ìœ : {reason}, ID: {item['id']})"
                )
            logger.warning(f"  ... ì´ {len(to_retry)}ê°œ í•­ëª© ì¤‘ ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ")

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì¬ì‹œë„ ì‹œì‘)
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ”„ ë²ˆì—­ ì¬ì‹œë„ ì¤‘",
                0,
                len(to_retry),
                f"ë¯¸ë²ˆì—­ í•­ëª© {len(to_retry)}ê°œ ì¬ì‹œë„ ({retry_count}ì°¨ ì‹œë„)",
            )

        # LLM í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        llm = state.get("llm_client")
        if llm is None:
            logger.error("LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            state["error"] = "LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            return state
        retry_chunks = TokenOptimizer.create_text_chunks(
            to_retry, state["max_tokens_per_chunk"]
        )

        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])
        sem = asyncio.Semaphore(state["max_concurrent_requests"])

        # ì¬ì‹œë„ íšŸìˆ˜ì— ë”°ë¼ temperature ë™ì  ì¡°ì • (ìµœëŒ€ 1.0ê¹Œì§€)
        retry_temperature = min(1.0, retry_count * 0.1)
        logger.info(f"ë²ˆì—­ ì¬ì‹œë„ temperature: {retry_temperature}")

        tasks = [
            _translate_chunk_worker_with_progress(
                chunk=c,
                state=state,
                llm=llm,
                target_language=state["target_language"],
                delay_manager=delay_mgr,
                semaphore=sem,
                chunk_num=i,
                total_chunks=len(retry_chunks),
                progress_callback=progress_callback,
                is_retry=True,
                temperature=retry_temperature,  # ì¬ì‹œë„ íšŸìˆ˜ì— ë”°ë¼ ë™ì  ì¡°ì •
            )
            for i, c in enumerate(retry_chunks, 1)
        ]
        retry_results = await asyncio.gather(*tasks)

        # ì¬ì‹œë„ ê²°ê³¼ ì—…ë°ì´íŠ¸ (í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ í¬í•¨)
        retry_count_success = 0
        retry_count_failed = 0
        retry_count_placeholder_fixed = 0

        for res in retry_results:
            for item in res:
                original_text = id_map.get(item.id, "").strip()
                new_translation = item.translated.strip()
                old_translation = translation_map.get(item.id, "").strip()

                # ë²ˆì—­ì´ ìœ íš¨í•œì§€ ì²´í¬ (í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ í¬í•¨)
                is_valid_translation = False
                validation_passed = False

                if new_translation:
                    # 1. ê¸°ë³¸ ë²ˆì—­ ìœ íš¨ì„± ì²´í¬
                    if new_translation != old_translation:
                        # ì´ì „ ë²ˆì—­ê³¼ ë‹¤ë¥´ë©´ ê°œì„ ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                        is_valid_translation = True
                    elif new_translation != original_text:
                        # ì›ë³¸ê³¼ ë‹¤ë¥´ë©´ ìœ íš¨
                        is_valid_translation = True
                    elif len(new_translation) >= 1:
                        # ìµœì†Œ 1ê¸€ì ì´ìƒì´ë©´ ìœ íš¨ (ë„ˆë¬´ ì—„ê²©í–ˆë˜ 2ê¸€ì ì¡°ê±´ ì™„í™”)
                        is_valid_translation = True

                    # 2. í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ (ë” ì¤‘ìš”í•œ ê²€ì¦)
                    if is_valid_translation:
                        if PlaceholderManager.validate_placeholder_preservation(
                            original_text, new_translation
                        ):
                            validation_passed = True

                            # í”Œë ˆì´ìŠ¤í™€ë” ì´ìŠˆê°€ í•´ê²°ëœ ê²½ìš° ì¶”ê°€ ì¹´ìš´íŠ¸
                            if not PlaceholderManager.validate_placeholder_preservation(
                                original_text, old_translation
                            ):
                                retry_count_placeholder_fixed += 1
                                logger.debug(
                                    f"í”Œë ˆì´ìŠ¤í™€ë” ë³µì› ì„±ê³µ: {item.id} -> {new_translation[:50]}..."
                                )
                        else:
                            # í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì—¬ì „íˆ ëˆ„ë½ëœ ê²½ìš°
                            missing_placeholders = (
                                PlaceholderManager.get_missing_placeholders(
                                    original_text, new_translation
                                )
                            )
                            logger.debug(
                                f"ì¬ì‹œë„ í›„ì—ë„ í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½: {item.id} -> '{new_translation}' "
                                f"(ëˆ„ë½: {missing_placeholders})"
                            )
                            validation_passed = False

                if validation_passed:
                    translation_map[item.id] = new_translation
                    retry_count_success += 1
                    logger.debug(f"ì¬ì‹œë„ ì„±ê³µ: {item.id} -> {new_translation[:50]}...")
                else:
                    retry_count_failed += 1
                    logger.debug(
                        f"ì¬ì‹œë„ ì‹¤íŒ¨: {item.id} -> '{new_translation}' (ì›ë³¸: '{original_text}')"
                    )

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì¬ì‹œë„ ì™„ë£Œ)
        if progress_callback:
            status_msg = f"ì¬ì‹œë„ ê²°ê³¼: ì„±ê³µ {retry_count_success}ê°œ, ì‹¤íŒ¨ {retry_count_failed}ê°œ"
            if retry_count_placeholder_fixed > 0:
                status_msg += f", í”Œë ˆì´ìŠ¤í™€ë” ë³µì› {retry_count_placeholder_fixed}ê°œ"
            progress_callback(
                "ğŸ”„ ë²ˆì—­ ì¬ì‹œë„ ì™„ë£Œ",
                len(retry_chunks),
                len(retry_chunks),
                status_msg,
            )

        log_msg = (
            f"ì¬ì‹œë„ ì™„ë£Œ: ì„±ê³µ {retry_count_success}ê°œ, ì‹¤íŒ¨ {retry_count_failed}ê°œ"
        )
        if retry_count_placeholder_fixed > 0:
            log_msg += f", í”Œë ˆì´ìŠ¤í™€ë” ë³µì› {retry_count_placeholder_fixed}ê°œ"
        logger.info(log_msg)

        # ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨í•œ í•­ëª©ë“¤ì´ ìˆìœ¼ë©´ ë¡œê·¸ ì¶œë ¥
        if retry_count_failed > 0:
            logger.warning(
                f"ì¬ì‹œë„ í›„ì—ë„ {retry_count_failed}ê°œ í•­ëª©ì´ ë²ˆì—­ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )

        return state

    except Exception as exc:
        logger.error(f"ì¬ì‹œë„ ì¤‘ ì˜¤ë¥˜: {exc}")
        state["error"] = f"ì¬ì‹œë„ ì¤‘ ì˜¤ë¥˜: {exc}"
        return state


async def _translate_single_item_worker(
    *,
    tid: str,
    state: TranslatorState,
    llm: Any,
    target_language: str,
    delay_manager: RequestDelayManager,
    semaphore: asyncio.Semaphore,
    max_retries: int = 2,
) -> tuple[str, Optional[str]]:
    """ê°œë³„ í•­ëª© ë²ˆì—­ì„ ìœ„í•œ ë¹„ë™ê¸° ì›Œì»¤ (ì¬ì‹œë„ ê¸°ëŠ¥ í¬í•¨)"""
    async with semaphore:
        id_map = state["id_to_text_map"]
        original_text = id_map[tid]

        # ì´ í•­ëª©ì— í•„ìš”í•œ í”Œë ˆì´ìŠ¤í™€ë” ëª©ë¡ ì¶”ì¶œ
        required_placeholders = PlaceholderManager._extract_internal_placeholders(
            original_text
        )
        placeholders_str = (
            "\n".join(f"- `{p}`" for p in required_placeholders)
            if required_placeholders
            else "ì—†ìŒ"
        )

        # ì¬ì‹œë„ ë£¨í”„
        last_error = None
        for attempt in range(max_retries + 1):
            await delay_manager.wait()

            # ìƒˆ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt = final_fallback_prompt(
                language=target_language,
                text_id=tid,
                original_text=original_text,
                placeholders=placeholders_str,
            )

            try:
                # ì¬ì‹œë„ ì‹œ temperatureë¥¼ ì•½ê°„ ë†’ì—¬ ë‹¤ë¥¸ ê²°ê³¼ ìœ ë„
                temperature = min(1.0, attempt * 0.2)
                configured_llm = llm.with_config(
                    configurable={"temperature": temperature}
                )
                llm_with_tools = configured_llm.bind_tools([TranslatedItem])

                response = await llm_with_tools.ainvoke(prompt)

                if response.tool_calls:
                    for tool_call in response.tool_calls:
                        if (
                            tool_call["name"] == "TranslatedItem"
                            and tool_call["args"].get("id") == tid
                        ):
                            item = TranslatedItem(**tool_call["args"])
                            # ìµœì¢… ê²€ì¦: í”Œë ˆì´ìŠ¤í™€ë” ë³´ì¡´ ì—¬ë¶€
                            if PlaceholderManager.validate_placeholder_preservation(
                                original_text, item.translated
                            ):
                                logger.info(
                                    f"âœ… ìµœì¢… ë²ˆì—­ ì¬ì‹œë„ ì„±ê³µ (ì‹œë„ {attempt + 1}): {tid} -> {item.translated[:50]}..."
                                )
                                return tid, item.translated
                            else:
                                last_error = "í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½"
                                logger.warning(
                                    f"âš ï¸ ìµœì¢… ë²ˆì—­ ì¬ì‹œë„({attempt + 1}) í›„ì—ë„ í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½: {tid} -> '{item.translated}'"
                                )
                else:
                    last_error = "ì‘ë‹µ ì—†ìŒ"
                    # logger.warning(
                    #     f"âš ï¸ ìµœì¢… ë²ˆì—­ ì¬ì‹œë„({attempt + 1}) ì‹¤íŒ¨ (ì‘ë‹µ ì—†ìŒ): {tid}"
                    # )

            except Exception as e:
                last_error = str(e)
                logger.error(
                    f"ğŸš¨ ìµœì¢… ë²ˆì—­ ì¬ì‹œë„({attempt + 1}) API í˜¸ì¶œ ì˜¤ë¥˜ (í•­ëª©: {tid}): {e}"
                )

            # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
            if attempt < max_retries:
                await asyncio.sleep(min(2.0, (attempt + 1) * 0.5))

        logger.error(
            f"âŒ ìµœì¢… ë²ˆì—­ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ({max_retries + 1}íšŒ): {tid}, ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}"
        )
        return tid, None


async def final_fallback_translation_node(state: TranslatorState) -> TranslatorState:
    """ìµœì¢…ì ìœ¼ë¡œ ëˆ„ë½ëœ í•­ëª©ë“¤ì„ í•˜ë‚˜ì”© ë‹¤ì‹œ ë²ˆì—­ ìš”ì²­í•©ë‹ˆë‹¤."""
    try:
        logger.info("ìµœì¢… ë²ˆì—­ ë‹¨ê³„ ì‹œì‘: ëˆ„ë½ëœ í•­ëª©ì„ ê°œë³„ì ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.")

        id_map = state["id_to_text_map"]
        translation_map = state["translation_map"]
        llm = state.get("llm_client")
        if llm is None:
            state["error"] = "LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return state

        # ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª© ì°¾ê¸° (should_retryì™€ ë™ì¼í•œ ë¡œì§)
        untranslated_items = []
        for tid, original_text in id_map.items():
            translated = translation_map.get(tid, "").strip()
            original = original_text.strip()
            needs_translation = False

            if not translated:
                needs_translation = True
            elif original:
                if len(translated) == 0:
                    needs_translation = True
                elif not PlaceholderManager.validate_placeholder_preservation(
                    original, translated
                ):
                    needs_translation = True
                elif (
                    original.startswith("[P") and original.endswith("]")
                ) or original == "[NEWLINE]":
                    needs_translation = False
                elif translated.strip() == original.strip() and len(original) > 3:
                    needs_translation = True

            if needs_translation:
                untranslated_items.append(tid)

        if not untranslated_items:
            logger.info("ëˆ„ë½ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ìµœì¢… ë²ˆì—­ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return state

        logger.info(
            f"{len(untranslated_items)}ê°œì˜ ëˆ„ë½ëœ í•­ëª©ì— ëŒ€í•œ ê°œë³„ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤."
        )

        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ“ ìµœì¢… ë²ˆì—­ ì¤‘",
                0,
                len(untranslated_items),
                f"ëˆ„ë½ëœ {len(untranslated_items)}ê°œ í•­ëª© ê°œë³„ ë²ˆì—­ ì‹œì‘",
            )

        target_language = state["target_language"]
        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])
        sem = asyncio.Semaphore(state["max_concurrent_requests"])

        final_fallback_max_retries = state.get("final_fallback_max_retries", 4)
        logger.info(f"ê°œë³„ í•­ëª©ë‹¹ ìµœëŒ€ {final_fallback_max_retries}íšŒ ì¬ì‹œë„í•©ë‹ˆë‹¤.")

        # ê°œë³„ ìš”ì²­ì´ì§€ë§Œ, ë™ì‹œì— ì—¬ëŸ¬ ê°œë¥¼ ë³´ë‚´ì„œ ì†ë„ í–¥ìƒ
        tasks = []
        for tid in untranslated_items:
            tasks.append(
                _translate_single_item_worker(
                    tid=tid,
                    state=state,
                    llm=llm,
                    target_language=target_language,
                    delay_manager=delay_mgr,
                    semaphore=sem,
                    max_retries=final_fallback_max_retries,
                )
            )

        results = await asyncio.gather(*tasks)

        count_success = 0
        for i, (tid, new_translation) in enumerate(results):
            if new_translation:
                translation_map[tid] = new_translation
                count_success += 1

            if progress_callback:
                progress_callback(
                    "ğŸ“ ìµœì¢… ë²ˆì—­ ì¤‘",
                    i + 1,
                    len(untranslated_items),
                    f"{i + 1}/{len(untranslated_items)} í•­ëª© ì²˜ë¦¬ë¨",
                )

        logger.info(
            f"ìµœì¢… ë²ˆì—­ ì™„ë£Œ. {count_success}/{len(untranslated_items)}ê°œ í•­ëª© ë²ˆì—­ ì„±ê³µ."
        )

        return state

    except Exception as exc:
        state["error"] = f"ìµœì¢… ê°œë³„ ë²ˆì—­ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {exc}"
        logger.error(f"ìµœì¢… ê°œë³„ ë²ˆì—­ ë‹¨ê³„ ì˜¤ë¥˜: {traceback.format_exc()}")
        return state


def rebuild_json_node(state: TranslatorState) -> TranslatorState:  # noqa: D401
    try:
        logger.info("ê²°ê³¼ JSON ì¬êµ¬ì„± ì‹œì‘...")
        id_map = state["translation_map"]

        def replace(obj: Any) -> Any:
            if isinstance(obj, dict):
                return {k: replace(v) for k, v in obj.items()}
            if isinstance(obj, list):
                return [replace(i) for i in obj]
            if isinstance(obj, str) and obj in id_map:
                return id_map[obj]
            return obj

        state["translated_json"] = replace(state["processed_json"])
        logger.info("ê²°ê³¼ JSON ì¬êµ¬ì„± ì™„ë£Œ.")
        return state
    except Exception as exc:
        state["error"] = f"JSON ì¬êµ¬ì„± ì˜¤ë¥˜: {exc}"
        return state


def should_retry(state: TranslatorState) -> str:  # noqa: D401
    if state.get("error"):
        logger.error(_m("translator.error_abort", error=state["error"]))
        return "end"

    current_retry = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 3)

    id_map = state.get("id_to_text_map", {})
    translation_map = state.get("translation_map", {})

    if not id_map:
        return "complete"

    # ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª© ì²´í¬ (ì§€ëŠ¥ì ìœ¼ë¡œ, í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ í¬í•¨)
    untranslated_items = []
    placeholder_failed_items = []

    for tid, original_text in id_map.items():
        translated = translation_map.get(tid, "").strip()
        original = original_text.strip()

        # ë²ˆì—­ì´ í•„ìš”í•œ ìƒí™© ì²´í¬
        needs_translation = False

        # 1. ë²ˆì—­ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
        if not translated:
            needs_translation = True
        # 2. ì›ë³¸ì´ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ë° ì œëŒ€ë¡œ ë²ˆì—­ë˜ì§€ ì•Šì€ ê²½ìš°
        elif original:
            if len(translated) == 0:
                needs_translation = True
            elif not PlaceholderManager.validate_placeholder_preservation(
                original, translated
            ):
                needs_translation = True
                placeholder_failed_items.append(tid)
            elif (
                original.startswith("[P") and original.endswith("]")
            ) or original == "[NEWLINE]":
                needs_translation = False
            elif translated.strip() == original.strip() and len(original) > 3:
                needs_translation = True
        if needs_translation:
            untranslated_items.append(tid)

    needs_retry = len(untranslated_items) > 0

    if needs_retry and current_retry < max_retries:
        logger.info(
            _m(
                "translator.untranslated_retry",
                attempt=current_retry + 1,
                max_attempts=max_retries,
            )
        )
        logger.debug(f"ì¬ì‹œë„ í•„ìš”í•œ í•­ëª© ìˆ˜: {len(untranslated_items)}")
        if placeholder_failed_items:
            logger.debug(f"í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½ í•­ëª© ìˆ˜: {len(placeholder_failed_items)}")
        return "retry"
    elif needs_retry:
        logger.warning(_m("translator.max_retry_reached", max_attempts=max_retries))
        logger.warning(f"ìµœì¢…ì ìœ¼ë¡œ ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª©: {len(untranslated_items)}ê°œ")
        if placeholder_failed_items:
            logger.warning(f"í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½ í•­ëª©: {len(placeholder_failed_items)}ê°œ")
        # ë²ˆì—­ë˜ì§€ ì•Šì€ í•­ëª©ì´ ìˆì–´ë„ ì™„ë£Œë¡œ ì²˜ë¦¬ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        logger.info("ê°œë³„ í•­ëª© ì¬ë²ˆì—­ì„ ìœ„í•œ ìµœì¢… ë‹¨ê³„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
        return "final_fallback"
    else:
        logger.info(_m("translator.translation_ok"))
        return "complete"


async def load_vanilla_glossary_node(state: TranslatorState) -> TranslatorState:
    """ë°”ë‹ë¼ ë§ˆì¸í¬ë˜í”„íŠ¸ glossaryë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not state.get("use_vanilla_glossary", False):
        state["vanilla_glossary"] = []
        return state

    vanilla_path = state.get("vanilla_glossary_path", "vanilla_glossary.json")

    # í•œêµ­ì–´ì¸ ê²½ìš° ë¯¸ë¦¬ ì¤€ë¹„ëœ ì‚¬ì „ ê²½ë¡œ ì‚¬ìš©
    target_language = state.get("target_language", "")
    if target_language == "í•œêµ­ì–´" and vanilla_path == "vanilla_glossary.json":
        # ê¸°ë³¸ ê²½ë¡œì¸ ê²½ìš°ë§Œ ë¯¸ë¦¬ ì¤€ë¹„ëœ ì‚¬ì „ìœ¼ë¡œ ë³€ê²½
        preset_path = "src/assets/vanilla_glossary/ko_kr.json"
        if Path(preset_path).exists():
            vanilla_path = preset_path
            logger.info("í•œêµ­ì–´ íƒ€ê²Ÿ ì–¸ì–´ ê°ì§€, ë¯¸ë¦¬ ì¤€ë¹„ëœ ë°”ë‹ë¼ ì‚¬ì „ ì‚¬ìš©")

    try:
        # ë°”ë‹ë¼ glossary builderë¥¼ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸ (ìˆœí™˜ ì„í¬íŠ¸ ë°©ì§€)
        from .vanilla_glossary_builder import VanillaGlossaryBuilder

        builder = VanillaGlossaryBuilder()

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ® ë°”ë‹ë¼ ì‚¬ì „ ë¡œë“œ ì¤‘",
                0,
                1,
                "ë°”ë‹ë¼ ë§ˆì¸í¬ë˜í”„íŠ¸ ì‚¬ì „ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            )

        vanilla_glossary = await builder.create_or_load_vanilla_glossary(
            glossary_path=vanilla_path,
            force_rebuild=False,  # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±
            max_entries_per_batch=200,
            max_concurrent_requests=3,
            progress_callback=progress_callback,
        )

        state["vanilla_glossary"] = vanilla_glossary

        if progress_callback:
            progress_callback(
                "ğŸ® ë°”ë‹ë¼ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ",
                1,
                1,
                f"ë°”ë‹ë¼ ì‚¬ì „ {len(vanilla_glossary)}ê°œ ìš©ì–´ ë¡œë“œ ì™„ë£Œ",
            )

        logger.info(f"ë°”ë‹ë¼ glossary ë¡œë“œ ì™„ë£Œ: {len(vanilla_glossary)}ê°œ ìš©ì–´")

    except Exception as exc:
        logger.warning(f"ë°”ë‹ë¼ glossary ë¡œë“œ ì‹¤íŒ¨: {exc}")
        state["vanilla_glossary"] = []

    return state


def load_glossary_node(state: TranslatorState) -> TranslatorState:
    """Load existing glossary from the specified file path."""
    path = state.get("glossary_path")
    if path and os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                state["important_terms"] = [GlossaryEntry(**item) for item in data]
                logger.info(
                    _m(
                        "translator.glossary_loaded",
                        count=len(state["important_terms"]),
                        path=path,
                    )
                )
        except (IOError, json.JSONDecodeError, TypeError) as exc:
            logger.warning(_m("translator.glossary_load_error", path=path, error=exc))
            state["important_terms"] = []
    else:
        state["important_terms"] = []
    return state


def save_glossary_node(state: TranslatorState) -> TranslatorState:
    """Save the final glossary to the specified file path."""
    path = state.get("glossary_path")
    glossary = state.get("important_terms")
    if path and glossary:
        try:
            logger.info("ìš©ì–´ì§‘ ì €ì¥ ì‹œì‘...")
            with open(path, "w", encoding="utf-8") as f:
                json.dump(
                    [term.dict() for term in glossary], f, ensure_ascii=False, indent=2
                )
            logger.info(_m("translator.glossary_saved", count=len(glossary), path=path))
        except IOError as exc:
            logger.error(_m("translator.glossary_save_error", path=path, error=exc))
    return state


def create_primary_glossary_node(state: TranslatorState) -> TranslatorState:
    """ê¸°ì¡´ ë²ˆì—­ ë°ì´í„°ë¡œ 1ì°¨ ì‚¬ì „ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    existing_translations = state.get("existing_translations")
    if not existing_translations:
        logger.info("ê¸°ì¡´ ë²ˆì—­ ë°ì´í„°ê°€ ì—†ì–´ 1ì°¨ ì‚¬ì „ êµ¬ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        state["primary_glossary"] = []
        return state

    logger.info(f"ê¸°ì¡´ ë²ˆì—­ ë°ì´í„° {len(existing_translations)}ê°œë¡œ 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì‹œì‘")

    # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
    progress_callback = state.get("progress_callback")
    if progress_callback:
        progress_callback(
            "ğŸ“– 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì¤‘",
            0,
            len(existing_translations),
            f"ê¸°ì¡´ ë²ˆì—­ ë°ì´í„° {len(existing_translations)}ê°œ ë¶„ì„ ì¤‘",
        )

    primary_terms = []
    processed_count = 0

    # ê¸°ì¡´ ë²ˆì—­ ë°ì´í„°ë¥¼ GlossaryEntryë¡œ ë³€í™˜
    for source_text, target_text in existing_translations.items():
        try:
            # ê°„ë‹¨í•œ ìš©ì–´ ì¶”ì¶œ (ë‹¨ì–´ ë‹¨ìœ„)
            words = source_text.split()
            if len(words) <= 3:  # 3ë‹¨ì–´ ì´í•˜ì˜ ì§§ì€ í‘œí˜„ë§Œ ìš©ì–´ë¡œ ê°„ì£¼
                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìš©ì–´ì¸ì§€ í™•ì¸
                existing_term = None
                for term in primary_terms:
                    if term.original.lower() == source_text.lower():
                        existing_term = term
                        break

                if existing_term:
                    # ê¸°ì¡´ ìš©ì–´ì— ìƒˆë¡œìš´ ì˜ë¯¸ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                    new_meaning = TermMeaning(
                        translation=target_text, context="ê¸°ì¡´ ë²ˆì—­"
                    )

                    # ì¤‘ë³µ ì²´í¬ (ë²ˆì—­ë§Œ ë¹„êµ)
                    translation_exists = any(
                        m.translation.lower().strip() == target_text.lower().strip()
                        for m in existing_term.meanings
                    )

                    if not translation_exists:
                        existing_term.meanings.append(new_meaning)
                else:
                    # ìƒˆë¡œìš´ ìš©ì–´ ì¶”ê°€
                    new_term = GlossaryEntry(
                        original=source_text,
                        meanings=[
                            TermMeaning(translation=target_text, context="ê¸°ì¡´ ë²ˆì—­")
                        ],
                    )
                    primary_terms.append(new_term)

            processed_count += 1

            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (500ê°œë§ˆë‹¤)
            if processed_count % 500 == 0 and progress_callback:
                progress_callback(
                    "ğŸ“– 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì¤‘",
                    processed_count,
                    len(existing_translations),
                    f"{processed_count}/{len(existing_translations)} í•­ëª© ì²˜ë¦¬ë¨",
                )

        except Exception as e:
            logger.debug(f"ìš©ì–´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({source_text}): {e}")

    # ìµœì¢…ì ìœ¼ë¡œ ëª¨ë“  ìš©ì–´ì—ì„œ ì¤‘ë³µ ì˜ë¯¸ ì œê±°
    for term in primary_terms:
        term.meanings = TokenOptimizer.deduplicate_glossary_meanings(term.meanings)

    state["primary_glossary"] = primary_terms

    # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì™„ë£Œ)
    if progress_callback:
        progress_callback(
            "ğŸ“– 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ",
            len(existing_translations),
            len(existing_translations),
            f"ì´ {len(primary_terms)}ê°œ ìš©ì–´ê°€ í¬í•¨ëœ 1ì°¨ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ",
        )

    logger.info(f"1ì°¨ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ: {len(primary_terms)}ê°œ ìš©ì–´ ìƒì„±")
    return state


async def quality_review_node(state: TranslatorState) -> TranslatorState:
    """ë²ˆì—­ í’ˆì§ˆì„ ê²€í† í•˜ëŠ” ë…¸ë“œ"""
    try:
        # í’ˆì§ˆ ê²€í† ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if not state.get("enable_quality_review", True):
            logger.info("í’ˆì§ˆ ê²€í† ê°€ ë¹„í™œì„±í™”ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            return state

        logger.info("ë²ˆì—­ í’ˆì§ˆ ê²€í†  ì‹œì‘...")

        id_map = state["id_to_text_map"]
        translation_map = state["translation_map"]
        llm = state.get("llm_client")

        if not llm or not id_map or not translation_map:
            logger.info("í’ˆì§ˆ ê²€í† ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤ (í•„ìˆ˜ ë°ì´í„° ì—†ìŒ)")
            return state

        # ê²€í† í•  í•­ëª©ë“¤ ì¤€ë¹„ (ë²ˆì—­ëœ ê²ƒë§Œ)
        placeholder_only_pattern = r"^\[(P\d{3,}|NEWLINE)\]$"
        review_items = []
        for tid, original_text in id_map.items():
            translated_text = translation_map.get(tid, "")
            if not translated_text.strip():
                continue

            # ì›ë¬¸ê³¼ ë²ˆì—­ì´ ëª¨ë‘ ë‚´ë¶€ í”Œë ˆì´ìŠ¤í™€ë”ë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê²½ìš° í’ˆì§ˆ ê²€í†  ê±´ë„ˆëœ€
            if re.match(placeholder_only_pattern, original_text.strip()) and re.match(
                placeholder_only_pattern, translated_text.strip()
            ):
                continue

            review_items.append(
                {
                    "id": tid,
                    "original": original_text,
                    "translated": translated_text,
                }
            )

        if not review_items:
            logger.info("ê²€í† í•  ë²ˆì—­ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return state

        logger.info(f"í’ˆì§ˆ ê²€í†  ëŒ€ìƒ: {len(review_items)}ê°œ í•­ëª©")

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ” ë²ˆì—­ í’ˆì§ˆ ê²€í†  ì¤‘",
                0,
                len(review_items),
                f"ì´ {len(review_items)}ê°œ í•­ëª© í’ˆì§ˆ ê²€í†  ì‹œì‘",
            )

        # 2000ê¸€ì ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        chunks = _create_quality_review_chunks(review_items, max_chars=4000)
        logger.info(f"í’ˆì§ˆ ê²€í† ë¥¼ ìœ„í•´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")

        # ë™ì‹œ ìš”ì²­ ì œí•œ
        sem = asyncio.Semaphore(state["max_concurrent_requests"])
        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])

        # ê° ì²­í¬ë³„ë¡œ í’ˆì§ˆ ê²€í†  ì‹¤í–‰
        tasks = []
        for chunk_idx, chunk in enumerate(chunks):
            tasks.append(
                _review_chunk_worker(
                    chunk=chunk,
                    target_language=state["target_language"],
                    llm=llm,
                    semaphore=sem,
                    delay_manager=delay_mgr,
                    chunk_idx=chunk_idx,
                    total_chunks=len(chunks),
                    progress_callback=progress_callback,
                )
            )

        review_results = await asyncio.gather(*tasks)

        # ê²°ê³¼ ì§‘ê³„ - ê° ì²­í¬ì—ì„œ ê°œë³„ QualityIssueë“¤ì„ ìˆ˜ì§‘
        all_issues = []

        for review_result in review_results:
            if review_result:  # review_resultëŠ” List[QualityIssue]
                all_issues.extend(review_result)

        # í’ˆì§ˆ ê²€í†  ê²°ê³¼ ë¡œê¹…
        if all_issues:
            logger.warning(f"ğŸ” í’ˆì§ˆ ê²€í†  ê²°ê³¼: {len(all_issues)}ê°œ ë¬¸ì œ ë°œê²¬")

            # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
            severity_counts = {}
            issue_type_counts = {}

            for issue in all_issues:
                severity_counts[issue.severity] = (
                    severity_counts.get(issue.severity, 0) + 1
                )
                issue_type_counts[issue.issue_type] = (
                    issue_type_counts.get(issue.issue_type, 0) + 1
                )

            logger.warning("ì‹¬ê°ë„ë³„ ë¶„ë¥˜:")
            for severity, count in severity_counts.items():
                logger.warning(f"  - {severity}: {count}ê°œ")

            logger.warning("ë¬¸ì œ ìœ í˜•ë³„ ë¶„ë¥˜:")
            for issue_type, count in issue_type_counts.items():
                logger.warning(f"  - {issue_type}: {count}ê°œ")

            # ì‹¬ê°í•œ ë¬¸ì œë“¤ ìƒì„¸ ë¡œê¹…
            high_severity_issues = [
                issue for issue in all_issues if issue.severity == "high"
            ]
            if high_severity_issues:
                logger.warning(f"ğŸš¨ ì‹¬ê°í•œ ë¬¸ì œ {len(high_severity_issues)}ê°œ:")
                for issue in high_severity_issues[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    logger.warning(
                        f"  - [{issue.text_id}] {issue.issue_type}: {issue.description}"
                    )
                    if issue.suggested_fix:
                        logger.warning(f"    ì œì•ˆ: {issue.suggested_fix}")
        else:
            logger.info("âœ… í’ˆì§ˆ ê²€í†  ê²°ê³¼: ì‹¬ê°í•œ ë¬¸ì œ ì—†ìŒ")

        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°ì€ ê°œë³„ QualityIssue ë°©ì‹ì—ì„œëŠ” ìƒëµ
        # ëŒ€ì‹  ì‹¬ê°ë„ë³„ í†µê³„ë¥¼ í†µí•´ ì „ì²´ í’ˆì§ˆ ìƒíƒœë¥¼ íŒŒì•…

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì™„ë£Œ)
        if progress_callback:
            summary = f"í’ˆì§ˆ ê²€í†  ì™„ë£Œ - ë¬¸ì œ {len(all_issues)}ê°œ ë°œê²¬"
            progress_callback(
                "ğŸ” ë²ˆì—­ í’ˆì§ˆ ê²€í†  ì™„ë£Œ", len(chunks), len(chunks), summary
            )

        # ìƒíƒœì— ê²€í†  ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)
        state["quality_issues"] = all_issues

        return state

    except Exception as exc:
        logger.error(f"í’ˆì§ˆ ê²€í†  ì¤‘ ì˜¤ë¥˜: {exc}")
        logger.error(traceback.format_exc())
        # í’ˆì§ˆ ê²€í†  ì‹¤íŒ¨ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
        return state


def _create_quality_review_chunks(
    review_items: List[Dict], max_chars: int = 2000
) -> List[List[Dict]]:
    """í’ˆì§ˆ ê²€í† ë¥¼ ìœ„í•´ í•­ëª©ë“¤ì„ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
    chunks = []
    current_chunk = []
    current_chars = 0

    for item in review_items:
        # í•­ëª©ì˜ ì˜ˆìƒ ë¬¸ì ìˆ˜ ê³„ì‚° (ID + ì›ë³¸ + ë²ˆì—­ + í¬ë§·íŒ…)
        item_chars = (
            len(item["id"]) + len(item["original"]) + len(item["translated"]) + 50
        )

        # ë‹¨ì¼ í•­ëª©ì´ max_charsë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë³„ë„ ì²­í¬ë¡œ ì²˜ë¦¬
        if item_chars > max_chars:
            if current_chunk:
                chunks.append(current_chunk)
                current_chunk = []
                current_chars = 0
            chunks.append([item])
        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í–ˆì„ ë•Œ ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
        elif current_chars + item_chars > max_chars:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = [item]
            current_chars = item_chars
        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
        else:
            current_chunk.append(item)
            current_chars += item_chars

    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append(current_chunk)

    return chunks


async def _review_chunk_worker(
    *,
    chunk: List[Dict],
    target_language: str,
    llm: Any,
    semaphore: asyncio.Semaphore,
    delay_manager: RequestDelayManager,
    chunk_idx: int,
    total_chunks: int,
    progress_callback: Optional[callable] = None,
) -> List[QualityIssue]:
    """ì²­í¬ë³„ í’ˆì§ˆ ê²€í† ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì›Œì»¤ - ê°œë³„ QualityIssueë“¤ì„ ë°˜í™˜"""
    async with semaphore:
        await delay_manager.wait()

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        if progress_callback:
            progress_callback(
                "ğŸ” ë²ˆì—­ í’ˆì§ˆ ê²€í†  ì¤‘",
                chunk_idx,
                total_chunks,
                f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ê²€í†  ì¤‘ ({len(chunk)}ê°œ í•­ëª©)",
            )

        try:
            # ê²€í† ìš© í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            review_text = _format_chunk_for_quality_review(chunk)

            # í’ˆì§ˆ ê²€í†  í”„ë¡¬í”„íŠ¸ ìƒì„±
            from src.prompts.llm_prompts import quality_review_prompt

            prompt = quality_review_prompt(target_language, review_text)

            # LLM í˜¸ì¶œ - QualityIssue ë„êµ¬ ë°”ì¸ë”©
            llm_with_tools = llm.bind_tools([QualityIssue])
            response = await llm_with_tools.ainvoke(prompt)

            # ì‘ë‹µ íŒŒì‹± - ê°œë³„ QualityIssueë“¤ ìˆ˜ì§‘
            quality_issues = []
            if response.tool_calls:
                for tool_call in response.tool_calls:
                    if tool_call["name"] == "QualityIssue":
                        try:
                            issue = QualityIssue(**tool_call["args"])
                            quality_issues.append(issue)
                            logger.debug(
                                f"í’ˆì§ˆ ë¬¸ì œ ë°œê²¬: [{issue.text_id}] {issue.issue_type} ({issue.severity})"
                            )
                        except Exception as e:
                            logger.warning(
                                f"QualityIssue íŒŒì‹± ì˜¤ë¥˜: {e}, args: {tool_call['args']}"
                            )

            logger.debug(
                f"ì²­í¬ {chunk_idx + 1} í’ˆì§ˆ ê²€í†  ì™„ë£Œ: {len(quality_issues)}ê°œ ë¬¸ì œ ë°œê²¬"
            )
            return quality_issues

        except Exception as exc:
            logger.error(f"ì²­í¬ {chunk_idx + 1} í’ˆì§ˆ ê²€í†  ì‹¤íŒ¨: {exc}")
            return []


def _format_chunk_for_quality_review(chunk: List[Dict]) -> str:
    """í’ˆì§ˆ ê²€í† ìš© ì²­í¬ í¬ë§·íŒ…"""
    lines = []

    for item in chunk:
        text_id = item["id"]
        original = item["original"]
        translated = item["translated"]

        lines.append(f"[{text_id}]")
        lines.append(f"ì›ë³¸: {original}")
        lines.append(f"ë²ˆì—­: {translated}")
        lines.append("")  # ë¹ˆ ì¤„ ì¶”ê°€

    return "\n".join(lines)


def should_retranslate_based_on_quality(state: TranslatorState) -> str:
    """í’ˆì§ˆ ê²€í†  ê²°ê³¼ì— ë”°ë¼ ì¬ë²ˆì—­ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if state.get("error"):
        return "end"

    quality_issues = state.get("quality_issues", [])

    # ëª¨ë“  í’ˆì§ˆ ë¬¸ì œë¥¼ ì¬ë²ˆì—­ ëŒ€ìƒìœ¼ë¡œ ê³ ë ¤
    if quality_issues:
        quality_retry_count = state.get("quality_retry_count", 0)
        max_quality_retries = state.get("max_quality_retries", 2)

        if quality_retry_count < max_quality_retries:
            logger.info(
                f"í’ˆì§ˆ ê²€í†  ê²°ê³¼ {len(quality_issues)}ê°œ ë¬¸ì œ ë°œê²¬, ì¬ë²ˆì—­ ì§„í–‰ ({quality_retry_count + 1}/{max_quality_retries})"
            )
            return "quality_retranslate"
        else:
            logger.warning(
                f"í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ìµœëŒ€ íšŸìˆ˜ ({max_quality_retries}) ë„ë‹¬, ì™„ë£Œë¡œ ì§„í–‰"
            )
            return "complete"
    else:
        logger.info("í’ˆì§ˆ ê²€í†  ê²°ê³¼ ë¬¸ì œ ì—†ìŒ, ì™„ë£Œë¡œ ì§„í–‰")
        return "complete"


async def quality_based_retranslation_node(state: TranslatorState) -> TranslatorState:
    """í’ˆì§ˆ ê²€í†  ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œê°€ ìˆëŠ” í•­ëª©ë“¤ì„ ë‹¤ì‹œ ë²ˆì—­í•©ë‹ˆë‹¤."""
    try:
        quality_issues = state.get("quality_issues", [])
        id_map = state["id_to_text_map"]
        translation_map = state["translation_map"]
        llm = state.get("llm_client")

        # ì¬ë²ˆì—­ ì¹´ìš´í„° ì¦ê°€
        quality_retry_count = state.get("quality_retry_count", 0) + 1
        state["quality_retry_count"] = quality_retry_count

        logger.info(f"í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì‹œì‘ ({quality_retry_count}ì°¨ ì‹œë„)")

        # ëª¨ë“  í’ˆì§ˆ ë¬¸ì œê°€ ìˆëŠ” í•­ëª©ë“¤ ì¶”ì¶œ
        all_issues = quality_issues

        # ì¬ë²ˆì—­í•  í•­ëª©ë“¤ ì¤€ë¹„
        items_to_retranslate = []
        for issue in all_issues:
            text_id = issue.text_id
            if text_id in id_map:
                original_text = id_map[text_id]
                current_translation = translation_map.get(text_id, "")

                items_to_retranslate.append(
                    {
                        "id": text_id,
                        "original": original_text,
                        "current_translation": current_translation,
                        "issue": issue,
                    }
                )

        # ì¤‘ë³µ ì œê±° (ê°™ì€ IDê°€ ì—¬ëŸ¬ ë¬¸ì œë¡œ ì¤‘ë³µë  ìˆ˜ ìˆìŒ)
        unique_items = {}
        for item in items_to_retranslate:
            text_id = item["id"]
            if text_id not in unique_items:
                unique_items[text_id] = item
            else:
                # ê¸°ì¡´ í•­ëª©ì— ì¶”ê°€ ì´ìŠˆ ì •ë³´ ë³‘í•©
                if "issues" not in unique_items[text_id]:
                    unique_items[text_id]["issues"] = [unique_items[text_id]["issue"]]
                    del unique_items[text_id]["issue"]
                unique_items[text_id]["issues"].append(item["issue"])

        items_to_retranslate = list(unique_items.values())

        if not items_to_retranslate:
            logger.info("ì¬ë²ˆì—­í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return state

        logger.info(
            f"í’ˆì§ˆ ë¬¸ì œë¡œ {len(items_to_retranslate)}ê°œ í•­ëª© ì¬ë²ˆì—­ ì§„í–‰ (ì „ì²´ ë¬¸ì œ {len(all_issues)}ê°œ)"
        )

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        progress_callback = state.get("progress_callback")
        if progress_callback:
            progress_callback(
                "ğŸ”„ í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì¤‘",
                0,
                len(items_to_retranslate),
                f"í’ˆì§ˆ ë¬¸ì œ {len(items_to_retranslate)}ê°œ í•­ëª© ì¬ë²ˆì—­ ì‹œì‘",
            )

        # ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        chunks = TokenOptimizer.create_text_chunks(
            items_to_retranslate, state["max_tokens_per_chunk"]
        )

        # ë™ì‹œ ìš”ì²­ ì œí•œ
        sem = asyncio.Semaphore(state["max_concurrent_requests"])
        delay_mgr = RequestDelayManager(state["delay_between_requests_ms"])

        # ì¬ë²ˆì—­ ì‹¤í–‰
        tasks = []
        for chunk_idx, chunk in enumerate(chunks):
            tasks.append(
                _quality_retranslate_chunk_worker(
                    chunk=chunk,
                    state=state,
                    llm=llm,
                    target_language=state["target_language"],
                    delay_manager=delay_mgr,
                    semaphore=sem,
                    chunk_idx=chunk_idx,
                    total_chunks=len(chunks),
                    progress_callback=progress_callback,
                    max_retries=3,  # í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ì—ì„œëŠ” ë” ë§ì€ ì¬ì‹œë„
                )
            )

        results = await asyncio.gather(*tasks)

        # ê²°ê³¼ ì—…ë°ì´íŠ¸
        success_count = 0
        failed_count = 0

        for chunk_results in results:
            for item in chunk_results:
                text_id = item.id
                new_translation = item.translated.strip()
                original_text = id_map.get(text_id, "")

                # í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦
                if PlaceholderManager.validate_placeholder_preservation(
                    original_text, new_translation
                ):
                    translation_map[text_id] = new_translation
                    success_count += 1
                    logger.debug(
                        f"í’ˆì§ˆ ì¬ë²ˆì—­ ì„±ê³µ: {text_id} -> {new_translation[:50]}..."
                    )
                else:
                    failed_count += 1
                    missing_placeholders = PlaceholderManager.get_missing_placeholders(
                        original_text, new_translation
                    )
                    logger.warning(
                        f"í’ˆì§ˆ ì¬ë²ˆì—­ í›„ì—ë„ í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½: {text_id} (ëˆ„ë½: {missing_placeholders})"
                    )

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ (ì™„ë£Œ)
        if progress_callback:
            progress_callback(
                "ğŸ”„ í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì™„ë£Œ",
                len(chunks),
                len(chunks),
                f"ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {failed_count}ê°œ",
            )

        logger.info(
            f"í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ"
        )

        # ì¬ë²ˆì—­ì„ í•œ ë²ˆ ìˆ˜í–‰í–ˆìœ¼ë¯€ë¡œ ì´í›„ í’ˆì§ˆ ê²€í† ëŠ” ìƒëµí•˜ë„ë¡ í”Œë˜ê·¸ ë¹„í™œì„±í™”
        # (ë¬´í•œ ë£¨í”„ ë° ë¶ˆí•„ìš”í•œ ì¶”ê°€ í’ˆì§ˆ ê²€í†  ë°©ì§€)
        state["enable_quality_review"] = False

        return state

    except Exception as exc:
        logger.error(f"í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {exc}")
        logger.error(traceback.format_exc())
        return state


async def _quality_retranslate_chunk_worker(
    *,
    chunk: List[Dict],
    state: TranslatorState,
    llm: Any,
    target_language: str,
    delay_manager: RequestDelayManager,
    semaphore: asyncio.Semaphore,
    chunk_idx: int,
    total_chunks: int,
    progress_callback: Optional[callable] = None,
    max_retries: int = 3,
) -> List[TranslatedItem]:
    """í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ì„ ìœ„í•œ ì²­í¬ ì›Œì»¤"""
    async with semaphore:
        await delay_manager.wait()

        # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
        if progress_callback:
            progress_callback(
                "ğŸ”„ í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ ì¤‘",
                chunk_idx,
                total_chunks,
                f"ì²­í¬ {chunk_idx + 1}/{total_chunks} ì¬ë²ˆì—­ ì¤‘ ({len(chunk)}ê°œ í•­ëª©)",
            )

        # ê¸€ë¡œì‹œë¦¬ì—ì„œ ê´€ë ¨ ìš©ì–´ í•„í„°ë§
        all_glossary_terms = state.get("important_terms", [])
        relevant_glossary = _filter_relevant_glossary_terms(chunk, all_glossary_terms)

        # ì¬ë²ˆì—­ ì‹œë„
        for attempt in range(max_retries + 1):
            try:
                # í”„ë¡¬í”„íŠ¸ ìƒì„± (í’ˆì§ˆ ë¬¸ì œë¥¼ ê³ ë ¤í•œ ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸)
                glossary_text = TokenOptimizer.format_glossary_for_llm(
                    relevant_glossary
                )
                retry_info = (
                    f"âš ï¸ ì¬ì‹œë„ {attempt}íšŒ - ì´ì „ ë²ˆì—­ì—ì„œ ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì£¼ì˜ê¹Šê²Œ í™•ì¸í•´ì£¼ì„¸ìš”: 1. í”Œë ˆì´ìŠ¤í™€ë”([P###], [NEWLINE] ë“±)ë¥¼ ì •í™•íˆ ë³´ì¡´ 2. ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ì „ë‹¬ 3. ìì—°ìŠ¤ëŸ½ê³  ì¼ê´€ëœ ë²ˆì—­"
                    if attempt > 0
                    else "í’ˆì§ˆ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¬ë²ˆì—­"
                )
                formatted_items = _format_items_for_quality_retranslation(chunk)

                prompt = quality_retranslation_prompt(
                    target_language, glossary_text, retry_info, formatted_items
                )

                # LLM í˜¸ì¶œ
                llm_with_tools = llm.bind_tools([TranslatedItem])
                response = await llm_with_tools.ainvoke(prompt)

                # ì‘ë‹µ íŒŒì‹±
                translations = []
                if response.tool_calls:
                    for tool_call in response.tool_calls:
                        if tool_call["name"] == "TranslatedItem":
                            try:
                                item = TranslatedItem(**tool_call["args"])
                                translations.append(item)
                            except Exception as e:
                                logger.warning(f"TranslatedItem íŒŒì‹± ì˜¤ë¥˜: {e}")

                # í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦
                valid_translations = []
                for translation in translations:
                    original_text = next(
                        (
                            item["original"]
                            for item in chunk
                            if item["id"] == translation.id
                        ),
                        "",
                    )

                    if PlaceholderManager.validate_placeholder_preservation(
                        original_text, translation.translated
                    ):
                        valid_translations.append(translation)
                    else:
                        logger.debug(f"í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ ì‹¤íŒ¨: {translation.id}")

                # ëª¨ë“  ë²ˆì—­ì´ ìœ íš¨í•˜ë©´ ì„±ê³µ
                if len(valid_translations) == len(chunk):
                    if attempt > 0:
                        logger.info(
                            f"í’ˆì§ˆ ì¬ë²ˆì—­ ì²­í¬ {chunk_idx + 1} ì„±ê³µ (ì¬ì‹œë„ {attempt}íšŒ)"
                        )
                    return valid_translations
                else:
                    logger.warning(
                        f"í’ˆì§ˆ ì¬ë²ˆì—­ ì²­í¬ {chunk_idx + 1} ì‹œë„ {attempt + 1}: ìœ íš¨í•œ ë²ˆì—­ {len(valid_translations)}/{len(chunk)}"
                    )

                    # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ê³„ì† ì§„í–‰
                    if attempt < max_retries:
                        await asyncio.sleep(min(2.0, (attempt + 1) * 0.5))
                        continue
                    else:
                        # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œëŠ” ìœ íš¨í•œ ë²ˆì—­ì´ë¼ë„ ë°˜í™˜
                        return valid_translations

            except Exception as exc:
                logger.error(
                    f"í’ˆì§ˆ ì¬ë²ˆì—­ ì²­í¬ {chunk_idx + 1} ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {exc}"
                )
                if attempt < max_retries:
                    await asyncio.sleep(min(2.0, (attempt + 1) * 0.5))
                else:
                    return []

        return []


def _format_items_for_quality_retranslation(chunk: List[Dict]) -> str:
    """í’ˆì§ˆ ê¸°ë°˜ ì¬ë²ˆì—­ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±ì— í•„ìš”í•œ í¬ë§·íŒ…"""
    lines = []
    for i, item in enumerate(chunk, 1):
        text_id = item["id"]
        original = item["original"]
        current_translation = item.get("current_translation", "")

        # í’ˆì§ˆ ë¬¸ì œ ì •ë³´ ì¶”ê°€
        issues = item.get("issues", [item.get("issue")] if item.get("issue") else [])

        lines.append(f"{i}. [{text_id}]")
        lines.append(f"   ì›ë³¸: {original}")
        if current_translation:
            lines.append(f"   ì´ì „ ë²ˆì—­: {current_translation}")

        if issues:
            lines.append("   ë°œê²¬ëœ ë¬¸ì œ:")
            for issue in issues:
                if issue:
                    lines.append(f"   - {issue.issue_type}: {issue.description}")
                    if issue.suggested_fix:
                        lines.append(f"     ì œì•ˆ: {issue.suggested_fix}")

        lines.append("")

    return "\n".join(lines)


###############################################################################
# 4. Main translator class                                                    #
###############################################################################


class JSONTranslator:
    """High-level faÃ§ade wrapping the LangGraph workflow."""

    def __init__(self, *, glossary_path: Optional[str] = None) -> None:
        self._workflow = self._create_workflow()
        self.glossary_path = glossary_path
        self.llm_manager = LLMManager()

    def _create_workflow(self) -> StateGraph:  # noqa: D401
        wf = StateGraph(TranslatorState)
        wf.add_node("parse_and_extract", parse_and_extract_node)
        wf.add_node("create_primary_glossary", create_primary_glossary_node)
        wf.add_node("load_vanilla_glossary", load_vanilla_glossary_node)
        wf.add_node("smart_translate", smart_translate_node)
        wf.add_node("validation_and_retry", validation_and_retry_node)
        wf.add_node("rebuild_json", rebuild_json_node)
        wf.add_node("í–£ ", restore_placeholders_node)
        wf.add_node("quality_review", quality_review_node)
        wf.add_node("quality_based_retranslation", quality_based_retranslation_node)
        wf.add_node(
            "extract_terms_from_json_chunks", extract_terms_from_json_chunks_node
        )
        wf.add_node("load_glossary", load_glossary_node)
        wf.add_node("save_glossary", save_glossary_node)
        wf.add_node("final_fallback_translation", final_fallback_translation_node)
        wf.add_node("final_check", lambda s: s)  # Passthrough node

        wf.set_entry_point("parse_and_extract")

        # Branch for glossary creation
        wf.add_conditional_edges(
            "parse_and_extract",
            should_create_glossary,
            {
                "create_glossary": "create_primary_glossary",
                "skip_glossary": "load_vanilla_glossary",  # ë°”ë‹ë¼ ì‚¬ì „ ë¡œë“œë¡œ ë³€ê²½
            },
        )

        # 1ì°¨ ì‚¬ì „ êµ¬ì¶• í›„ ë°”ë‹ë¼ ì‚¬ì „ ë¡œë“œ
        wf.add_edge("create_primary_glossary", "load_vanilla_glossary")

        # ë°”ë‹ë¼ ì‚¬ì „ ë¡œë“œ í›„ ê¸°ì¡´ ì‚¬ì „ ë¡œë“œ
        wf.add_edge("load_vanilla_glossary", "load_glossary")

        # Glossary path
        wf.add_edge("load_glossary", "extract_terms_from_json_chunks")
        wf.add_edge("extract_terms_from_json_chunks", "smart_translate")

        # Main translation path with retries
        wf.add_conditional_edges(
            "smart_translate",
            should_retry,
            {
                "retry": "validation_and_retry",
                "complete": "rebuild_json",
                "end": "final_check",  # Go to final check on error
                "final_fallback": "final_fallback_translation",
            },
        )
        wf.add_conditional_edges(
            "validation_and_retry",
            should_retry,
            {
                "retry": "validation_and_retry",
                "complete": "rebuild_json",
                "end": "final_check",  # Go to final check on error
                "final_fallback": "final_fallback_translation",
            },
        )

        # Success path
        wf.add_edge("final_fallback_translation", "rebuild_json")
        wf.add_edge("rebuild_json", "restore_placeholders")
        wf.add_edge("restore_placeholders", "quality_review")

        # Quality review based retranslation
        wf.add_conditional_edges(
            "quality_review",
            should_retranslate_based_on_quality,
            {
                "quality_retranslate": "quality_based_retranslation",
                "complete": "final_check",
                "end": "final_check",
            },
        )

        # After quality-based retranslation, rebuild and go back to quality review
        wf.add_edge("quality_based_retranslation", "rebuild_json")

        # Final check to decide on saving
        wf.add_conditional_edges(
            "final_check",
            should_save_glossary,
            {"save_glossary": "save_glossary", "end": END},
        )
        wf.add_edge("save_glossary", END)

        # í’ˆì§ˆ ê²€í† ë¥¼ ì¬ë²ˆì—­ ì „ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ë„ë¡ ì¡°ê±´ë¶€ ë¶„ê¸°
        wf.add_conditional_edges(
            "restore_placeholders",
            should_run_quality_review,
            {"review": "quality_review", "skip": "final_check"},
        )

        return wf.compile()

    async def translate(
        self,
        json_input: Any,
        target_language: str = "í•œêµ­ì–´",
        *,
        use_glossary: bool = True,
        use_vanilla_glossary: bool = True,
        vanilla_glossary_path: Optional[str] = None,
        max_retries: int = 3,
        max_tokens_per_chunk: int = 3000,
        max_concurrent_requests: int = 5,
        delay_between_requests_ms: int = 200,
        progress_callback: Optional[callable] = None,
        existing_translations: Optional[Dict[str, str]] = None,
        llm_provider: str = "gemini",
        llm_model: str = "gemini-1.5-flash",
        temperature: float = 0.1,
        final_fallback_max_retries: int = 2,
        enable_quality_review: bool = True,
        max_quality_retries: int = 1,
    ) -> str:
        if isinstance(json_input, dict):
            json_dict = json_input
        else:
            json_dict = json.loads(str(json_input).strip())

        logger.info(
            _m(
                "translator.settings",
                max_retries=max_retries,
                chunk_tokens=max_tokens_per_chunk,
            )
        )
        logger.info(
            _m(
                "translator.parallel_settings",
                concurrent=max_concurrent_requests,
                delay=delay_between_requests_ms,
            )
        )

        # LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        llm_client = await self.llm_manager.create_llm_client(
            llm_provider, llm_model, temperature=temperature, max_tokens=100000
        )

        initial_state: TranslatorState = TranslatorState(
            parsed_json=json_dict,
            placeholders={},
            id_to_text_map={},
            important_terms=[],
            processed_json={},
            translation_map={},
            translated_json={},
            final_json="",
            target_language=target_language,
            retry_count=0,
            max_retries=max_retries,
            max_tokens_per_chunk=max_tokens_per_chunk,
            max_concurrent_requests=max_concurrent_requests,
            delay_between_requests_ms=delay_between_requests_ms,
            error=None,
            glossary_path=self.glossary_path,
            use_glossary=use_glossary,
            progress_callback=progress_callback,
            existing_translations=existing_translations,
            primary_glossary=[],
            use_vanilla_glossary=use_vanilla_glossary,
            vanilla_glossary_path=vanilla_glossary_path or "vanilla_glossary.json",
            vanilla_glossary=[],
            llm_client=llm_client,
            final_fallback_max_retries=final_fallback_max_retries,
            enable_quality_review=enable_quality_review,
            quality_issues=[],
            quality_retry_count=0,
            max_quality_retries=max_quality_retries,
        )

        result = await self._workflow.ainvoke(initial_state, {"recursion_limit": 50})
        if result.get("error"):
            raise RuntimeError(result["error"])
        return result["final_json"]


###############################################################################
# 5. Optional runnable example                                                #
###############################################################################


async def run_example() -> None:  # pragma: no cover â€“ utility function
    """Run a quick interactive demo similar to the original script."""

    logging.basicConfig(level=logging.INFO)
    logger.info(_m("translator.init_translator"))
    translator = JSONTranslator()

    # Put your test JSON here or load from file.
    test_json = {"hello": "world"}

    try:
        logger.info(_m("translator.starting_translation"))
        translated = await translator.translate(test_json, "í•œêµ­ì–´")
        logger.info(_m("translator.result"))
        logger.info(translated)
    except Exception:  # pragma: no cover â€“ human experimentation
        logger.error(_m("translator.error_occurred"))
        logger.error(traceback.format_exc())


if __name__ == "__main__":  # pragma: no cover
    asyncio.run(run_example())


def should_run_quality_review(state: TranslatorState) -> str:
    """í’ˆì§ˆ ê²€í† ë¥¼ ìˆ˜í–‰í• ì§€ ê²°ì •í•œë‹¤.

    - ì¬ë²ˆì—­ ì „(quality_retry_count == 0) ì—ë§Œ í’ˆì§ˆ ê²€í†  ì‹¤í–‰
    - ì¬ë²ˆì—­ í›„ì—ëŠ” ë°”ë¡œ ì™„ë£Œ ë‹¨ê³„ë¡œ ì´ë™í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë‘ ë²ˆì§¸ ê²€í† ë¥¼ ê±´ë„ˆë›´ë‹¤
    """
    if (
        state.get("enable_quality_review", True)
        and state.get("quality_retry_count", 0) == 0
    ):
        return "review"
    return "skip"
